{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import zipfile\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_dir = \"/resources/corpora/COHA/text/\"\n",
    "_dir = \"/resources/corpora/COHA/CCOHA/tagged/\"\n",
    "#_dir = \"/resources/corpora/COHA/ALL/\"\n",
    "files = sorted(os.listdir(_dir))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = zipfile.ZipFile(os.path.join(_dir, 'text_1810s_kso.zip'))\n",
    "#zinfos = z.infolist()\n",
    "\n",
    "#zip_file    = zipfile.ZipFile(os.path.join(_dir, 'text_1810s_kso.zip'))\n",
    "#items_file  = zip_file.open(zip_file, 'r')\n",
    "#items_file  = io.TextIOWrapper(io.BytesIO(items_file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for f in files:\n",
    "    year = f.split(\"_\")[1].split(\".\")[0][:-1]\n",
    "    if os.path.basename(os.path.normpath(_dir)) == \"ALL\":\n",
    "        df_decade = pd.read_csv(os.path.join(_dir, f), sep='\\t', quotechar='\"', quoting=csv.QUOTE_NONE, header=None, usecols=[2,3,4], encoding = \"latin\")\n",
    "    elif os.path.basename(os.path.normpath(_dir)) == \"tagged\":\n",
    "        z = zipfile.ZipFile(os.path.join(_dir, f))\n",
    "        zinfos = z.infolist()\n",
    "        for zinfo in zinfos:\n",
    "            df_decade = pd.read_csv(z.open(zinfo.filename), sep='\\t', quotechar='\"', quoting=csv.QUOTE_NONE, header=None, usecols=[0,1,2], encoding = \"utf-8\")\n",
    "            df_decade.columns = ['token', 'lemma', 'pos']\n",
    "            df_decade[\"decade\"] = year\n",
    "            df_list.append(df_decade)\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['pos'].str.contains('<sub>', na=False)].reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get number of tokens per decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_token_counts = df.decade.value_counts().to_frame()\n",
    "decade_token_counts.columns=['tokencount']\n",
    "decade_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decade_token_counts.to_csv('coha_year_token_count.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get adjacent N-N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep exactly two nn1-nn1 adjacent tokens, discard patterns with more, e.g. nn1-nn1-nn1. Also disallow the token after nn1-nn1 to be vhd, because in the older data there is a lot of \"'d\" and often the verb before gets tagged as a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.Index(df[df[\"pos\"].str.match(\"^nn1$\") & \n",
    "                    df[\"pos\"].shift(-1).str.match(\"^nn1$\") & \n",
    "                    ~(df[\"pos\"].shift(-2).str.match(\"^nn1$\").astype(\"bool\") |\n",
    "                      df[\"pos\"].shift(-2).str.contains(\"vhd\").astype(\"bool\"))].index)\n",
    "index_next = index + 1\n",
    "index_full = index.union(index_next)\n",
    "nn = df.loc[index_full]\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_window=index\n",
    "window = 10\n",
    "window_end = index+(window+1)\n",
    "window_begin = index-window\n",
    "for i in range(-window,window+2,1):\n",
    "    index_window = index_window.union(index+i)\n",
    "df_windowed = df.loc[index_window]\n",
    "df_windowed.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windowed[\"span\"] = \"i\"\n",
    "df_windowed[\"span\"][window_begin] = \"b\"\n",
    "df_windowed.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = {}\n",
    "for i, row in df_windowed.iterrows():\n",
    "    if row[\"span\"] == \"b\":\n",
    "        window_content = []\n",
    "        for j in range(i,i+((window*2)+2)):\n",
    "            window_content.append(str(df_windowed.loc[j].lemma) + \"_\" + str(df_windowed.loc[j].pos))\n",
    "        if df_windowed.loc[j].decade not in windows:\n",
    "            windows[df_windowed.loc[j].decade] = [window_content]\n",
    "        else:\n",
    "            windows[df_windowed.loc[j].decade].append(window_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove randomly a left or right context.\n",
    "# This replicates 5-grams\n",
    "windows_fivegrams = {}\n",
    "for d in windows:\n",
    "    for w in windows[d]:\n",
    "        random.seed(1991)\n",
    "        n = random.randint(0,1)\n",
    "        if n == 0:\n",
    "            if d in windows_fivegrams:\n",
    "                windows_fivegrams[d].append(w[1:])\n",
    "            else:\n",
    "                windows_fivegrams[d] = [w[1:]]\n",
    "        if n == 1:\n",
    "            if d in windows_fivegrams:\n",
    "                windows_fivegrams[d].append(w[:-1])\n",
    "            else:\n",
    "                windows_fivegrams[d] = [w[:-1]]\n",
    "windows = windows_fivegrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_list = []\n",
    "for d in windows:\n",
    "    for w in windows[d]:\n",
    "        tmp_list.append({'ngram': \" \".join(w), 'year': d, 'match_count': 1, 'volume_count': 1})\n",
    "df_ngrams = pd.DataFrame(tmp_list)\n",
    "df_ngrams.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngrams.groupby(['ngram', 'year'])['match_count'].sum().to_frame()\n",
    "df_ngrams.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ngrams.to_csv('coha_compounds/coha_fivegrams.tsv', index = False, header=False, sep=\"\\t\")\n",
    "df_ngrams.to_csv('coha_compounds/coha_twelvegrams.tsv', index = False, header=False, sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
