{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import argparse\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import re\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "#sns.set(rc={'figure.figsize':(10,10)})\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "#matplotlib.style.use('ggplot')\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a97f2f-9803-4129-a245-b1258d1a8549",
   "metadata": {},
   "source": [
    "0, 10\n",
    "\n",
    "0, 100\n",
    "\n",
    "contextual non con"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20e8e1",
   "metadata": {},
   "source": [
    "Check formula again\n",
    "\n",
    "Agnostic and Aware - \n",
    "cutoff 20, 50 \n",
    "temporal 0, 10\n",
    "\n",
    "Sparse and Dense - \n",
    "\n",
    "\n",
    "x2 x2 x2 x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_binner(year,val=10):\n",
    "    if val==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return year - year%val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3459ba52-7170-4bc7-b749-bc55ed41e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_reduction(df):\n",
    "    \n",
    "    dtype = pd.SparseDtype(np.float64, fill_value=0)\n",
    "    df=df.astype(dtype)\n",
    "\n",
    "    df_sparse, rows, cols = df.sparse.to_coo(row_levels=['common','time'],column_levels=['context'],sort_labels=False)\n",
    "\n",
    "    print(len(cols))\n",
    "    rcomp = re.compile(\".+\\s.+\")\n",
    "    compound_rows=[]\n",
    "    compound_time=[]\n",
    "    constituent_rows=[]\n",
    "    constituent_time=[]\n",
    "\n",
    "    for r in rows:\n",
    "        if re.match(rcomp, r[0]):\n",
    "            compound_rows.append(r[0])\n",
    "            compound_time.append(r[1])\n",
    "        else:\n",
    "            constituent_rows.append(r[0])\n",
    "            constituent_time.append(r[1])        \n",
    "\n",
    "    assert (len(compound_rows)+len(constituent_rows))==df_sparse.shape[0]\n",
    "    train_df=df_sparse.tocsr()[0:len(compound_rows),:]\n",
    "    test_df=df_sparse.tocsr()[len(compound_rows):,:]\n",
    "    assert (train_df.shape[0]+test_df.shape[0])==df_sparse.shape[0]\n",
    "\n",
    "    svd = TruncatedSVD(n_components=300, algorithm='arpack', random_state=args.seed)\n",
    "    print(f'Explained variance ratio {(svd.fit(train_df).explained_variance_ratio_.sum()):2.3f}')\n",
    "    \n",
    "    compound_reduced = svd.fit_transform(train_df)\n",
    "    compound_reduced = Normalizer(copy=False).fit_transform(compound_reduced)\n",
    "\n",
    "    compound_reduced=pd.DataFrame(compound_reduced,index=list(zip(compound_rows,compound_time)))\n",
    "    compound_reduced.index = pd.MultiIndex.from_tuples(compound_reduced.index, names=['compound', 'time'])\n",
    "\n",
    "    compound_reduced.reset_index(inplace=True)\n",
    "    compound_reduced[['modifier','head']]=compound_reduced['compound'].str.split(' ',expand=True)\n",
    "    compound_reduced.drop(['compound'],axis=1,inplace=True)\n",
    "    compound_reduced.set_index(['modifier','head','time'],inplace=True)\n",
    "    #compound_reduced.reset_index(inplace=True)\n",
    "    \n",
    "    constituents_reduced=svd.transform(test_df)\n",
    "    constituents_reduced = Normalizer(copy=False).fit_transform(constituents_reduced)\n",
    "    constituents_reduced=pd.DataFrame(constituents_reduced,index=list(zip(constituent_rows,constituent_time)))\n",
    "    constituents_reduced.index = pd.MultiIndex.from_tuples(constituents_reduced.index, names=['constituent', 'time'])\n",
    "    constituents_reduced.reset_index(inplace=True)\n",
    "    \n",
    "    return compound_reduced,constituents_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e229de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Compute features from sparse dataset via SVD')\n",
    "\n",
    "parser.add_argument('--temporal',  type=int,default=0,\n",
    "                    help='Value to bin the temporal information: 0 (remove temporal information), 1 (no binning), 10 (binning to decades), 20 (binning each 20 years) or 50 (binning each 50 years)')\n",
    "\n",
    "parser.add_argument('--cutoff', type=int, default=50,\n",
    "                    help='Cut-off frequency for each compound per time period : none (0), 20, 50 and 100')\n",
    "parser.add_argument('--seed', type=int, default=1991,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--contextual', action='store_true',\n",
    "                    help='Is the model contextual')\n",
    "parser.add_argument('--inputdir',type=str,\n",
    "                    help='Provide directory where features are located')\n",
    "parser.add_argument('--outputdir',type=str,\n",
    "                    help='Where should the output be stored?')\n",
    "\n",
    "args = parser.parse_args('--inputdir /home/users0/pageljs/dh/repos/Compounding/datasets/ --contextual --cutoff 10 --temporal 10 --outputdir ../Compounding/coha_compounds/'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cutoff: {args.cutoff}')\n",
    "print(f'Time span:  {args.temporal}')\n",
    "temp_cutoff_str=str(args.temporal)+'_'+str(args.cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0984d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = pickle.load( open( f'{args.inputdir}context.pkl', \"rb\" ) )\n",
    "len(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c4437-fb62-4150-b4a5-a54fcac02f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compounds=pd.read_csv(f\"/home/users0/pageljs/dh/repos/Compounding_github/data/all_compounds.txt\",sep=\"\\t\")\n",
    "all_compounds.modifier=all_compounds.modifier.str.split('_',n=1,expand=True)[[0]]\n",
    "all_compounds['head']=all_compounds['head'].str.split('_',n=1,expand=True)[[0]]\n",
    "all_compounds=all_compounds.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbefaf-cb87-4ac8-b2b9-1b29868ac587",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.contextual:\n",
    "    context='CompoundAware'\n",
    "else:\n",
    "    context='CompoundAgnostic'\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad61dbc-6ebc-4eb0-a2d8-bb4a0105bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=context+'_Dense_'+temp_cutoff_str\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324c2a4-e200-4021-8d43-a0baf84b1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(f'{args.outputdir}/{save_path}')==False:\n",
    "\n",
    "    os.mkdir(f'{args.outputdir}/{save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5577dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.contextual:\n",
    "    print(\"CompoundCentric Model\")\n",
    "\n",
    "    print('Reading compounds')\n",
    "    compounds=pd.read_pickle(args.inputdir+\"/compounds.pkl\")\n",
    "    print(compounds.shape[0])\n",
    "    compounds.context=compounds.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    compounds=compounds.loc[compounds.context.isin(context_list)]\n",
    "    print(compounds.shape[0])\n",
    "    \n",
    "    compounds.modifier=compounds.modifier.str.replace(r'_.+','',regex=True)\n",
    "    compounds['head']=compounds['head'].str.replace(r'_.+','',regex=True)\n",
    "\n",
    "    if args.temporal==0:\n",
    "        print('No temporal information is stored')\n",
    "    else:\n",
    "        print(f'Temporal information is stored with intervals {args.temporal}')\n",
    "\n",
    "    #compounds=compounds.loc[~compounds.modifier.str.contains('^(?:of|the|-)_.+')]\n",
    "    #compounds=compounds.loc[~compounds['head'].str.contains('^(?:of|the|-)_.+')]\n",
    "    \n",
    "    compounds.year=compounds.year.astype(\"int32\")\n",
    "    #compounds.query('1800 <= year <= 2010',inplace=True)\n",
    "    compounds['time']=year_binner(compounds['year'].values,args.temporal)\n",
    "    compounds=compounds.loc[compounds.groupby(['modifier','head','time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "    print(compounds.shape[0])\n",
    "\n",
    "    compounds=compounds.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "    print(compounds.shape[0])    \n",
    "    \n",
    "    modifier_lst=compounds.modifier.unique().tolist()\n",
    "    print(f'Number of unique modifiers {len(modifier_lst)}')\n",
    "\n",
    "    head_lst=compounds['head'].unique().tolist()\n",
    "    len(head_lst)    \n",
    "    print(f'Number of unique heads {len(head_lst)}')\n",
    "\n",
    "    compounds['common']=compounds['modifier']+\" \"+compounds['head']\n",
    "\n",
    "\n",
    "    compounds=compounds.groupby(['common','time','context'])['count'].sum()\n",
    "        \n",
    "    print('Done reading compounds')\n",
    "\n",
    "    print('Reading modifiers')\n",
    "\n",
    "    modifiers=pd.read_pickle(args.inputdir+\"/modifiers.pkl\")\n",
    "    print(modifiers.shape[0])\n",
    "    modifiers.context=modifiers.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    modifiers=modifiers.loc[modifiers.context.isin(context_list)]\n",
    "    print(modifiers.shape[0])\n",
    "    modifiers.modifier=modifiers.modifier.str.replace(r'_.+','',regex=True)\n",
    "\n",
    "\n",
    "    modifiers.year=modifiers.year.astype(\"int32\")\n",
    "    #modifiers.query('1800 <= year <= 2010',inplace=True)        \n",
    "    modifiers['time']=year_binner(modifiers['year'].values,args.temporal)\n",
    "    modifiers=modifiers.groupby(['modifier','time','context'])['count'].sum().to_frame().reset_index()\n",
    "    modifiers.columns=['common','time','context','count']\n",
    "    \n",
    "    print(modifiers.shape[0])\n",
    "    \n",
    "    modifiers=modifiers.loc[modifiers.common.isin(modifier_lst)]\n",
    "    print(modifiers.shape[0])\n",
    "\n",
    "    modifiers.common=modifiers.common+\"_m\"\n",
    "\n",
    "    modifiers=modifiers.groupby(['common','time','context'])['count'].sum()\n",
    "\n",
    "    print('Done reading modifiers')        \n",
    "\n",
    "    print('Reading heads')\n",
    "\n",
    "    heads=pd.read_pickle(args.inputdir+\"/heads.pkl\")\n",
    "    print(heads.shape[0])\n",
    "    heads.context=heads.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    heads=heads.loc[heads.context.isin(context_list)]\n",
    "    print(heads.shape[0])\n",
    "    heads['head']=heads['head'].str.replace(r'_.+','',regex=True)\n",
    "    \n",
    "    \n",
    "    heads.year=heads.year.astype(\"int32\")\n",
    "    #heads.query('1800 <= year <= 2010',inplace=True)\n",
    "    heads['time']=year_binner(heads['year'].values,args.temporal)\n",
    "    heads=heads.groupby(['head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "    heads.columns=['common','time','context','count']\n",
    "    print(heads.shape[0])\n",
    "    \n",
    "    heads=heads.loc[heads.common.isin(modifier_lst)]\n",
    "    print(heads.shape[0])\n",
    "    \n",
    "    heads.common=heads.common+\"_h\"\n",
    "    \n",
    "    heads=heads.groupby(['common','time','context'])['count'].sum()\n",
    "\n",
    "\n",
    "    print('Done reading heads')\n",
    "\n",
    "    print('Concatenating all the datasets together')\n",
    "    \n",
    "    \n",
    "    df=pd.concat([compounds,heads,modifiers], sort=False)\n",
    "\n",
    "else:\n",
    "    print(\"CompoundAgnostic Model\")\n",
    "    \n",
    "    print('Reading phrases')\n",
    "    compounds=pd.read_pickle(args.inputdir+\"/phrases.pkl\")\n",
    "    print(compounds.shape[0])\n",
    "    compounds.context=compounds.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    compounds=compounds.loc[compounds.context.isin(context_list)]\n",
    "    print(compounds.shape[0])\n",
    "    \n",
    "    compounds.modifier=compounds.modifier.str.replace(r'_.+','',regex=True)\n",
    "    compounds['head']=compounds['head'].str.replace(r'_.+','',regex=True)\n",
    "\n",
    "    if args.temporal==0:\n",
    "        print('No temporal information is stored')\n",
    "    else:\n",
    "        print(f'Temporal information is stored with intervals {args.temporal}')\n",
    "\n",
    "    #compounds=compounds.loc[~compounds.modifier.str.contains('^(?:of|the|-)_.+')]\n",
    "    #compounds=compounds.loc[~compounds['head'].str.contains('^(?:of|the|-)_.+')]\n",
    "\n",
    "    compounds.year=compounds.year.astype(\"int32\")\n",
    "    #compounds.query('1800 <= year <= 2010',inplace=True)\n",
    "    compounds['time']=year_binner(compounds['year'].values,args.temporal)\n",
    "    compounds=compounds.loc[compounds.groupby(['modifier','head','time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "    print(compounds.shape[0])\n",
    "\n",
    "    compounds=compounds.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "    constituents_lst=list(set(compounds.modifier.unique().tolist()+compounds['head'].unique().tolist()))\n",
    "\n",
    "    compounds['common']=compounds['modifier']+\" \"+compounds['head']\n",
    "    compounds=compounds.groupby(['common','time','context'])['count'].sum()\n",
    "\n",
    "    print('Done reading compounds')\n",
    "    \n",
    "    print(f'Number of unique constituents {len(constituents_lst)}')\n",
    "    \n",
    "    print('Reading constituents')\n",
    "    constituents=pd.read_pickle(args.inputdir+\"/words.pkl\")\n",
    "    print(constituents.shape[0])\n",
    "    constituents.context=constituents.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    constituents=constituents.loc[constituents.context.isin(context_list)]\n",
    "    print(constituents.shape[0])\n",
    "    constituents.word=constituents.word.str.replace(r'_.+','',regex=True)\n",
    "    constituents=constituents.loc[constituents.word.isin(constituents_lst)]\n",
    "    print(constituents.shape[0])\n",
    "    \n",
    "    constituents.year=constituents.year.astype(\"int32\")\n",
    "    #constituents.query('1800 <= year <= 2010',inplace=True)\n",
    "    constituents['time']=year_binner(constituents['year'].values,args.temporal)\n",
    "    constituents=constituents.groupby(['word','time','context'])['count'].sum().to_frame().reset_index()\n",
    "    constituents.columns=['common','time','context','count']\n",
    "    constituents=constituents.groupby(['common','time','context'])['count'].sum()\n",
    "\n",
    "    print(constituents.shape[0])\n",
    "    print('Done reading constituents')\n",
    "    \n",
    "    print('Concatenating all the datasets together')\n",
    "    \n",
    "    df=pd.concat([compounds,constituents], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8298738-e162-459f-ae57-8b183642ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_lst=compounds.index.unique(level='time').to_list()\n",
    "time_lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5326d58-b5b3-43d0-a699-b4716f4115e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def productivity_features(df):\n",
    "    t1=time.time()\n",
    "    print(\"Productivity\")\n",
    "     \n",
    "    all_comps=df.reset_index()[['modifier','head','time']]\n",
    "    mod_prod=df.groupby(['modifier','time']).size().to_frame()\n",
    "    mod_prod.columns=['mod_prod']\n",
    "    head_prod=df.groupby(['head','time']).size().to_frame()\n",
    "    head_prod.columns=['head_prod']\n",
    "    prod1=pd.merge(all_comps,mod_prod.reset_index(),how='left',on=['modifier','time'])\n",
    "    productivity=pd.merge(prod1,head_prod.reset_index(),how='left',on=['head','time'])\n",
    "    productivity.set_index(['modifier','head','time'],inplace=True)\n",
    "    \n",
    "    #print(f\"Time taken {time.time()-t1} secs\") \n",
    "    return productivity\n",
    "    \n",
    "\n",
    "def freq_features(df):\n",
    "    \n",
    "    t1=time.time()\n",
    "    print(\"Frequency features\")\n",
    "        \n",
    "    compound_decade_counts=df.groupby('time').sum().sum(axis=1).to_frame()\n",
    "    compound_decade_counts.columns=['N']\n",
    "\n",
    "    XY=df.groupby(['modifier','head','time']).sum().sum(axis=1).to_frame()\n",
    "    X_star=df.groupby(['modifier','time']).sum().sum(axis=1).to_frame()\n",
    "    Y_star=df.groupby(['head','time']).sum().sum(axis=1).to_frame()\n",
    "\n",
    "    XY.columns=['a']\n",
    "    X_star.columns=['x_star']\n",
    "    Y_star.columns=['star_y']\n",
    "\n",
    "\n",
    "    merge1=pd.merge(XY.reset_index(),X_star.reset_index(),on=['modifier','time'])\n",
    "\n",
    "    frequency_feat=pd.merge(merge1,Y_star.reset_index(),on=['head','time'])\n",
    "    \n",
    "    frequency_feat=frequency_feat.rename(columns = {'a':'comp_freq','x_star':'mod_freq','star_y':'head_freq'})\n",
    "    frequency_feat.set_index(['modifier','head','time'],inplace=True)\n",
    "\n",
    "    #print(f\"Time taken {time.time()-t1} secs\") \n",
    "    return frequency_feat\n",
    "\n",
    "def it_features(df):\n",
    "    \n",
    "    t1=time.time()\n",
    "    print(\"Information Theory features\")\n",
    "    \n",
    "    compound_decade_counts=df.groupby('time').sum().sum(axis=1).to_frame()\n",
    "    compound_decade_counts.columns=['N']\n",
    "\n",
    "    XY=df.groupby(['modifier','head','time']).sum().sum(axis=1).to_frame()\n",
    "    X_star=df.groupby(['modifier','time']).sum().sum(axis=1).to_frame()\n",
    "    Y_star=df.groupby(['head','time']).sum().sum(axis=1).to_frame()\n",
    "\n",
    "\n",
    "    XY.columns=['a']\n",
    "    X_star.columns=['x_star']\n",
    "    Y_star.columns=['star_y']\n",
    "\n",
    "\n",
    "    merge1=pd.merge(XY.reset_index(),X_star.reset_index(),on=['modifier','time'])\n",
    "\n",
    "    information_feat=pd.merge(merge1,Y_star.reset_index(),on=['head','time'])\n",
    "    \n",
    "   \n",
    "    information_feat['b']=information_feat['x_star']-information_feat['a']\n",
    "    information_feat['c']=information_feat['star_y']-information_feat['a']\n",
    "\n",
    "    information_feat=pd.merge(information_feat,compound_decade_counts.reset_index(),on=['time'])\n",
    "    information_feat['d']=information_feat['N']-(information_feat['a']+information_feat['b']+information_feat['c'])\n",
    "    information_feat['x_bar_star']=information_feat['N']-information_feat['x_star']\n",
    "    information_feat['star_y_bar']=information_feat['N']-information_feat['star_y']\n",
    "\n",
    "    information_feat.set_index(['modifier','head','time'],inplace=True)\n",
    "\n",
    "    information_feat['ppmi']=np.log2((information_feat['a']*information_feat['N']+1)/(information_feat['x_star']*information_feat['star_y']+1))\n",
    "    information_feat['local_mi']=information_feat['a']*information_feat['ppmi']\n",
    "    information_feat['log_ratio']=2*(information_feat['local_mi']+\\\n",
    "    information_feat['b']*np.log2((information_feat['b']*information_feat['N']+1)/(information_feat['x_star']*information_feat['star_y_bar']+1))+\\\n",
    "    information_feat['c']*np.log2((information_feat['c']*information_feat['N']+1)/(information_feat['x_bar_star']*information_feat['star_y']+1))+\\\n",
    "    information_feat['d']*np.log2((information_feat['d']*information_feat['N']+1)/(information_feat['x_bar_star']*information_feat['star_y_bar']+1)))\n",
    "\n",
    "    information_feat.ppmi.loc[information_feat.ppmi<=0]=0\n",
    "    information_feat.drop(['a','x_star','star_y','b','c','d','N','d','x_bar_star','star_y_bar'],axis=1,inplace=True)\n",
    "    \n",
    "    #print(f\"Time taken {time.time()-t1} secs\") \n",
    "    return information_feat\n",
    "\n",
    "\n",
    "\n",
    "def cosine_features(compound_df,modifier_df,head_df):\n",
    "    \n",
    "    t1=time.time()\n",
    "    print(\"Cosine Similarity features\")\n",
    "\n",
    "    \n",
    "    compound_modifier_sim=(compound_df*modifier_df).dropna().sum(axis=1).to_frame()\n",
    "    compound_modifier_sim.columns=['sim_with_modifier']\n",
    "    compound_modifier_sim=compound_modifier_sim.swaplevel('time','head')\n",
    "\n",
    "\n",
    "    compound_head_sim=(compound_df*head_df).dropna().sum(axis=1).to_frame()\n",
    "    compound_head_sim.columns=['sim_with_head']\n",
    "    compound_head_sim=compound_head_sim.swaplevel('time','modifier')\n",
    "    compound_head_sim=compound_head_sim.swaplevel('head','modifier')\n",
    "\n",
    "\n",
    "    constituent_sim=compounds_reduced.reset_index()[['modifier','head','time']].merge(modifiers_reduced.reset_index(),how='left',on=['modifier','time'])\n",
    "    constituent_sim.set_index(['modifier','head','time'],inplace=True)\n",
    "\n",
    "\n",
    "    constituent_sim=(constituent_sim*heads_reduced).dropna().sum(axis=1).to_frame()\n",
    "    constituent_sim.columns=['sim_bw_constituents']\n",
    "    constituent_sim=constituent_sim.swaplevel('time','modifier')\n",
    "    constituent_sim=constituent_sim.swaplevel('head','modifier')\n",
    "    #print(f\"Time taken {time.time()-t1} secs\") \n",
    "    return compound_modifier_sim,compound_head_sim,constituent_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4f08a-d37c-451a-b63e-d568e6e66656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_time in time_lst:\n",
    "    print(cur_time)\n",
    "    print('Running SVD')   \n",
    "    compounds_reduced,constituents_reduced=dim_reduction(df.loc[df.index.get_level_values(1)==cur_time])\n",
    "    \n",
    "    if args.contextual:\n",
    "        heads_reduced=constituents_reduced.loc[constituents_reduced.constituent.str.contains(r'.+_h$')]\n",
    "        heads_reduced=heads_reduced.rename(columns = {'constituent':'head'})\n",
    "        heads_reduced['head']=heads_reduced['head'].str.replace(r'_.+','',regex=True)\n",
    "        heads_reduced.set_index(['head','time'],inplace=True)\n",
    "        \n",
    "        modifiers_reduced=constituents_reduced.loc[constituents_reduced.constituent.str.contains(r'.+_m$')]\n",
    "        modifiers_reduced=modifiers_reduced.rename(columns = {'constituent':'modifier'})\n",
    "        modifiers_reduced['modifier']=modifiers_reduced['modifier'].str.replace(r'_.+','',regex=True)\n",
    "        modifiers_reduced.set_index(['modifier','time'],inplace=True)            \n",
    "    \n",
    "    else:\n",
    "        heads_reduced=constituents_reduced.copy()\n",
    "        heads_reduced=heads_reduced.rename(columns = {'constituent':'head'})\n",
    "        heads_reduced.set_index(['head','time'],inplace=True)\n",
    "\n",
    "        modifiers_reduced=constituents_reduced.copy()\n",
    "        modifiers_reduced=modifiers_reduced.rename(columns = {'constituent':'modifier'})\n",
    "        modifiers_reduced.set_index(['modifier','time'],inplace=True)            \n",
    "\n",
    "    \n",
    "    print(\"Calculating features\")\n",
    "    #compounds_reduced=compounds_reduced+1\n",
    "    \n",
    "    productivity=productivity_features(compounds_reduced)\n",
    "\n",
    "    frequency=freq_features(compounds_reduced)\n",
    "    information_feat=it_features(compounds_reduced+1)\n",
    "    compound_modifier_sim,compound_head_sim,constituent_sim=cosine_features(compounds_reduced,modifiers_reduced,heads_reduced)\n",
    "    \n",
    "    dfs = [constituent_sim, compound_head_sim, compound_modifier_sim, information_feat,frequency,productivity]\n",
    "\n",
    "    compounds_final = reduce(lambda left,right: pd.merge(left,right,left_index=True, right_index=True), dfs)\n",
    "\n",
    "\n",
    "    compounds_final=pd.pivot_table(compounds_final.reset_index(), index=['modifier','head'], columns=['time'])\n",
    "    #print(compounds_final.isna().sum().sum())\n",
    "    #compounds_final.fillna(0,inplace=True)\n",
    "        #compounds_final -= compounds_final.min()\n",
    "        #compounds_final /= compounds_final.max()\n",
    "    compounds_final_1=compounds_final.columns.get_level_values(0)\n",
    "    compounds_final_2=compounds_final.columns.get_level_values(1)\n",
    "\n",
    "    cur_year=0\n",
    "    new_columns=[]\n",
    "    for year in compounds_final_2:\n",
    "        new_columns.append(compounds_final_1[cur_year]+\"_\"+str(year))\n",
    "        cur_year+=1\n",
    "    compounds_final.columns=new_columns\n",
    "    \n",
    "    merge_210_df=all_compounds.merge(compounds_final.reset_index(),on=['modifier','head'],how='inner')\n",
    "    merge_210_df.set_index([\"modifier\", \"head\"], inplace = True)\n",
    "    print(f\"Num of compounds found in {cur_time}: {merge_210_df.shape[0]}\")\n",
    "    merge_210_df.to_csv(f'{args.outputdir}/{save_path}/{cur_time}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff20aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in features_df:\n",
    "\n",
    "    corr=df.corr()\n",
    "    \n",
    "    display(corr.loc[:,['modifier_mean','head_mean','compound_mean']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.compound_mean.sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a1028-f2f4-4f69-b6f1-5f9a49e4ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv('/home/users0/pageljs/dh/repos/Compounding/coha_compounds/trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f6151-ee69-48a2-b2f0-5bdd6860d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense\n",
    "\n",
    "Frequency\n",
    "\n",
    "Probeer dein Code zu benutzen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
