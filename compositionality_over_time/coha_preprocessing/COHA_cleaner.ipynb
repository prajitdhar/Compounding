{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "from zipfile import ZipFile,ZipInfo\n",
    "import editdistance\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "#spacy.prefer_gpu()\n",
    "#import contextualSpellCheck\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length=10_000_000\n",
    "#contextualSpellCheck.add_to_pipe(nlp)\n",
    "#from spacy_hunspell import spaCyHunSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editer(row):\n",
    "    return editdistance.distance(row['before'],row['after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(input_zip):\n",
    "    input_zip=ZipFile(input_zip)\n",
    "    return {name: input_zip.read(name).decode('utf-8').strip() for name in input_zip.namelist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#roberta = torch.hub.load('pytorch/fairseq', 'roberta.large')\n",
    "#roberta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docenizer(dict_content):\n",
    "\n",
    "    str1=re.sub(\"@@.*|<P>|/\\S*/\",\"\",dict_content)\n",
    "    str2 = re.sub(' +',' ',str1)\n",
    "    str2=str2.replace(\"@ @ @ @ @ @ @ @ @ @\",\"@@@@@@@@@@\")\n",
    "    str2=str2.replace(\"\\n\\n\",\"\")\n",
    "    doc=nlp(str2)\n",
    "    to_hold=[]\n",
    "    n_words=0\n",
    "    n_sents=0\n",
    "    for sent in doc.sents:\n",
    "        if len(sent)==1:\n",
    "            continue\n",
    "        cur_sent=[]\n",
    "        for word in sent:\n",
    "            cur_sent.append(word.text)\n",
    "            n_words+=1\n",
    "        to_hold.append(' '.join(cur_sent))\n",
    "        n_sents+=1\n",
    "    content='\\n'.join(to_hold)\n",
    "    content=content.lower()\n",
    "    #content=content.replace(\"@@@@@@@@@@\", \"<mask>\")\n",
    "    return content,n_words,n_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_maker(dict_content):\n",
    "    token_list=[]\n",
    "    pos_list=[]\n",
    "    lemma_list=[]\n",
    "    str1=re.sub(\"@@.*|<P>|/\\S*/\",\"\",dict_content)\n",
    "    str2 = re.sub(' +',' ',str1)\n",
    "    str2=str2.replace(\"@ @ @ @ @ @ @ @ @ @\",\"@@@@@@@@@@\")\n",
    "    str2=str2.replace(\"\\n\\n\",\"\")\n",
    "    doc=nlp(str2)\n",
    "    for token in doc:\n",
    "        token_list.append(token.text)\n",
    "        pos_list.append(token.pos_)\n",
    "        lemma_list.append(token.lemma_)\n",
    "    return token_list,pos_list,lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(fnames,dec,set_type):\n",
    "    save_file=\"./\"+str(dec)+\"/\"+set_type+\".txt\"\n",
    "    print(save_file)\n",
    "    with open(save_file,'w') as f:\n",
    "        for doc in fnames:\n",
    "            f.write(zipfiles[doc]+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = \"/resources/corpora/COHA/text/\"\n",
    "#_dir = \"/resources/corpora/COHA/CCOHA/tagged/\"\n",
    "#_dir = \"/resources/corpora/COHA/ALL/\"\n",
    "files = sorted(os.listdir(_dir))\n",
    "\n",
    "to_keep=[]\n",
    "for f in files:\n",
    "    if 'zip' in f:\n",
    "        to_keep.append(f)\n",
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process=to_keep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfiles=extract_zip(os.path.join(_dir, to_process))\n",
    "zfnames=list(zipfiles.keys())\n",
    "docs=list(zipfiles.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_year=to_process.split('_')[1][:-1]\n",
    "curr_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proc = mp.cpu_count()-1\n",
    "\n",
    "pool = Pool(n_proc)\n",
    "results=pool.map_async(vocab_maker,docs)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=results.get()\n",
    "token_list = [val[0] for val in results]\n",
    "pos_list=[val[1] for val in results]\n",
    "lemma_list=[val[2] for val in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_token_list = [item for sublist in token_list for item in sublist]\n",
    "len(full_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proc = mp.cpu_count()-1\n",
    "\n",
    "pool = Pool(n_proc)\n",
    "results=pool.map_async(docenizer,docs)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results=results.get()\n",
    "contents=[val[0] for val in results]\n",
    "n_words=[val[1] for val in results]\n",
    "n_sents=[val[2] for val in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfiles=dict(zip(zfnames,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setmaker(fname,curr_year):\n",
    "    cur_set=zipfiles[fname].split('\\n')\n",
    "    testset=cur_set[:1000]\n",
    "    validset=cur_set[1001:2001]\n",
    "    \n",
    "    test_file=\"./\"+str(curr_year)+\"/test.txt\"\n",
    "    print(test_file)\n",
    "    with open(test_file,'w') as f:\n",
    "        for sent in testset:\n",
    "            f.write(sent+\"\\n\\n\")\n",
    "\n",
    "    valid_file=\"./\"+str(curr_year)+\"/valid.txt\"           \n",
    "    with open(valid_file,'w') as f:\n",
    "        for sent in testset:\n",
    "            f.write(sent+\"\\n\\n\") \n",
    "    return '\\n'.join(cur_set[2002:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres=pd.DataFrame({'fname':zfnames,'content':contents,'n_words':n_words,'n_sents':n_sents})\n",
    "genres['gtype']=genres['fname'].str.split('_').str[0]\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_docs=genres.sort_values('n_sents', ascending=False).head(1)['fname'].to_list()\n",
    "min_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfiles[min_docs[0]]=setmaker(min_docs[0],curr_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_list=zipfiles.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_dfs=pd.concat(dfs)\n",
    "all_dfs.columns=['token','lemma','pos','dep','shape','is_alpha','is_stop']\n",
    "all_dfs=all_dfs.loc[~(all_dfs.pos=='SPACE')]\n",
    "all_dfs.token=all_dfs.token.str.lower()\n",
    "#all_dfs.index[all_dfs['pos'] == 'PUNCT']\n",
    "all_dfs.reset_index(inplace=True,drop=True)\n",
    "#all_dfs.loc[all_dfs.is_stop==True]['lemma'].value_counts().head(30)\n",
    "punct_list=all_dfs.index[(all_dfs['pos'] == 'PUNCT') & (all_dfs['token'] == '.')]\n",
    "punct_list+=1\n",
    "all_token=all_dfs.token.to_list()\n",
    "len(punct_list)\n",
    "sents=np.split(all_token,punct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(trainset_list,curr_year,'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
