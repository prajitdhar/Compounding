{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc99f6-c6b3-4449-bb87-ab6e166cb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398129b-fa35-40dd-a68c-0c3b29bbc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Program to run compounder for the coha files')\n",
    "\n",
    "parser.add_argument('--input', type=str,\n",
    "                    help='location of the pickle data files')\n",
    "\n",
    "parser.add_argument('--word', action='store_true',\n",
    "                    help='Extracting context for words only?')\n",
    "\n",
    "parser.add_argument('--output', type=str,\n",
    "                    help='directory to save dataset in')\n",
    "\n",
    "\n",
    "args = parser.parse_args('--input /home/users0/pageljs/dh/repos/coha_datasets/ --output /home/users0/pageljs/dh/repos/Compounding/datasets/ --word'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47047c8b-b8f7-41f3-96be-7d157ae8ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_id_vars=['modifier','head','year','count','num_comp','comp_ner_sent']\n",
    "modifier_id_vars=['modifier','year','count','num_comp','comp_ner_sent']\n",
    "head_id_vars=['head','year','count','num_comp','comp_ner_sent']\n",
    "constituent_id_vars=['constituent','year','count','num_comp','comp_ner_sent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b64e97-8c40-4aac-82f1-6d8b743772e8",
   "metadata": {},
   "source": [
    "N N _ _ _\n",
    "_ N N _ _\n",
    "_ _ N N _\n",
    "_ _ _ N N\n",
    "\n",
    "N N N _ _\n",
    "\n",
    "_ N N N _\n",
    "\n",
    "_ _ N N N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522e0590-a94e-4086-87af-113e4b6b5e4a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "nn='.+_(?!(?:NOUN|PROPN)).*'\n",
    "hyphen_comp='.+_(?:NOUN|PROPN)\\s-_.+\\s.+_(?:NOUN|PROPN)'\n",
    "\n",
    "h1=f'^{hyphen_comp}\\s{nn}\\s{word}$'\n",
    "h2=f'^{nn}\\s{hyphen_comp}\\s{nn}$'\n",
    "h3=f'^{word}\\s{nn}\\s{hyphen_comp}$'\n",
    "\n",
    "ph1=f'^{hyphen_comp}\\s{word}\\s{word}$'\n",
    "ph2=f'^{word}\\s{hyphen_comp}\\s{word}$'\n",
    "ph3=f'^{word}\\s{word}\\s{hyphen_comp}$'\n",
    "\n",
    "\n",
    "head_of_mod='.+_(?:NOUN|PROPN)\\sof_.*\\s.+_(?:NOUN|PROPN)'\n",
    "\n",
    "m1=f'^{head_of_mod}\\s{nn}\\s{word}$'\n",
    "m2=f'^{nn}\\s{head_of_mod}\\s{nn}$'\n",
    "m3=f'^{word}\\s{nn}\\s{head_of_mod}$'\n",
    "\n",
    "pm1=f'^{head_of_mod}\\s{word}\\s{word}$'\n",
    "pm2=f'^{word}\\s{head_of_mod}\\s{word}$'\n",
    "pm3=f'^{word}\\s{word}\\s{head_of_mod}$'\n",
    "\n",
    "\n",
    "\n",
    "head_of_the_mod='.+_(?:NOUN|PROPN)\\sof_.*\\sthe_.+\\s.+_(?:NOUN|PROPN)'\n",
    "\n",
    "t1=f'^{head_of_the_mod}\\s{nn}$'\n",
    "t2=f'^{nn}\\s{head_of_the_mod}$'\n",
    "\n",
    "pt1=f'^{head_of_the_mod}\\s{word}$'\n",
    "pt2=f'^{word}\\s{head_of_the_mod}$'\n",
    "\n",
    "\n",
    "possessives=\".+_(?:NOUN|PROPN)\\s'.*_.+\\s.+_(?:NOUN|PROPN)\"\n",
    "\n",
    "pos1=f'^{possessives}\\s{word}\\s{word}$'\n",
    "pos2=f'^{word}\\s{possessives}\\s{word}$'\n",
    "pos3=f'^{word}\\s{word}\\s{possessives}$'\n",
    "\n",
    "ppos1=f'^{possessives}\\s{word}\\s{word}$'\n",
    "ppos2=f'^{word}\\s{possessives}\\s{word}$'\n",
    "ppos3=f'^{word}\\s{word}\\s{possessives}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b29f01-d208-4eb0-8e27-47e673f7b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "word='.+_.+'\n",
    "comp='.+_(?:NOUN|PROPN)\\s.+_(?:NOUN|PROPN)'\n",
    "\n",
    "\n",
    "p2=f'^{comp}\\s{word}\\s{word}\\s{word}$'\n",
    "p3=f'^{word}\\s{comp}\\s{word}\\s{word}$'\n",
    "p4=f'^{word}\\s{word}\\s{comp}\\s{word}$'\n",
    "p5=f'^{word}\\s{word}\\s{word}\\s{comp}$'\n",
    "\n",
    "phrase_dict={2:p2,3:p3,4:p4,5:p5}\n",
    "\n",
    "noun='.+_(?:NOUN|PROPN)'\n",
    "\n",
    "n1=f'^{noun}\\s{word}\\s{word}\\s{word}\\s{word}$'\n",
    "n2=f'^{word}\\s{noun}\\s{word}\\s{word}\\s{word}$'\n",
    "n3=f'^{word}\\s{word}\\s{noun}\\s{word}\\s{word}$'\n",
    "n4=f'^{word}\\s{word}\\s{word}\\s{noun}\\s{word}$'\n",
    "n5=f'^{word}\\s{word}\\s{word}\\s{word}\\s{noun}$'\n",
    "\n",
    "word_dict={1:n1,2:n2,3:n3,4:n4,5:n5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d540c9dd-d37a-4a9d-915c-b875968b506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid1_parser(df,phrases=False): # _ N N _ _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "def mid2_parser(df,phrases=False): # _ _ N N _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "    \n",
    "def mid_parser_hyphen(df,phrases=False): # _ N - N _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','hyphen','head','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','hyphen','head','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "    \n",
    "def mid_parser_of(df,phrases=False): # _ N of N _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','head','of','modifier','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','head','of','modifier','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "\n",
    "def mid_parser_pos(df,phrases=False): # _ N s N _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','s','head','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','s','head','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6db9a5-5f52-4dbe-87bd-b6c45700ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_side_parser(df,phrases=False): # N N _ _ _ \n",
    "    cur_df=df.copy()\n",
    "    if not phrases:\n",
    "\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "\n",
    "def left_side_parser_hyphen(df,phrases=False): # N - N _ _ \n",
    "    cur_df=df.copy()\n",
    "    if not phrases:\n",
    "\n",
    "        try:\n",
    "            cur_df[['modifier','hyphen','head','w1','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['modifier','hyphen','head','w1','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "def left_side_parser_of(df,phrases=False): # N of N _ _ \n",
    "    cur_df=df.copy()\n",
    "    if not phrases:\n",
    "\n",
    "        try:\n",
    "            cur_df[['head','of','modifier','w1','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['head','of','modifier','w1','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "    \n",
    "def left_side_parser_pos(df,phrases=False): # N 's N _ _ \n",
    "    cur_df=df.copy()\n",
    "    if not phrases:\n",
    "\n",
    "        try:\n",
    "            cur_df[['modifier','s','head','w1','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['modifier','s','head','w1','w2']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881946c0-db23-4e9a-a6ed-0dcf07b0e7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_side_parser(df,phrases=False): # _ _ _ N N\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "    \n",
    "def right_side_parser_hyphen(df,phrases=False): # _ _ N - N\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','hyphen','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','hyphen','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "    \n",
    "def right_side_parser_of(df,phrases=False): # _ _ N of N\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','head','of','modifier']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','head','of','modifier']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "    \n",
    "def right_side_parser_pos(df,phrases=False): # _ _ N 's N\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','s','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','s','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2'],value_name='context')\n",
    "        \n",
    "        return phrase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f07227-18a2-465f-addc-f11d56e96be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df,phrases=False):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # N N _ N N\n",
    "        compound_left_df,modifier_left_df,head_left_df=left_side_parser(df)\n",
    "        compound_right_df,modifier_right_df,head_right_df=right_side_parser(df)\n",
    "        \n",
    "        final_compound_df=pd.concat([compound_left_df,compound_right_df],ignore_index=True)\n",
    "        final_modifier_df=pd.concat([modifier_left_df,modifier_right_df],ignore_index=True)\n",
    "        final_head_df=pd.concat([head_left_df,head_right_df],ignore_index=True)\n",
    "           \n",
    "    elif pattern==2: # N N _ _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=left_side_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=left_side_parser(df,phrases=True)\n",
    "\n",
    "    elif pattern==3: # _ N N _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid1_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid1_parser(df,phrases=True)\n",
    "    \n",
    "    elif pattern==4: # _ _ N N _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid2_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid2_parser(df,phrases=True)\n",
    "        \n",
    "    elif pattern==5: # _ _ _ N N\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=right_side_parser(df)   \n",
    "        else:\n",
    "            final_phrases_df=right_side_parser(df,phrases=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "    elif pattern==6: # N - N _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=left_side_parser_hyphen(df) \n",
    "        else:\n",
    "            final_phrases_df=left_side_parser_hyphen(df,phrases=True)\n",
    "            \n",
    "    elif pattern==7: # _ N - N _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid_parser_hyphen(df)  \n",
    "        else:\n",
    "            final_phrases_df=mid_parser_hyphen(df,phrases=True)\n",
    "            \n",
    "    elif pattern==8: # _ _ N - N\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=right_side_parser_hyphen(df)\n",
    "        else:\n",
    "            final_phrases_df=right_side_parser_hyphen(df,phrases=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    elif pattern==9: # N of N _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=left_side_parser_of(df)\n",
    "            \n",
    "        else:\n",
    "            final_phrases_df=left_side_parser_of(df,phrases=True)\n",
    "                  \n",
    "    elif pattern==10: # _ N of N _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid_parser_of(df)\n",
    "            \n",
    "        else:\n",
    "            final_phrases_df=mid_parser_of(df,phrases=True)\n",
    "                    \n",
    "    elif pattern==11: # _ _ N of N\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=right_side_parser_of(df)\n",
    "            \n",
    "        else:\n",
    "            final_phrases_df=right_side_parser_of(df,phrases=True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    elif pattern==12: # N s N _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=left_side_parser_pos(df)\n",
    "            \n",
    "        else:\n",
    "            final_phrases_df=left_side_parser_pos(df,phrases=True)\n",
    "            \n",
    "    elif pattern==13: # _ N s N _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid_parser_pos(df)\n",
    "            \n",
    "        else:\n",
    "            final_phrases_df=mid_parser_pos(df,phrases=True)\n",
    "            \n",
    "    elif pattern==14: # _ _ N s N\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=right_side_parser_pos(df)\n",
    "            \n",
    "        else:\n",
    "            final_phrases_df=right_side_parser_pos(df,phrases=True)\n",
    "            \n",
    "\n",
    "    if not phrases:\n",
    "        return final_compound_df,final_modifier_df,final_head_df\n",
    "    else:\n",
    "        return final_phrases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e17e4-0821-45b1-9a8c-121d6b11de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_extracter(df,phrases=False):\n",
    "    \n",
    "    comp_df_list=[]\n",
    "    head_df_list=[]\n",
    "    mod_df_list=[]\n",
    "    phrase_df_list=[]\n",
    "    if not phrases:\n",
    "        \n",
    "        for i in range(1,6):\n",
    "            \n",
    "            if df.loc[df.comp_class==i].shape[0]!=0:\n",
    "                cur_comp_df,cur_mod_df,cur_head_df=syntactic_reducer(df.loc[df.comp_class==i])\n",
    "\n",
    "                comp_df_list.append(cur_comp_df)\n",
    "                mod_df_list.append(cur_mod_df)\n",
    "                head_df_list.append(cur_head_df)\n",
    "    \n",
    "        compounds=pd.concat(comp_df_list,ignore_index=True,sort=False)\n",
    "        modifiers=pd.concat(mod_df_list,ignore_index=True,sort=False)\n",
    "        heads=pd.concat(head_df_list,ignore_index=True,sort=False)\n",
    "\n",
    "        \n",
    "        compounds.dropna(inplace=True)\n",
    "        compounds=compounds.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "\n",
    "        modifiers.dropna(inplace=True)\n",
    "        modifiers=modifiers.groupby(['modifier','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        heads.dropna(inplace=True)\n",
    "        heads=heads.groupby(['head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        return compounds,modifiers,heads\n",
    "    \n",
    "    else:\n",
    "        for i in range(2,6):\n",
    "            cur_df=df.loc[df.lemma_pos.str.contains(phrase_dict[i])].copy()\n",
    "            if cur_df.shape[0]!=0:\n",
    "                cur_df.comp_class=i\n",
    "                cur_phrase_df=syntactic_reducer(cur_df,phrases=True)\n",
    "                phrase_df_list.append(cur_phrase_df)\n",
    "\n",
    "        phrases=pd.concat(phrase_df_list,ignore_index=True,sort=False)\n",
    "        \n",
    "        phrases.dropna(inplace=True)\n",
    "        phrases=phrases.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        phrases.reset_index(inplace=True)\n",
    "        \n",
    "        return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30f3dd-d354-44ac-ab60-5ca90b7e1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n1_word_parser(df):\n",
    "    df[['constituent','w1','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "    \n",
    "    word_df=pd.melt(df,id_vars=constituent_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "    return word_df\n",
    "\n",
    "def n2_word_parser(df):\n",
    "    df[['w1','constituent','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "    \n",
    "    word_df=pd.melt(df,id_vars=constituent_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "        \n",
    "    return word_df\n",
    "\n",
    "def n3_word_parser(df):\n",
    "    df[['w1','w2','constituent','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "    \n",
    "    word_df=pd.melt(df,id_vars=constituent_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "        \n",
    "    return word_df\n",
    "\n",
    "def n4_word_parser(df):\n",
    "    df[['w1','w2','w3','constituent','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "    \n",
    "    word_df=pd.melt(df,id_vars=constituent_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "        \n",
    "    return word_df\n",
    "\n",
    "def n5_word_parser(df):\n",
    "    df[['w1','w2','w3','w4','constituent']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "    \n",
    "    word_df=pd.melt(df,id_vars=constituent_id_vars,value_vars=['w2','w3','w4'],value_name='context')\n",
    "        \n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33ab25-b12c-4b70-b45a-45601f521bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_reducer(df):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # N _ _ _ _\n",
    "        final_word_df=n1_word_parser(df)\n",
    "        \n",
    "    elif pattern==2: # _ N _ _ _\n",
    "        final_word_df=n2_word_parser(df)\n",
    "\n",
    "    elif pattern==3: # _ _ N _ _\n",
    "        final_word_df=n3_word_parser(df)\n",
    "\n",
    "    elif pattern==4: # _ _ _ N _\n",
    "        final_word_df=n4_word_parser(df)\n",
    "\n",
    "    elif pattern==5: # _ _ _ _ N\n",
    "        final_word_df=n5_word_parser(df)\n",
    "\n",
    "    return final_word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9d1db-f81e-4611-bb19-cb8579e517de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extractor(df):\n",
    "    word_df_list=[]\n",
    "\n",
    "    for i in range(1,6):\n",
    "        cur_df=df.loc[df.lemma_pos.str.contains(word_dict[i])].copy()\n",
    "        if cur_df.shape[0]!=0:\n",
    "            cur_df.comp_class=i\n",
    "            cur_word_df=word_reducer(cur_df)\n",
    "            word_df_list.append(cur_word_df)\n",
    "\n",
    "    words=pd.concat(word_df_list,ignore_index=True,sort=False)\n",
    "        \n",
    "    words.dropna(inplace=True)\n",
    "    words=words.groupby(['constituent','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "    words.reset_index(inplace=True)\n",
    "    \n",
    "    return words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c24c9-beec-430c-8aeb-dd054651a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df,phrases=False):\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_partitions)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if not args.word:\n",
    "        \n",
    "        if not phrases:\n",
    "            #Processing heads, modifiers and compounds for Compound Aware\n",
    "\n",
    "            results=pool.map_async(compound_extracter,df_split)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            results=results.get()\n",
    "\n",
    "            print(\"Done parallelizing\")\n",
    "            compound_list = [ result[0] for result in results]\n",
    "            compounds=pd.concat(compound_list,ignore_index=True)\n",
    "            compounds=compounds.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            compounds.reset_index(inplace=True)\n",
    "\n",
    "            modifier_list = [ result[1] for result in results]\n",
    "            modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "            modifiers=modifiers.groupby(['modifier','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            modifiers.reset_index(inplace=True)\n",
    "\n",
    "            head_list = [ result[2] for result in results]\n",
    "            heads=pd.concat(head_list,ignore_index=True)\n",
    "            heads=heads.groupby(['head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            heads.reset_index(inplace=True)\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return compounds,modifiers,heads\n",
    "        else:\n",
    "            \n",
    "            #Processing phrases for Compound Agnostic\n",
    "            results=pool.starmap_async(compound_extracter,zip(df_split,repeat(phrases)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            phrase_list=results.get()\n",
    "            \n",
    "            print(\"Done parallelizing\")\n",
    "            \n",
    "            phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "            phrases=phrases.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            phrases.reset_index(inplace=True)\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return phrases\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Processing words for Compound Agnostic\n",
    "\n",
    "        words_list=[]\n",
    "        results=pool.map_async(word_extractor,df_split)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        words_list=results.get()\n",
    "\n",
    "        print(\"Done parallelizing\")\n",
    "        \n",
    "        words = pd.concat(words_list,ignore_index=True)\n",
    "        words=words.groupby(['constituent','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a63e6-3c17-490f-8c62-d787da85c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_df_processor(f):\n",
    "    t1=time.time()\n",
    "    cur_decade=f.split('.')[0]\n",
    "    print(f'Current decade: {cur_decade}')\n",
    "\n",
    "    cur_dec_df=pd.read_pickle(args.input+f)\n",
    "    if not args.word:\n",
    "        reduced_df=cur_dec_df.loc[cur_dec_df.comp_class!=0].reset_index(drop=True)\n",
    "        compound_df,modifier_df,head_df=parallelize_dataframe(reduced_df)\n",
    "\n",
    "\n",
    "        #Gathering phrases\n",
    "        #Removing compound classes as they are already gathered\n",
    "        #Use old comp_class to now store phrase_class\n",
    "        print(\"Phrases\")\n",
    "\n",
    "        phrase_df=parallelize_dataframe(cur_dec_df,phrases=True)\n",
    "\n",
    "        print(f\"Total time for decade {cur_decade} taken {round(time.time()-t1)} secs\")\n",
    "        return compound_df,modifier_df,head_df,phrase_df\n",
    "    else:\n",
    "        print(\"Words\")\n",
    "        \n",
    "        word_df=parallelize_dataframe(cur_dec_df)\n",
    "        print(f\"Total time for decade {cur_decade} taken {round(time.time()-t1)} secs\")\n",
    "        return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d96fc-c3ec-4abf-96e1-b37960efd607",
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = args.input\n",
    "#_dir = \"/resources/corpora/COHA/CCOHA/tagged/\"\n",
    "#_dir = \"/resources/corpora/COHA/CCOHA/tagged\"\n",
    "files_orig = sorted(os.listdir(_dir))#[1:]\n",
    "print(len(files_orig))\n",
    "files_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f02ec3-4916-4e18-a7f9-134d6c891ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=dec_df_processor(files_orig[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da543e6-b8a3-44e0-9930-5ba8536e06db",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.num_comp.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202d3e9-138d-4a7e-aa63-7da030a32124",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
