{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "from zipfile import ZipFile,ZipInfo\n",
    "import editdistance\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import unicodedata\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "import pickle as pkl\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.max_length=10_000_000\n",
    "#contextualSpellCheck.add_to_pipe(nlp)\n",
    "#from spacy_hunspell import spaCyHunSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(input_zip):\n",
    "    input_zip=ZipFile(input_zip)\n",
    "    return {name: input_zip.read(name).decode('utf-8').strip() for name in input_zip.namelist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_maker(dict_content):\n",
    "    token_list=[]\n",
    "    pos_list=[]\n",
    "    lemma_list=[]\n",
    "    str1=re.sub(\"@@.*|<P>|/\\S*/\",\"\",dict_content)\n",
    "    str2 = re.sub(' +',' ',str1)\n",
    "    str2=str2.replace(\"@ @ @ @ @ @ @ @ @ @\",\"@@@@@@@@@@\")\n",
    "    str2=str2.replace(\"\\n\\n\",\"\")\n",
    "    doc=nlp(str2)\n",
    "    for token in doc:\n",
    "        token_list.append(token.text)\n",
    "        pos_list.append(token.pos_)\n",
    "        lemma_list.append(token.lemma_)\n",
    "    return token_list,pos_list,lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_collecter(decade):\n",
    "    zipfiles=extract_zip(os.path.join(_dir, decade))\n",
    "    zfnames=list(zipfiles.keys())\n",
    "    docs=list(zipfiles.values())\n",
    "    \n",
    "    decade=decade.split('_')[1][:-1]\n",
    "    print(f\"Current decade \"+str(decade))\n",
    "    cur_time=time.time()\n",
    "    n_proc = mp.cpu_count()-1\n",
    "\n",
    "    pool = Pool(n_proc)\n",
    "    results=pool.map_async(vocab_maker,docs)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print(\"Done parallelizing\")\n",
    "    print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "    \n",
    "    results=results.get()\n",
    "    cur_token_list = [val[0] for val in results]\n",
    "    full_token_list = [item for sublist in cur_token_list for item in sublist]\n",
    "\n",
    "    cur_pos_list=[val[1] for val in results]\n",
    "    full_pos_list = [item for sublist in cur_pos_list for item in sublist]\n",
    "\n",
    "    cur_lemma_list=[val[2] for val in results]\n",
    "    full_lemma_list = [item for sublist in cur_lemma_list for item in sublist]\n",
    "\n",
    "    assert len(full_token_list)==len(full_pos_list)==len(full_lemma_list)\n",
    "    \n",
    "    df=pd.DataFrame({'token':full_token_list,'pos':full_pos_list,'lemma':full_lemma_list})\n",
    "    df=df.loc[df.pos.isin(['NOUN','VERB','ADJ','ADV','AUX'])]\n",
    "    df.token=df.token.str.lower()\n",
    "    df.pos=df.pos.str.lower()\n",
    "    df.lemma=df.lemma.str.lower()\n",
    "    df.token=df.token+\"_\"+df.pos\n",
    "    df.lemma=df.lemma+\"_\"+df.pos\n",
    "\n",
    "    token_counter=Counter(df.token.values)\n",
    "    lemma_counter=Counter(df.lemma.values)  \n",
    "    return token_counter,lemma_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(fnames,dec,set_type):\n",
    "    save_file=\"./\"+str(dec)+\"/\"+set_type+\".txt\"\n",
    "    print(save_file)\n",
    "    with open(save_file,'w') as f:\n",
    "        for doc in fnames:\n",
    "            f.write(zipfiles[doc]+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dir = \"/resources/corpora/COHA/text/\"\n",
    "#_dir = \"/resources/corpora/COHA/CCOHA/tagged/\"\n",
    "#_dir = \"/resources/corpora/COHA/ALL/\"\n",
    "files = sorted(os.listdir(_dir))\n",
    "\n",
    "to_keep=[]\n",
    "for f in files:\n",
    "    if 'zip' in f:\n",
    "        to_keep.append(f)\n",
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter=Counter()\n",
    "lemma_counter=Counter()\n",
    "\n",
    "for decade in to_keep:\n",
    "    token_list,lemma_list=vocab_collecter(decade)\n",
    "    token_counter+=token_list\n",
    "    lemma_counter+=lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas=pd.DataFrame(lemma_counter.most_common(len(lemma_counter)))\n",
    "#lemmas.reset_index(inplace=True)\n",
    "lemmas.columns=['lemma','count']\n",
    "lemmas.lemma=lemmas.lemma.str.replace(r'_aux$', r'_verb',regex=True)\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas['lemma'].head(5).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas.head(50_000).to_csv('../Compounding_github/data/coha_context.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump( lemmas['lemma'].head(50_000).to_list(), open( '../Compounding_github/data/coha_context.pkl', \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
