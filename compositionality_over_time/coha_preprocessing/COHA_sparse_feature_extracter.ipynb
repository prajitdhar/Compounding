{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "#sns.set(rc={'figure.figsize':(10,10)})\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "#matplotlib.style.use('ggplot')\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20e8e1",
   "metadata": {},
   "source": [
    "Check formula again\n",
    "\n",
    "Agnostic and Aware - \n",
    "cutoff 20, 50 \n",
    "temporal 0, 10\n",
    "\n",
    "Sparse and Dense - \n",
    "\n",
    "\n",
    "x2 x2 x2 x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_binner(year,val=10):\n",
    "    return year - year%val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e229de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Compute features from sparse dataset')\n",
    "\n",
    "parser.add_argument('--temporal',  type=int,default=0,\n",
    "                    help='Value to bin the temporal information: 0 (remove temporal information), 1 (no binning), 10 (binning to decades), 20 (binning each 20 years) or 50 (binning each 50 years)')\n",
    "\n",
    "parser.add_argument('--cutoff', type=int, default=50,\n",
    "                    help='Cut-off frequency for each compound per time period : none (0), 20, 50 and 100')\n",
    "\n",
    "parser.add_argument('--contextual', action='store_true',\n",
    "                    help='Is the model contextual')\n",
    "parser.add_argument('--inputdir',type=str,\n",
    "                    help='Provide directory where features are located')\n",
    "parser.add_argument('--outputdir',type=str,\n",
    "                    help='Where should the output be stored?')\n",
    "\n",
    "parser.add_argument('--input_format',type=str,default='csv',choices=['csv','pkl'],\n",
    "                    help='In what format are the input files : csv or pkl')\n",
    "parser.add_argument('--save_format', type=str,default='pkl',choices=['pkl','csv'],\n",
    "                    help='In what format should the reduced datasets be saved : csv or pkl')\n",
    "\n",
    "args = parser.parse_args('--input_format pkl --inputdir /home/users0/pageljs/dh/repos/Compounding/datasets/ --cutoff 10 --temporal 10 --outputdir ../Compounding/coha_compounds/'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Cutoff: {args.cutoff}')\n",
    "print(f'Time span:  {args.temporal}')\n",
    "temp_cutoff_str=str(args.temporal)+'_'+str(args.cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0984d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = pickle.load( open( f'{args.inputdir}context.pkl', \"rb\" ) )\n",
    "len(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbefaf-cb87-4ac8-b2b9-1b29868ac587",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.contextual:\n",
    "    context='CompoundAware'\n",
    "else:\n",
    "    context='CompoundAgnostic'\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad61dbc-6ebc-4eb0-a2d8-bb4a0105bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path=context+'_Sparse_'+temp_cutoff_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5577dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.contextual:\n",
    "    print(\"CompoundCentric Model\")\n",
    "\n",
    "    print('Reading compounds')\n",
    "    compounds=pd.read_pickle(args.inputdir+\"/compounds.pkl\")\n",
    "    print(compounds.shape[0])\n",
    "    compounds.context=compounds.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    compounds=compounds.loc[compounds.context.isin(context_list)]\n",
    "    print(compounds.shape[0])\n",
    "    \n",
    "    compounds.modifier=compounds.modifier.str.replace(r'_.+','',regex=True)\n",
    "    compounds['head']=compounds['head'].str.replace(r'_.+','',regex=True)\n",
    "\n",
    "    if args.temporal==0:\n",
    "        print('No temporal information is stored')\n",
    "\n",
    "        #compounds=compounds.loc[~compounds.modifier.str.contains('^(?:of|the|-)_.+')]\n",
    "        #compounds=compounds.loc[~compounds['head'].str.contains('^(?:of|the|-)_.+')]\n",
    "\n",
    "        compounds=compounds.loc[compounds.groupby(['modifier','head'])['count'].transform('sum').gt(args.cutoff)]\n",
    "        print(compounds.shape[0])\n",
    "\n",
    "        compounds=compounds.groupby(['modifier','head','context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "    else:\n",
    "        print(f'Temporal information is stored with intervals {args.temporal}')\n",
    "        compounds.year=compounds.year.astype(\"int32\")\n",
    "        #compounds.query('1800 <= year <= 2010',inplace=True)\n",
    "        compounds['time']=year_binner(compounds['year'].values,args.temporal)\n",
    "        compounds=compounds.loc[compounds.groupby(['modifier','head','time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "        print(compounds.shape[0])\n",
    "\n",
    "        compounds=compounds.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "    \n",
    "    print(compounds.shape[0])    \n",
    "    print('Done reading compounds')\n",
    "    \n",
    "    modifier_lst=compounds.modifier.unique().tolist()\n",
    "    print(f'Number of unique modifiers {len(modifier_lst)}')\n",
    "\n",
    "    head_lst=compounds['head'].unique().tolist()\n",
    "    len(head_lst)    \n",
    "    print(f'Number of unique heads {len(head_lst)}')\n",
    "\n",
    "    print('Reading modifiers')\n",
    "\n",
    "    modifiers=pd.read_pickle(args.inputdir+\"/modifiers.pkl\")\n",
    "    print(modifiers.shape[0])\n",
    "    modifiers.context=modifiers.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    modifiers=modifiers.loc[modifiers.context.isin(context_list)]\n",
    "    print(modifiers.shape[0])\n",
    "    modifiers.modifier=modifiers.modifier.str.replace(r'_.+','',regex=True)\n",
    "\n",
    "    if args.temporal==0:\n",
    "        modifiers=modifiers.groupby(['modifier','context'])['count'].sum().to_frame().reset_index()\n",
    "    else:\n",
    "        modifiers.year=modifiers.year.astype(\"int32\")\n",
    "        #modifiers.query('1800 <= year <= 2010',inplace=True)        \n",
    "        modifiers['time']=year_binner(modifiers['year'].values,args.temporal)\n",
    "        modifiers=modifiers.groupby(['modifier','time','context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "    print(modifiers.shape[0])\n",
    "    \n",
    "    modifiers=modifiers.loc[modifiers.modifier.isin(modifier_lst)]\n",
    "    print(modifiers.shape[0])\n",
    "\n",
    "    print('Done reading modifiers')        \n",
    "\n",
    "    print('Reading heads')\n",
    "\n",
    "    heads=pd.read_pickle(args.inputdir+\"/heads.pkl\")\n",
    "    print(heads.shape[0])\n",
    "    heads.context=heads.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    heads=heads.loc[heads.context.isin(context_list)]\n",
    "    print(heads.shape[0])\n",
    "    heads['head']=heads['head'].str.replace(r'_.+','',regex=True)\n",
    "    \n",
    "    if args.temporal==0:\n",
    "        heads=heads.groupby(['head','context'])['count'].sum().to_frame().reset_index()\n",
    "    else:\n",
    "        heads.year=heads.year.astype(\"int32\")\n",
    "        #heads.query('1800 <= year <= 2010',inplace=True)\n",
    "        heads['time']=year_binner(heads['year'].values,args.temporal)\n",
    "        heads=heads.groupby(['head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "    print(heads.shape[0])\n",
    "    \n",
    "    heads=heads.loc[heads['head'].isin(modifier_lst)]\n",
    "    print(heads.shape[0])\n",
    "    print('Done reading heads') \n",
    "\n",
    "else:\n",
    "    print(\"CompoundAgnostic Model\")\n",
    "    \n",
    "    print('Reading phrases')\n",
    "    compounds=pd.read_pickle(args.inputdir+\"/phrases.pkl\")\n",
    "    print(compounds.shape[0])\n",
    "    compounds.context=compounds.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    compounds=compounds.loc[compounds.context.isin(context_list)]\n",
    "    print(compounds.shape[0])\n",
    "    \n",
    "    compounds.modifier=compounds.modifier.str.replace(r'_.+','',regex=True)\n",
    "    compounds['head']=compounds['head'].str.replace(r'_.+','',regex=True)\n",
    "\n",
    "    if args.temporal==0:\n",
    "        print('No temporal information is stored')\n",
    "\n",
    "        #compounds=compounds.loc[~compounds.modifier.str.contains('^(?:of|the|-)_.+')]\n",
    "        #compounds=compounds.loc[~compounds['head'].str.contains('^(?:of|the|-)_.+')]\n",
    "\n",
    "        compounds=compounds.loc[compounds.groupby(['modifier','head'])['count'].transform('sum').gt(args.cutoff)]\n",
    "        print(compounds.shape[0])\n",
    "\n",
    "        compounds=compounds.groupby(['modifier','head','context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "    else:\n",
    "        print(f'Temporal information is stored with intervals {args.temporal}')\n",
    "        compounds.year=compounds.year.astype(\"int32\")\n",
    "        #compounds.query('1800 <= year <= 2010',inplace=True)\n",
    "        compounds['time']=year_binner(compounds['year'].values,args.temporal)\n",
    "        compounds=compounds.loc[compounds.groupby(['modifier','head','time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "        print(compounds.shape[0])\n",
    "\n",
    "        compounds=compounds.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "        \n",
    "    print(compounds.shape[0])    \n",
    "    print('Done reading compounds')\n",
    "    \n",
    "    constituents_lst=list(set(compounds.modifier.unique().tolist()+compounds['head'].unique().tolist()))\n",
    "    print(f'Number of unique constituents {len(constituents_lst)}')\n",
    "    \n",
    "    print('Reading constituents')\n",
    "    constituents=pd.read_pickle(args.inputdir+\"/words.pkl\")\n",
    "    print(constituents.shape[0])\n",
    "    constituents.context=constituents.context.str.replace(r'.+_NUM','NUM',regex=True)\n",
    "    constituents=constituents.loc[constituents.context.isin(context_list)]\n",
    "    print(constituents.shape[0])\n",
    "    constituents.word=constituents.word.str.replace(r'_.+','',regex=True)\n",
    "    constituents=constituents.loc[constituents.word.isin(constituents_lst)]\n",
    "    print(constituents.shape[0])\n",
    "    \n",
    "    if args.temporal==0:\n",
    "        constituents=constituents.groupby(['word','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        modifiers=constituents.copy()\n",
    "        modifiers.columns=['modifier','context','count']\n",
    "        heads=constituents.copy()\n",
    "        heads.columns=['head','context','count']\n",
    "    else:\n",
    "        constituents.year=constituents.year.astype(\"int32\")\n",
    "        #constituents.query('1800 <= year <= 2010',inplace=True)\n",
    "        constituents['time']=year_binner(constituents['year'].values,args.temporal)\n",
    "        constituents=constituents.groupby(['word','time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        modifiers=constituents.copy()\n",
    "        modifiers.columns=['modifier','time','context','count']\n",
    "        heads=constituents.copy()\n",
    "        heads.columns=['head','time','context','count']\n",
    "    print(constituents.shape[0])\n",
    "    print('Done reading constituents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.temporal==0:\n",
    "    all_comps=compounds[['modifier','head']].copy()\n",
    "    all_comps.drop_duplicates(inplace=True)\n",
    "    mod_prod=all_comps.groupby(['modifier']).size().to_frame()\n",
    "    mod_prod.columns=['mod_prod']\n",
    "    mod_prod['N']=mod_prod['mod_prod'].sum()\n",
    "    mod_prod['mod_family_size']=-np.log2((mod_prod.mod_prod+1)/(mod_prod.N-mod_prod.mod_prod+1))\n",
    "    \n",
    "    head_prod=all_comps.groupby(['head']).size().to_frame()\n",
    "    head_prod.columns=['head_prod']\n",
    "    head_prod['N']=head_prod['head_prod'].sum()\n",
    "    head_prod['head_family_size']=-np.log2((head_prod.head_prod+1)/(head_prod.N-head_prod.head_prod+1))\n",
    "    \n",
    "    prod1=pd.merge(all_comps,mod_prod.reset_index(),how='left',on=['modifier'])\n",
    "    productivity=pd.merge(prod1,head_prod.reset_index(),how='left',on=['head'])\n",
    "    productivity.set_index(['modifier','head'],inplace=True)\n",
    "    productivity.drop(['N_x','N_y'],axis=1,inplace=True)\n",
    "else:\n",
    "\n",
    "    all_comps=compounds[['modifier','head','time']].copy()\n",
    "    all_comps.drop_duplicates(inplace=True)\n",
    "    compound_counts=all_comps.groupby(['time']).size().to_frame()\n",
    "    compound_counts.columns=['N']    \n",
    "    \n",
    "    mod_prod=all_comps.groupby(['modifier','time']).size().to_frame()\n",
    "    mod_prod.columns=['mod_prod']\n",
    "    mod_prod=pd.merge(mod_prod.reset_index(),compound_counts.reset_index(),on=['time'],how='left')\n",
    "    mod_prod['mod_family_size']=-np.log2((mod_prod.mod_prod+1)/(mod_prod.N-mod_prod.mod_prod+1))\n",
    "    \n",
    "    \n",
    "    head_prod=all_comps.groupby(['head','time']).size().to_frame()\n",
    "    head_prod.columns=['head_prod']\n",
    "    head_prod=pd.merge(head_prod.reset_index(),compound_counts.reset_index(),on=['time'],how='left')\n",
    "    head_prod['head_family_size']=-np.log2((head_prod.head_prod+1)/(head_prod.N-head_prod.head_prod+1))\n",
    "    \n",
    "    prod1=pd.merge(all_comps,mod_prod,how='left',on=['modifier','time'])\n",
    "    productivity=pd.merge(prod1,head_prod,how='left',on=['head','time'])\n",
    "    productivity.set_index(['modifier','head','time'],inplace=True)\n",
    "    productivity.drop(['N_x','N_y'],axis=1,inplace=True)\n",
    "productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.temporal!=0:\n",
    "    \n",
    "    compound_decade_counts=compounds.groupby('time').sum().sum(axis=1).to_frame()\n",
    "    compound_decade_counts.columns=['N']\n",
    "\n",
    "    XY=compounds.groupby(['modifier','head','time']).sum().sum(axis=1).to_frame()\n",
    "    X_star=compounds.groupby(['modifier','time']).sum().sum(axis=1).to_frame()\n",
    "    Y_star=compounds.groupby(['head','time']).sum().sum(axis=1).to_frame()\n",
    "\n",
    "\n",
    "else:\n",
    "    XY=compounds.groupby(['modifier','head']).sum().sum(axis=1).to_frame()\n",
    "    X_star=compounds.groupby(['modifier']).sum().sum(axis=1).to_frame()\n",
    "    Y_star=compounds.groupby(['head']).sum().sum(axis=1).to_frame()\n",
    "\n",
    "XY.columns=['a']\n",
    "\n",
    "X_star.columns=['x_star']\n",
    "Y_star.columns=['star_y']\n",
    "\n",
    "if args.temporal!=0:\n",
    " \n",
    "    merge1=pd.merge(XY.reset_index(),X_star.reset_index(),on=['modifier','time'])\n",
    "\n",
    "    information_feat=pd.merge(merge1,Y_star.reset_index(),on=['head','time'])\n",
    "else:\n",
    "    merge1=pd.merge(XY.reset_index(),X_star.reset_index(),on=['modifier'])\n",
    "\n",
    "    information_feat=pd.merge(merge1,Y_star.reset_index(),on=['head'])    \n",
    "\n",
    "information_feat['b']=information_feat['x_star']-information_feat['a']\n",
    "information_feat['c']=information_feat['star_y']-information_feat['a']\n",
    "\n",
    "if args.temporal!=0:\n",
    "    information_feat=pd.merge(information_feat,compound_decade_counts.reset_index(),on=['time'])\n",
    "\n",
    "else: \n",
    "    information_feat['N']=compounds['count'].sum()\n",
    "    \n",
    "\n",
    "information_feat['d']=information_feat['N']-(information_feat['a']+information_feat['b']+information_feat['c'])\n",
    "information_feat['x_bar_star']=information_feat['N']-information_feat['x_star']\n",
    "information_feat['star_y_bar']=information_feat['N']-information_feat['star_y']\n",
    "\n",
    "if args.temporal!=0:\n",
    "\n",
    "    information_feat.set_index(['modifier','head','time'],inplace=True)\n",
    "else:\n",
    "    information_feat.set_index(['modifier','head'],inplace=True)\n",
    "    \n",
    "information_feat['log_ratio']=2*(information_feat['a']*np.log2((information_feat['a']*information_feat['N']+1)/(information_feat['x_star']*information_feat['star_y']+1))+\\\n",
    "information_feat['b']*np.log2((information_feat['b']*information_feat['N']+1)/(information_feat['x_star']*information_feat['star_y_bar']+1))+\\\n",
    "information_feat['c']*np.log2((information_feat['c']*information_feat['N']+1)/(information_feat['x_bar_star']*information_feat['star_y']+1))+\\\n",
    "information_feat['d']*np.log2((information_feat['d']*information_feat['N']+1)/(information_feat['x_bar_star']*information_feat['star_y_bar']+1)))\n",
    "information_feat['ppmi']=np.log2((information_feat['a']*information_feat['N']+1)/(information_feat['x_star']*information_feat['star_y']))\n",
    "information_feat['local_mi']=information_feat['a']*information_feat['ppmi']\n",
    "information_feat.ppmi.loc[information_feat.ppmi<=0]=0\n",
    "information_feat.drop(['a','x_star','star_y','b','c','d','N','d','x_bar_star','star_y_bar'],axis=1,inplace=True)\n",
    "information_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1170c0d-6a0d-48e2-b026-301dd33eae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=information_feat.hist(by='time',column ='local_mi', figsize=(10, 10),sharex=True,sharey=True,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=information_feat.hist(column ='ppmi', figsize=(10, 10),bins=100,sharex=True,sharey=True,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "information_feat.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc08075",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=information_feat.hist(column ='log_ratio', figsize=(10, 10),bins=100,sharex=True,sharey=True,density=True,range=(-0.1,5_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=information_feat.hist(column ='local_mi', figsize=(10, 10),bins=100,sharex=True,sharey=True,density=True,range=(-0.1,5_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e57926-9fb7-42c1-8d75-1fd2ea315200",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.temporal!=0:\n",
    " \n",
    "    merge1=pd.merge(XY.reset_index(),X_star.reset_index(),on=['modifier','time'])\n",
    "\n",
    "    frequency=pd.merge(merge1,Y_star.reset_index(),on=['head','time'])\n",
    "    frequency.set_index(['modifier','head','time'],inplace=True)\n",
    "else:\n",
    "    merge1=pd.merge(XY.reset_index(),X_star.reset_index(),on=['modifier'])\n",
    "\n",
    "    frequency=pd.merge(merge1,Y_star.reset_index(),on=['head'])\n",
    "    frequency.set_index(['modifier','head'],inplace=True)\n",
    "\n",
    "frequency.columns=['comp_freq','mod_freq','head_freq']\n",
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b568099",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.temporal==0:\n",
    "\n",
    "    compound_denom=compounds.groupby(['modifier','head'])['count'].agg(lambda x: np.sqrt(np.sum(np.square(x)))).to_frame()\n",
    "    compound_denom.columns=['compound_denom']\n",
    "    \n",
    "    modifier_denom=modifiers.groupby(['modifier'])['count'].agg(lambda x: np.sqrt(np.sum(np.square(x)))).to_frame()\n",
    "    modifier_denom.columns=['modifier_denom']\n",
    "\n",
    "    head_denom=heads.groupby(['head'])['count'].agg(lambda x: np.sqrt(np.sum(np.square(x)))).to_frame()\n",
    "    head_denom.columns=['head_denom']\n",
    "else:\n",
    "\n",
    "    compound_denom=compounds.groupby(['modifier','head','time'])['count'].agg(lambda x: np.sqrt(np.sum(np.square(x)))).to_frame()\n",
    "    compound_denom.columns=['compound_denom']\n",
    "\n",
    "    modifier_denom=modifiers.groupby(['modifier','time'])['count'].agg(lambda x: np.sqrt(np.sum(np.square(x)))).to_frame()\n",
    "    modifier_denom.columns=['modifier_denom']\n",
    "    \n",
    "    head_denom=heads.groupby(['head','time'])['count'].agg(lambda x: np.sqrt(np.sum(np.square(x)))).to_frame()\n",
    "    head_denom.columns=['head_denom']\n",
    "\n",
    "mod_cols=modifiers.columns.tolist()\n",
    "mod_cols[-1]=\"mod_count\"\n",
    "modifiers.columns=mod_cols\n",
    "\n",
    "\n",
    "\n",
    "head_cols=heads.columns.tolist()\n",
    "head_cols[-1]=\"head_count\"\n",
    "heads.columns=head_cols\n",
    "\n",
    "#compounds.drop(['comp_count'],axis=1,inplace=True)\n",
    "comp_cols=compounds.columns.tolist()\n",
    "comp_cols[-1]=\"comp_count\"\n",
    "compounds.columns=comp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb0e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.temporal==0:\n",
    "    \n",
    "    compound_modifier_sim=pd.merge(compounds,modifiers,on=[\"modifier\",\"context\"])\n",
    "    compound_modifier_sim['numerator']=compound_modifier_sim['comp_count']*compound_modifier_sim['mod_count']\n",
    "    compound_modifier_sim=compound_modifier_sim.groupby(['modifier','head'])['numerator'].sum().to_frame()\n",
    "    compound_modifier_sim=pd.merge(compound_modifier_sim.reset_index(),compound_denom.reset_index(),on=[\"modifier\",\"head\"])\n",
    "    compound_modifier_sim=pd.merge(compound_modifier_sim,modifier_denom.reset_index(),on=['modifier'])\n",
    "    compound_modifier_sim['sim_with_modifier']=compound_modifier_sim['numerator']/(compound_modifier_sim['compound_denom']*compound_modifier_sim['modifier_denom'])\n",
    "    compound_modifier_sim.set_index(['modifier','head'],inplace=True)\n",
    "    compound_modifier_sim.drop(['numerator','compound_denom'],axis=1,inplace=True)\n",
    "else:\n",
    "    mod_cols=modifiers.columns.tolist()\n",
    "    mod_cols[-1]=\"mod_count\"\n",
    "    modifiers.columns=mod_cols\n",
    "    #compounds.drop(['comp_count'],axis=1,inplace=True)\n",
    "    comp_cols=compounds.columns.tolist()\n",
    "    comp_cols[-1]=\"comp_count\"\n",
    "    compounds.columns=comp_cols\n",
    "    compound_modifier_sim=pd.merge(compounds,modifiers,on=[\"modifier\",\"context\",'time'])\n",
    "    compound_modifier_sim['numerator']=compound_modifier_sim['comp_count']*compound_modifier_sim['mod_count']\n",
    "    compound_modifier_sim=compound_modifier_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "    compound_modifier_sim=pd.merge(compound_modifier_sim.reset_index(),compound_denom.reset_index(),on=[\"modifier\",\"head\",'time'])\n",
    "    compound_modifier_sim=pd.merge(compound_modifier_sim,modifier_denom.reset_index(),on=['modifier','time'])\n",
    "    compound_modifier_sim['sim_with_modifier']=compound_modifier_sim['numerator']/(compound_modifier_sim['compound_denom']*compound_modifier_sim['modifier_denom'])\n",
    "    compound_modifier_sim.set_index(['modifier','head','time'],inplace=True)\n",
    "    compound_modifier_sim.drop(['numerator','compound_denom'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=compound_modifier_sim.hist(column ='sim_with_modifier', figsize=(10, 10),bins=100,sharex=True,sharey=True,density=True,range=(-0.1,1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_modifier_sim.sim_with_modifier.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.temporal==0:\n",
    "    \n",
    "    compound_head_sim=pd.merge(compounds,heads,on=[\"head\",\"context\"])\n",
    "    compound_head_sim['numerator']=compound_head_sim['comp_count']*compound_head_sim['head_count']\n",
    "    compound_head_sim=compound_head_sim.groupby(['modifier','head'])['numerator'].sum().to_frame()\n",
    "    compound_head_sim=pd.merge(compound_head_sim.reset_index(),compound_denom.reset_index(),on=[\"modifier\",\"head\"])\n",
    "    compound_head_sim=pd.merge(compound_head_sim,head_denom.reset_index(),on=['head'])\n",
    "    compound_head_sim['sim_with_head']=compound_head_sim['numerator']/(compound_head_sim['compound_denom']*compound_head_sim['head_denom'])\n",
    "    compound_head_sim.set_index(['modifier','head'],inplace=True)\n",
    "    compound_head_sim.drop(['numerator','compound_denom'],axis=1,inplace=True)\n",
    "else:\n",
    "    compound_head_sim=pd.merge(compounds,heads,on=[\"head\",\"context\",'time'])\n",
    "    compound_head_sim['numerator']=compound_head_sim['comp_count']*compound_head_sim['head_count']\n",
    "    compound_head_sim=compound_head_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "    compound_head_sim=pd.merge(compound_head_sim.reset_index(),compound_denom.reset_index(),on=[\"modifier\",\"head\",'time'])\n",
    "    compound_head_sim=pd.merge(compound_head_sim,head_denom.reset_index(),on=['head','time'])\n",
    "    compound_head_sim['sim_with_head']=compound_head_sim['numerator']/(compound_head_sim['compound_denom']*compound_head_sim['head_denom'])\n",
    "    compound_head_sim.set_index(['modifier','head','time'],inplace=True)\n",
    "    compound_head_sim.drop(['numerator','compound_denom'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e0afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=compound_head_sim.hist(column ='sim_with_head', figsize=(10, 10),bins=100,sharex=True,sharey=True,density=True,range=(-0.1,1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb60bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.temporal==0:\n",
    "    \n",
    "    constituent_sim=pd.merge(heads,compounds,on=[\"head\",\"context\"])\n",
    "    #constituent_sim.drop('comp_count',axis=1,inplace=True)\n",
    "    constituent_sim=pd.merge(constituent_sim,modifiers,on=[\"modifier\",\"context\"])\n",
    "    constituent_sim['numerator']=constituent_sim['head_count']*constituent_sim['mod_count']\n",
    "    constituent_sim=constituent_sim.groupby(['modifier','head'])['numerator'].sum().to_frame()\n",
    "    constituent_sim=pd.merge(constituent_sim.reset_index(),head_denom.reset_index(),on=[\"head\"])\n",
    "    constituent_sim=pd.merge(constituent_sim,modifier_denom.reset_index(),on=[\"modifier\"])\n",
    "    constituent_sim['sim_bw_constituents']=constituent_sim['numerator']/(constituent_sim['head_denom']*constituent_sim['modifier_denom'])\n",
    "    constituent_sim.set_index(['modifier','head'],inplace=True)\n",
    "    constituent_sim.drop(['numerator','modifier_denom','head_denom'],axis=1,inplace=True)\n",
    "else:\n",
    "    constituent_sim=pd.merge(heads,compounds,on=[\"head\",\"context\",\"time\"])\n",
    "    #constituent_sim.drop('comp_count',axis=1,inplace=True)\n",
    "    constituent_sim=pd.merge(constituent_sim,modifiers,on=[\"modifier\",\"context\",\"time\"])\n",
    "    constituent_sim['numerator']=constituent_sim['head_count']*constituent_sim['mod_count']\n",
    "    constituent_sim=constituent_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "    constituent_sim=pd.merge(constituent_sim.reset_index(),head_denom.reset_index(),on=[\"head\",\"time\"])\n",
    "    constituent_sim=pd.merge(constituent_sim,modifier_denom.reset_index(),on=[\"modifier\",\"time\"])\n",
    "    constituent_sim['sim_bw_constituents']=constituent_sim['numerator']/(constituent_sim['head_denom']*constituent_sim['modifier_denom'])\n",
    "    constituent_sim.set_index(['modifier','head','time'],inplace=True)\n",
    "    constituent_sim.drop(['numerator','modifier_denom','head_denom'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdefc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=constituent_sim.hist(column ='sim_bw_constituents', figsize=(10, 10),bins=100,sharex=True,sharey=True,density=True,range=(-0.1,1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda78d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [constituent_sim, compound_head_sim, compound_modifier_sim, information_feat,frequency,productivity]\n",
    "compounds_final = reduce(lambda left,right: pd.merge(left,right,left_index=True, right_index=True), dfs)\n",
    "\n",
    "\n",
    "\n",
    "if args.temporal!=0:\n",
    "    compounds_final=pd.pivot_table(compounds_final.reset_index(), index=['modifier','head'], columns=['time'])\n",
    "\n",
    "    compounds_final.fillna(0,inplace=True)\n",
    "    #compounds_final -= compounds_final.min()\n",
    "    #compounds_final /= compounds_final.max()\n",
    "    compounds_final_1=compounds_final.columns.get_level_values(0)\n",
    "    compounds_final_2=compounds_final.columns.get_level_values(1)\n",
    "\n",
    "    cur_year=0\n",
    "    new_columns=[]\n",
    "    for year in compounds_final_2:\n",
    "        new_columns.append(str(year)+\"_\"+compounds_final_1[cur_year])\n",
    "        cur_year+=1\n",
    "    compounds_final.columns=new_columns\n",
    "\n",
    "\n",
    "else:\n",
    "    #compounds_final = reduce(lambda left,right: pd.merge(left,right,on=['modifier','head']), dfs)\n",
    "    #compounds_final.drop(['head_denom','modifier_denom'],axis=1,inplace=True)\n",
    "    compounds_final.fillna(0,inplace=True)\n",
    "    #compounds_final -= compounds_final.min()\n",
    "    #compounds_final /= compounds_final.max()\n",
    "compounds_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822d65e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds_final.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224749cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddy11_study=pd.read_csv(f\"/home/users0/pageljs/dh/repos/Compounding/ijcnlp_compositionality_data/MeanAndDeviations.clean.txt\",sep=\"\\t\")\n",
    "#print(reddy11_study.columns)\n",
    "reddy11_study.columns=['compound','to_divide']\n",
    "\n",
    "reddy11_study[['modifier_mean','modifier_std','head_mean','head_std','compound_mean','compound_std','ww']]=reddy11_study.to_divide.str.split(\" \", n=7, expand=True)\n",
    "reddy11_study[['modifier','head']]=reddy11_study['compound'].str.split(\" \",2, expand=True)\n",
    "reddy11_study.modifier=reddy11_study.modifier.str[:-2]\n",
    "reddy11_study['head']=reddy11_study['head'].str[:-2]\n",
    "reddy11_study.drop(['compound','to_divide','ww'],axis=1,inplace=True)\n",
    "#reddy11_study['modifier']=np.vectorize(lemma_maker)(reddy11_study['modifier'],'noun')\n",
    "#reddy11_study['head']=np.vectorize(lemma_maker)(reddy11_study['head'],'noun')\n",
    "#reddy11_study.replace(spelling_replacement,inplace=True)\n",
    "#reddy11_study['modifier']=reddy11_study['modifier']+\"_noun\"\n",
    "#reddy11_study['head']=reddy11_study['head']+\"_noun\"\n",
    "reddy11_study=reddy11_study.apply(pd.to_numeric, errors='ignore')\n",
    "#reddy11_study.set_index(['modifier','head'],inplace=True)\n",
    "reddy11_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df=reddy11_study.merge(compounds_final.reset_index(),on=['modifier','head'],how='inner')\n",
    "merge_df.set_index([\"modifier\", \"head\"], inplace = True)\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b038e-23f2-4f9d-a8f3-a49dcb35a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compounds=pd.read_csv(f\"/home/users0/pageljs/dh/repos/Compounding_github/data/all_compounds.txt\",sep=\"\\t\")\n",
    "all_compounds.modifier=all_compounds.modifier.str.split('_',n=1,expand=True)[[0]]\n",
    "all_compounds['head']=all_compounds['head'].str.split('_',n=1,expand=True)[[0]]\n",
    "all_compounds=all_compounds.apply(pd.to_numeric, errors='ignore')\n",
    "all_compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d22fd-9a34-4e7b-a7b2-2bd21313105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_210_df=all_compounds.merge(compounds_final.reset_index(),on=['modifier','head'],how='inner')\n",
    "merge_210_df.set_index([\"modifier\", \"head\"], inplace = True)\n",
    "merge_210_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff20aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=merge_210_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.compound_mean.sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv('/home/users0/pageljs/dh/repos/Compounding/coha_compounds/trial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f6151-ee69-48a2-b2f0-5bdd6860d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense\n",
    "\n",
    "Frequency\n",
    "\n",
    "Probeer dein Code zu benutzen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
