{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#import seaborn as sns\n",
    "#sns.set(style=\"whitegrid\", font_scale = 2.5)\n",
    "#sns.set_context(rc={\"lines.markersize\": 17, \"lines.linewidth\": 2})\n",
    "\n",
    "import matplotlib.pyplot as plt      \n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number number NOUN NN nsubj Xxxxx True False\n",
      "crunching crunching NOUN NN acl xxxx True False\n",
      "is be AUX VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "term term NOUN NN attr xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "processing process VERB VBG pcomp xxxx True False\n",
      "numerical numerical ADJ JJ amod xxxx True False\n",
      "data datum NOUN NNS dobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "which which PRON WDT pobj xxxx True True\n",
      "to to PART TO aux xx True True\n",
      "draw draw VERB VB advcl xxxx True False\n",
      "conclusions conclusion NOUN NNS dobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "provides provide VERB VBZ conj xxxx True False\n",
      "context context NOUN NN dobj xxxx True False\n",
      "to to ADP IN prep xx True True\n",
      "business business NOUN NN compound xxxx True False\n",
      "situations situation NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "decisions decision NOUN NNS conj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "results result NOUN NNS conj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "Number number NOUN NN compound Xxxxx True False\n",
      "crunching crunching NOUN NN nsubj xxxx True False\n",
      "is be AUX VBZ ROOT xx True True\n",
      "an an DET DT det xx True True\n",
      "essential essential ADJ JJ amod xxxx True False\n",
      "part part NOUN NN attr xxxx True True\n",
      "of of ADP IN prep xx True True\n",
      "producing produce VERB VBG pcomp xxxx True False\n",
      "objective objective ADJ JJ amod xxxx True False\n",
      "data datum NOUN NNS dobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Number crunching is a term for processing numerical data from which to draw conclusions and provides context to business situations, decisions and results. Number crunching is an essential part of producing objective data.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET DT det Xxx True True\n",
      "spinning spin VERB VBG amod xxxx True False\n",
      "jenny jenny PROPN NNP nsubj xxxx True False\n",
      "is be AUX VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "multi multi ADJ JJ amod xxxx True False\n",
      "- - ADJ JJ amod - False False\n",
      "spindle spindle ADJ JJ amod xxxx True False\n",
      "spinning spinning NOUN NN amod xxxx True False\n",
      "frame frame NOUN NN attr xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "and and CCONJ CC cc xxx True True\n",
      "was be AUX VBD conj xxx True True\n",
      "one one NUM CD attr xxx True True\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "key key ADJ JJ amod xxx True False\n",
      "developments development NOUN NNS pobj xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "industrialisation industrialisation NOUN NN pobj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "textile textile NOUN NN compound xxxx True False\n",
      "manufacturing manufacturing NOUN NN pobj xxxx True False\n",
      "during during ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "early early ADJ JJ amod xxxx True False\n",
      "Industrial Industrial PROPN NNP compound Xxxxx True False\n",
      "Revolution Revolution PROPN NNP pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "It it PRON PRP nsubjpass Xx True True\n",
      "was be AUX VBD auxpass xxx True True\n",
      "invented invent VERB VBN ROOT xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "1764 1764 NUM CD pobj dddd False False\n",
      "- - SYM SYM punct - False False\n",
      "1765 1765 NUM CD prep dddd False False\n",
      "by by ADP IN agent xx True True\n",
      "James James PROPN NNP compound Xxxxx True False\n",
      "Hargreaves Hargreaves PROPN NNP pobj Xxxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "Stan Stan PROPN NNP compound Xxxx True False\n",
      "hill hill PROPN NNP pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "Oswaldtwistle Oswaldtwistle PROPN NNP conj Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "Lancashire Lancashire PROPN NNP conj Xxxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "England England PROPN NNP pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The spinning jenny is a multi-spindle spinning frame, and was one of the key developments in the industrialisation of textile manufacturing during the early Industrial Revolution. It was invented in 1764-1765 by James Hargreaves in Stan hill, Oswaldtwistle, Lancashire in England.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the DET DT det xxx True True\n",
      "smoking smoking NOUN NN compound xxxx True False\n",
      "jacket jacket NOUN NN ROOT xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"the smoking jacket.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He he PRON PRP nsubj Xx True True\n",
      "knew know VERB VBD ROOT xxxx True False\n",
      "his his PRON PRP$ poss xxx True True\n",
      "place place NOUN NN dobj xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "pecking pecking NOUN NN compound xxxx True False\n",
      "order order NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He knew his place in the pecking order.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He he PRON PRP nsubj Xx True True\n",
      "is be AUX VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "closed closed ADJ JJ amod xxxx True False\n",
      "book book NOUN NN attr xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"He is a closed book\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Compute temporal variation features from sparse dataset for google version')\n",
    "\n",
    "parser.add_argument('--inputdir',type=str,\n",
    "                    help='Provide directory where features are located')\n",
    "parser.add_argument('--outputdir',type=str,\n",
    "                    help='Where should the output be stored?')\n",
    "parser.add_argument('--tag', action='store_true',\n",
    "                    help='Should the POS tag be kept?')\n",
    "parser.add_argument('--ppmi', action='store_true',\n",
    "                    help='Should co-occurence matrix be converted to PPMI values')\n",
    "parser.add_argument('--temporal',  type=int,\n",
    "                    help='Value to bin the temporal information: 10000 (remove temporal information), 1 (no binning), 10 (binning to decades), 20 (binning each 20 years) or 50 (binning each 50 years)')\n",
    "parser.add_argument('--cutoff', type=int, default=0,\n",
    "                    help='Cut-off frequency for each compound per time period : none (0), 20, 50 and 100')\n",
    "args = parser.parse_args('--inputdir /work/dhar/data/Compounding/ --outputdir /work/dhar/data/Compounding/google/ --temporal 100'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddy_df=pd.read_csv('data/reddy_90.txt',sep='\\t')\n",
    "reddy_df['source']='reddy'\n",
    "\n",
    "\n",
    "cordeiro90_df=pd.read_csv('data/cordeiro_90.txt',sep='\\t')\n",
    "cordeiro90_df['source']='cordeiro90'\n",
    "\n",
    "\n",
    "cordeiro100_df=pd.read_csv('data/cordeiro_100.txt',sep='\\t')\n",
    "cordeiro100_df['source']='cordeiro100'\n",
    "\n",
    "    \n",
    "comp_ratings_df=pd.concat([reddy_df,cordeiro90_df,cordeiro100_df])\n",
    "#comp_ratings_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset_tagger(df):\n",
    "\n",
    "    #### NOUN NOUN\n",
    "    \n",
    "    copy_df_1=df.copy()\n",
    "    copy_df_1.modifier=copy_df_1.modifier+'_NOUN'\n",
    "    copy_df_1['head']=copy_df_1['head']+'_NOUN'\n",
    "\n",
    "    ### PROPN NOUN\n",
    "\n",
    "    copy_df_2=df.copy()\n",
    "    copy_df_2.modifier=copy_df_2.modifier+'_PROPN'\n",
    "    copy_df_2['head']=copy_df_2['head']+'_NOUN'\n",
    "    \n",
    "    ### NOUN PROPN\n",
    "\n",
    "    copy_df_3=df.copy()\n",
    "    copy_df_3.modifier=copy_df_3.modifier+'_NOUN'\n",
    "    copy_df_3['head']=copy_df_3['head']+'_PROPN'\n",
    "    \n",
    "    ### PROPN PROPN    \n",
    "\n",
    "    copy_df_4=df.copy()\n",
    "    copy_df_4.modifier=copy_df_4.modifier+'_PROPN'\n",
    "    copy_df_4['head']=copy_df_4['head']+'_PROPN'\n",
    "    \n",
    "   \n",
    "    ### ADJ/NOUN NOUN\n",
    "    \n",
    "    copy_df_5=df.copy()\n",
    "    \n",
    "    copy_df_5.loc[copy_df_5.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_5.loc[copy_df_5.is_adj==False,\"modifier\"]+=\"_NOUN\"\n",
    "    copy_df_5['head']=copy_df_5['head']+'_NOUN'   \n",
    "    \n",
    "    \n",
    "    ### ADJ/NOUN PROPN\n",
    "    \n",
    "    copy_df_6=df.copy()\n",
    "    copy_df_6.loc[copy_df_6.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_6.loc[copy_df_6.is_adj==False,\"modifier\"]+=\"_NOUN\"\n",
    "    copy_df_6['head']=copy_df_6['head']+'_PROPN'  \n",
    "\n",
    "    \n",
    "    #### ADJ/PROPN NOUN\n",
    "    \n",
    "    copy_df_7=df.copy()\n",
    "    copy_df_7.loc[copy_df_7.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_7.loc[copy_df_7.is_adj==False,\"modifier\"]+=\"_PROPN\"\n",
    "    copy_df_7['head']=copy_df_7['head']+'_NOUN' \n",
    "    \n",
    "    \n",
    "    #### ADJ/PROPN PROPN\n",
    "    \n",
    "    copy_df_8=df.copy()\n",
    "    copy_df_8.loc[copy_df_8.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_8.loc[copy_df_8.is_adj==False,\"modifier\"]+=\"_PROPN\"\n",
    "    copy_df_8['head']=copy_df_8['head']+'_PROPN' \n",
    "    \n",
    "    \n",
    "    complete_df=pd.concat([copy_df_1,copy_df_2,copy_df_3,copy_df_4,copy_df_5,copy_df_6,copy_df_7,copy_df_8],ignore_index=True)\n",
    "                           \n",
    "    return complete_df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>head</th>\n",
       "      <th>avgModifier</th>\n",
       "      <th>stdevModifier</th>\n",
       "      <th>avgHead</th>\n",
       "      <th>stdevHead</th>\n",
       "      <th>compositionality</th>\n",
       "      <th>stdevHeadModifier</th>\n",
       "      <th>is_adj</th>\n",
       "      <th>compound</th>\n",
       "      <th>source</th>\n",
       "      <th>is_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end</td>\n",
       "      <td>user</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>1.117537</td>\n",
       "      <td>4.866667</td>\n",
       "      <td>0.339935</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.871165</td>\n",
       "      <td>False</td>\n",
       "      <td>end_user</td>\n",
       "      <td>reddy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>firing</td>\n",
       "      <td>line</td>\n",
       "      <td>1.607143</td>\n",
       "      <td>1.654848</td>\n",
       "      <td>1.892857</td>\n",
       "      <td>1.496169</td>\n",
       "      <td>1.703704</td>\n",
       "      <td>1.717337</td>\n",
       "      <td>False</td>\n",
       "      <td>firing_line</td>\n",
       "      <td>reddy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>game</td>\n",
       "      <td>plan</td>\n",
       "      <td>2.821429</td>\n",
       "      <td>1.964935</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>3.827586</td>\n",
       "      <td>1.233693</td>\n",
       "      <td>False</td>\n",
       "      <td>game_plan</td>\n",
       "      <td>reddy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>application</td>\n",
       "      <td>form</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>0.422953</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.476095</td>\n",
       "      <td>False</td>\n",
       "      <td>application_form</td>\n",
       "      <td>reddy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snail</td>\n",
       "      <td>mail</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>4.586207</td>\n",
       "      <td>1.099129</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>1.020596</td>\n",
       "      <td>False</td>\n",
       "      <td>snail_mail</td>\n",
       "      <td>reddy</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>wedding</td>\n",
       "      <td>day</td>\n",
       "      <td>4.764700</td>\n",
       "      <td>0.562300</td>\n",
       "      <td>4.058800</td>\n",
       "      <td>1.434900</td>\n",
       "      <td>4.941200</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>False</td>\n",
       "      <td>wedding_day</td>\n",
       "      <td>cordeiro100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>white</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>1.112300</td>\n",
       "      <td>4.043500</td>\n",
       "      <td>1.429500</td>\n",
       "      <td>1.173900</td>\n",
       "      <td>1.230400</td>\n",
       "      <td>True</td>\n",
       "      <td>white_noise</td>\n",
       "      <td>cordeiro100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>white</td>\n",
       "      <td>spirit</td>\n",
       "      <td>1.538500</td>\n",
       "      <td>1.240300</td>\n",
       "      <td>2.038500</td>\n",
       "      <td>1.949000</td>\n",
       "      <td>1.307700</td>\n",
       "      <td>1.257600</td>\n",
       "      <td>True</td>\n",
       "      <td>white_spirit</td>\n",
       "      <td>cordeiro100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>winter</td>\n",
       "      <td>solstice</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.681800</td>\n",
       "      <td>1.086100</td>\n",
       "      <td>4.545500</td>\n",
       "      <td>1.335500</td>\n",
       "      <td>False</td>\n",
       "      <td>winter_solstice</td>\n",
       "      <td>cordeiro100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>world</td>\n",
       "      <td>conference</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.361300</td>\n",
       "      <td>4.291700</td>\n",
       "      <td>1.267600</td>\n",
       "      <td>3.958300</td>\n",
       "      <td>1.366700</td>\n",
       "      <td>False</td>\n",
       "      <td>world_conference</td>\n",
       "      <td>cordeiro100</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        modifier        head  avgModifier  stdevModifier   avgHead  stdevHead  \\\n",
       "0            end        user     3.866667       1.117537  4.866667   0.339935   \n",
       "1         firing        line     1.607143       1.654848  1.892857   1.496169   \n",
       "2           game        plan     2.821429       1.964935  4.862069   0.344828   \n",
       "3    application        form     4.766667       0.422953  4.862069   0.344828   \n",
       "4          snail        mail     0.600000       0.800000  4.586207   1.099129   \n",
       "..           ...         ...          ...            ...       ...        ...   \n",
       "97       wedding         day     4.764700       0.562300  4.058800   1.434900   \n",
       "98         white       noise     0.652200       1.112300  4.043500   1.429500   \n",
       "99         white      spirit     1.538500       1.240300  2.038500   1.949000   \n",
       "100       winter    solstice     5.000000       0.000000  4.681800   1.086100   \n",
       "101        world  conference     3.875000       1.361300  4.291700   1.267600   \n",
       "\n",
       "     compositionality  stdevHeadModifier  is_adj          compound  \\\n",
       "0            4.250000           0.871165   False          end_user   \n",
       "1            1.703704           1.717337   False       firing_line   \n",
       "2            3.827586           1.233693   False         game_plan   \n",
       "3            4.800000           0.476095   False  application_form   \n",
       "4            1.310345           1.020596   False        snail_mail   \n",
       "..                ...                ...     ...               ...   \n",
       "97           4.941200           0.242500   False       wedding_day   \n",
       "98           1.173900           1.230400    True       white_noise   \n",
       "99           1.307700           1.257600    True      white_spirit   \n",
       "100          4.545500           1.335500   False   winter_solstice   \n",
       "101          3.958300           1.366700   False  world_conference   \n",
       "\n",
       "          source  is_original  \n",
       "0          reddy         True  \n",
       "1          reddy         True  \n",
       "2          reddy         True  \n",
       "3          reddy         True  \n",
       "4          reddy         True  \n",
       "..           ...          ...  \n",
       "97   cordeiro100         True  \n",
       "98   cordeiro100         True  \n",
       "99   cordeiro100         True  \n",
       "100  cordeiro100         True  \n",
       "101  cordeiro100         True  \n",
       "\n",
       "[287 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.tag:\n",
    "    comp_ratings_df=testset_tagger(comp_ratings_df)\n",
    "comp_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_decades_compound(dec_list,input_dir,unique_mod_list,unique_head_list,ctype='compound'):\n",
    "\n",
    "    if os.path.exists(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\"):\n",
    "        print('Reading file')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\",compression=\"bz2\")\n",
    "        \n",
    "    elif os.path.exists(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl.bz2\") and args.temporal!=10000:\n",
    "        print(f'Reading decades file {ctype}s/10_{dec_list[0]}_{tag_str}.pkl.bz2')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl.bz2\",compression=\"bz2\")\n",
    "        \n",
    "        print(f'Reducing to {args.temporal}')\n",
    "        complete_df['time']=complete_df['time']-complete_df['time']%args.temporal\n",
    "\n",
    "        complete_df=complete_df.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\")\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_list=[]\n",
    "\n",
    "        for dec in dec_list:\n",
    "            print(dec)\n",
    "            cur_df=pd.read_pickle(f'{input_dir}/{ctype}s/{dec}.pkl.bz2')\n",
    "            \n",
    "            if not args.tag:\n",
    "                cur_df=compound_tag_remover(cur_df)\n",
    "            cur_df['time']=dec\n",
    "            cur_df['time']=cur_df['time']-cur_df['time']%args.temporal\n",
    "            df_list.append(cur_df)\n",
    "\n",
    "        print('Done reading compound dataframes')\n",
    "        complete_df=pd.concat(df_list,ignore_index=True)\n",
    "\n",
    "        if args.temporal!=10:\n",
    "            complete_df=complete_df.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\",compression=\"bz2\")\n",
    "    \n",
    "    reduced_complete_df=complete_df.loc[(complete_df.modifier.isin(unique_mod_list))&(complete_df['head'].isin(unique_head_list))]            \n",
    "    return reduced_complete_df\n",
    "\n",
    "\n",
    "def process_decades_constituent(dec_list,input_dir,unique_constituent_list,ctype='word'):\n",
    "        \n",
    "    if os.path.exists(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\"):\n",
    "        print('Reading file')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\",compression=\"bz2\")\n",
    "        \n",
    "    elif os.path.exists(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl.bz2\") and args.temporal!=10000:\n",
    "        print(f'Reading decades file {ctype}s/10_{dec_list[0]}_{tag_str}.pkl.bz2')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl.bz2\",compression=\"bz2\")\n",
    "        \n",
    "        print(f'Reducing to {args.temporal}')\n",
    "        complete_df['time']=complete_df['time']-complete_df['time']%args.temporal\n",
    "        complete_df=complete_df.groupby([ctype,'time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\",compression=\"bz2\")\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_list=[]\n",
    "\n",
    "        for dec in dec_list:\n",
    "            cur_df=pd.read_pickle(f'{input_dir}/{ctype}s/{dec}.pkl.bz2',compression=\"bz2\")\n",
    "            if not args.tag:\n",
    "                cur_df=constituent_tag_remover(cur_df,ctype)\n",
    "            cur_df['time']=dec\n",
    "            cur_df['time']=cur_df['time']-cur_df['time']%args.temporal\n",
    "            df_list.append(cur_df)\n",
    "\n",
    "        print(f'Done reading {ctype} dataframes')\n",
    "        complete_df=pd.concat(df_list,ignore_index=True)\n",
    "        \n",
    "        if args.temporal!=10:\n",
    "            complete_df=complete_df.groupby([ctype,'time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl.bz2\",compression=\"bz2\")\n",
    "\n",
    "    if ctype=='modifier':\n",
    "        reduced_complete_df=complete_df.loc[complete_df.modifier.isin(unique_constituent_list)]\n",
    "    elif ctype=='head':\n",
    "        reduced_complete_df=complete_df.loc[complete_df['head'].isin(unique_constituent_list)]\n",
    "    else:\n",
    "        reduced_complete_df=complete_df.loc[complete_df.word.isin(unique_constituent_list)]\n",
    "\n",
    "    return reduced_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compound_tag_remover(compounds):\n",
    "    \n",
    "    print('Removing tags for compound dataset')\n",
    "    compounds['head']=compounds['head'].str.replace('_NOUN|_PROPN','',regex=True)\n",
    "    compounds.modifier=compounds.modifier.str.replace('_NOUN|_PROPN|_ADJ','',regex=True)\n",
    "    \n",
    "    compounds=compounds.groupby(['modifier','head','context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "    return compounds\n",
    "\n",
    "\n",
    "def constituent_tag_remover(constituents,ctype='word'):\n",
    "    \n",
    "    print(f'Removing tags for {ctype} dataset')\n",
    "    constituents[ctype]=constituents[ctype].str.replace('_NOUN|_PROPN|_ADJ','',regex=True)\n",
    "    \n",
    "    constituents=constituents.groupby([ctype,'context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "    return constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cutoff_compound(df):\n",
    "\n",
    "    df=df.loc[df.groupby(['modifier','head','time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_cutoff_constituent(df,ctype='word'):\n",
    "\n",
    "    df=df.loc[df.groupby([ctype,'time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppmi(ppmi_df):\n",
    "    \n",
    "    ppmi_cols=ppmi_df.columns.tolist()\n",
    "    ppmi_cols=['XY' if 'count' in x else x for x in ppmi_cols]\n",
    "    ppmi_df.columns=ppmi_cols\n",
    "\n",
    "    ppmi_time_counts=ppmi_df.groupby('time')['XY'].sum().to_frame()\n",
    "    ppmi_time_counts.columns=['N']\n",
    "\n",
    "\n",
    "    Y_star=ppmi_df.groupby(['context','time'])['XY'].sum().to_frame()\n",
    "    Y_star.columns=['Y']\n",
    "\n",
    "    ppmi_df=pd.merge(ppmi_df,Y_star.reset_index(),on=['context','time'])\n",
    "    \n",
    "    X_cols=[x for x in ppmi_cols if x not in ['context','XY'] ]\n",
    "\n",
    "\n",
    "    X_star=ppmi_df.groupby(X_cols)['XY'].sum().to_frame()\n",
    "    X_star.columns=['X']\n",
    "\n",
    "    ppmi_df=pd.merge(ppmi_df,X_star.reset_index(),on=X_cols)\n",
    "    ppmi_df=pd.merge(ppmi_df,ppmi_time_counts.reset_index(),on=['time'])\n",
    "    ppmi_df['count']=np.log2((ppmi_df['XY']*ppmi_df['N'])/(ppmi_df['X']*ppmi_df['Y']))\n",
    "    ppmi_df=ppmi_df.loc[ppmi_df['count']>=0]\n",
    "    ppmi_df.drop(['XY','X','Y','N'],axis=1,inplace=True)\n",
    "    \n",
    "    return ppmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_comp_ratings(features_df):\n",
    "\n",
    "    features_df=pd.pivot_table(features_df, index=['modifier','head'], columns=['time'])\n",
    "    features_df_columns_1=features_df.columns.get_level_values(0)\n",
    "    features_df_columns_2=features_df.columns.get_level_values(1)\n",
    "\n",
    "    cur_year=0\n",
    "    new_columns=[]\n",
    "    for year in features_df_columns_2:\n",
    "        new_columns.append(features_df_columns_1[cur_year]+\":\"+str(year))\n",
    "        cur_year+=1\n",
    "\n",
    "    features_df.columns=new_columns\n",
    "    cur_ratings_df_na=features_df.reset_index().merge(comp_ratings_df,on=['modifier','head'])\n",
    "\n",
    "\n",
    "    imputer= SimpleImputer(strategy=\"median\")\n",
    "    df_med=pd.DataFrame(imputer.fit_transform(features_df))\n",
    "    df_med.columns=features_df.columns\n",
    "    df_med.index=features_df.index\n",
    "\n",
    "    cur_ratings_df_med=df_med.reset_index().merge(comp_ratings_df,on=['modifier','head'])\n",
    "    \n",
    "    return cur_ratings_df_na,cur_ratings_df_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100_0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dec_list=[[1820,1830,1840,1850,1860,1870,1880,1890],[1900,1910,1920,1930,1940,1950,1960,1970,1980,1990],[2000,2010]]\n",
    "    \n",
    "    \n",
    "if args.ppmi:\n",
    "    ppmi_str=\"PPMI\"\n",
    "else:\n",
    "    ppmi_str=\"RAW\"\n",
    "    \n",
    "if args.tag:\n",
    "    tag_str='Tagged'\n",
    "else:\n",
    "    tag_str='UnTagged'\n",
    "    \n",
    "temp_cutoff_str=str(args.temporal)+'_'+str(args.cutoff)\n",
    "temp_cutoff_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "    unique_mod_list=comp_ratings_df[['modifier']].drop_duplicates()['modifier'].to_list()\n",
    "    unique_head_list=comp_ratings_df[['head']].drop_duplicates()['head'].to_list() \n",
    "    unique_constituent_list=list(set(unique_mod_list+unique_head_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dec list [1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890]\n",
      "Reading file\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Memo value not found at index 165739370",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_list \u001b[38;5;129;01min\u001b[39;00m total_dec_list:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent dec list \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdec_list\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     cur_compounds_agnostic\u001b[38;5;241m=\u001b[39m\u001b[43mprocess_decades_compound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_list\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43munique_mod_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43munique_head_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mctype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mphrase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     cur_constituents\u001b[38;5;241m=\u001b[39mprocess_decades_constituent(dec_list,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39minputdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,unique_constituent_list,ctype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     cur_compounds_aware\u001b[38;5;241m=\u001b[39mprocess_decades_compound(dec_list,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39minputdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,unique_mod_list,unique_head_list,ctype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mprocess_decades_compound\u001b[0;34m(dec_list, input_dir, unique_mod_list, unique_head_list, ctype)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mtemporal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdec_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl.bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     complete_df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mctype\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43ms/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdec_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtag_str\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl.bz2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbz2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/10_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdec_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl.bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtemporal\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m10000\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReading decades file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/10_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdec_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl.bz2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/spacy/lib/python3.11/site-packages/pandas/io/pickle.py:206\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# We want to silence any warnings about, e.g. moved modules.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Memo value not found at index 165739370"
     ]
    }
   ],
   "source": [
    "compounds_agnostic_list=[]\n",
    "constituents_list=[]\n",
    "compounds_aware_list=[]\n",
    "modifiers_aware_list=[]\n",
    "heads_aware_list=[]\n",
    "for dec_list in total_dec_list:\n",
    "    \n",
    "    print(f'Current dec list {dec_list}')\n",
    "    \n",
    "    cur_compounds_agnostic=process_decades_compound(dec_list,f'{args.inputdir}',unique_mod_list,unique_head_list,ctype=\"phrase\")\n",
    "    cur_constituents=process_decades_constituent(dec_list,f'{args.inputdir}',unique_constituent_list,ctype='word')\n",
    "    \n",
    "    cur_compounds_aware=process_decades_compound(dec_list,f'{args.inputdir}',unique_mod_list,unique_head_list,ctype=\"compound\")\n",
    "\n",
    "    cur_modifiers_aware=process_decades_constituent(dec_list,f'{args.inputdir}',unique_mod_list,ctype='modifier')\n",
    "\n",
    "    cur_heads_aware=process_decades_constituent(dec_list,f'{args.inputdir}',unique_head_list,ctype='head')\n",
    "    \n",
    "    compounds_agnostic_list.append(cur_compounds_agnostic)\n",
    "    constituents_list.append(cur_constituents)\n",
    "    \n",
    "    compounds_aware_list.append(cur_compounds_aware)\n",
    "    modifiers_aware_list.append(cur_modifiers_aware)\n",
    "    heads_aware_list.append(cur_heads_aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds_agnostic=pd.concat(compounds_agnostic_list,ignore_index=True)\n",
    "constituents=pd.concat(constituents_list,ignore_index=True)\n",
    "\n",
    "compounds_aware=pd.concat(compounds_aware_list,ignore_index=True)\n",
    "modifiers_aware=pd.concat(modifiers_aware_list,ignore_index=True)\n",
    "heads_aware=pd.concat(heads_aware_list,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cut-off applied\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    if args.cutoff==0:\n",
    "        print('No cut-off applied')          \n",
    "    else:\n",
    "        print(f'Cut-off: {args.cutoff}')\n",
    "        print(compounds_aware)\n",
    "\n",
    "        compounds_aware=process_cutoff_compound(compounds_aware)\n",
    "        \n",
    "        print(compounds_aware)\n",
    "\n",
    "        print(compounds_agnostic)\n",
    "\n",
    "        compounds_agnostic=process_cutoff_compound(compounds_agnostic)\n",
    "        \n",
    "        print(compounds_agnostic)\n",
    "\n",
    "        \n",
    "        constituents=process_cutoff_constituent(constituents,ctype='word')\n",
    "        modifiers_aware=process_cutoff_constituent(modifiers_aware,ctype='modifier')\n",
    "        heads_aware=process_cutoff_constituent(heads_aware,ctype='head')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features\n",
      "CompoundAware features\n"
     ]
    }
   ],
   "source": [
    "    if args.ppmi:\n",
    "        print('Applying PPMI')\n",
    "        compounds_aware=ppmi(compounds_aware)\n",
    "        modifiers_aware=ppmi(modifiers_aware)\n",
    "        heads_aware=ppmi(heads_aware)\n",
    "                        \n",
    "        compounds_agnostic=ppmi(compounds_agnostic)\n",
    "        constituents=ppmi(constituents)\n",
    "    timespan_list_aware_df=pd.DataFrame(compounds_aware.time.unique())\n",
    "    timespan_list_aware_df.columns=['time']\n",
    "\n",
    "    compound_list_aware_df=comp_ratings_df[['modifier','head']].copy()\n",
    "    compound_list_aware_df=compound_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "\n",
    "    modifier_list_aware_df=comp_ratings_df[['modifier']].drop_duplicates().copy()\n",
    "    modifier_list_aware_df=modifier_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "\n",
    "    head_list_aware_df=comp_ratings_df[['head']].drop_duplicates().copy()\n",
    "    head_list_aware_df=head_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "            \n",
    "    all_comps_aware=compounds_aware[['modifier','head','time']].copy()\n",
    "    all_comps_aware.drop_duplicates(inplace=True)\n",
    "           \n",
    "    all_mods_aware=compounds_aware[['modifier','time']].copy()\n",
    "    all_mods_aware.drop_duplicates(inplace=True)\n",
    "            \n",
    "    all_heads_aware=compounds_aware[['head','time']].copy()\n",
    "    all_heads_aware.drop_duplicates(inplace=True)\n",
    "            \n",
    "    not_found_compounds_aware_df=compound_list_aware_df.merge(all_comps_aware, on=['modifier','head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_compounds_aware_df=not_found_compounds_aware_df.loc[not_found_compounds_aware_df['_merge']=='left_only']\n",
    "    not_found_compounds_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "            \n",
    "    not_found_modifiers_aware_df=modifier_list_aware_df.merge(all_mods_aware, on=['modifier','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_modifiers_aware_df=not_found_modifiers_aware_df.loc[not_found_modifiers_aware_df['_merge']=='left_only']\n",
    "    not_found_modifiers_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "    not_found_heads_aware_df=head_list_aware_df.merge(all_heads_aware, on=['head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_heads_aware_df=not_found_heads_aware_df.loc[not_found_heads_aware_df['_merge']=='left_only']\n",
    "    not_found_heads_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    timespan_list_agnostic_df=pd.DataFrame(compounds_agnostic.time.unique())\n",
    "    timespan_list_agnostic_df.columns=['time']\n",
    "\n",
    "    compound_list_agnostic_df=comp_ratings_df[['modifier','head']].copy()\n",
    "    compound_list_agnostic_df=compound_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "\n",
    "    modifier_list_agnostic_df=comp_ratings_df[['modifier']].drop_duplicates().copy()\n",
    "    modifier_list_agnostic_df=modifier_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "\n",
    "    head_list_agnostic_df=comp_ratings_df[['head']].drop_duplicates().copy()\n",
    "    head_list_agnostic_df=head_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "            \n",
    "    all_comps_agnostic=compounds_agnostic[['modifier','head','time']].copy()\n",
    "    all_comps_agnostic.drop_duplicates(inplace=True)\n",
    "           \n",
    "    all_mods_agnostic=compounds_agnostic[['modifier','time']].copy()\n",
    "    all_mods_agnostic.drop_duplicates(inplace=True)\n",
    "            \n",
    "    all_heads_agnostic=compounds_agnostic[['head','time']].copy()\n",
    "    all_heads_agnostic.drop_duplicates(inplace=True)\n",
    "            \n",
    "    not_found_compounds_agnostic_df=compound_list_agnostic_df.merge(all_comps_agnostic, on=['modifier','head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_compounds_agnostic_df=not_found_compounds_agnostic_df.loc[not_found_compounds_agnostic_df['_merge']=='left_only']\n",
    "    not_found_compounds_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "                    \n",
    "    not_found_modifiers_agnostic_df=modifier_list_agnostic_df.merge(all_mods_agnostic, on=['modifier','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_modifiers_agnostic_df=not_found_modifiers_agnostic_df.loc[not_found_modifiers_agnostic_df['_merge']=='left_only']\n",
    "    not_found_modifiers_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "    not_found_heads_agnostic_df=head_list_agnostic_df.merge(all_heads_agnostic, on=['head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_heads_agnostic_df=not_found_heads_agnostic_df.loc[not_found_heads_agnostic_df['_merge']=='left_only']\n",
    "    not_found_heads_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "    \n",
    "    compounds_aware=compounds_aware.merge(comp_ratings_df[['modifier','head']],on=['modifier','head'])\n",
    "\n",
    "    compounds_agnostic=compounds_agnostic.merge(comp_ratings_df[['modifier','head']],on=['modifier','head'])\n",
    "\n",
    "    heads_agnostic=constituents.copy()\n",
    "    heads_agnostic_cols=heads_agnostic.columns\n",
    "    heads_agnostic_cols=['head' if 'word' in x else x for x in heads_agnostic_cols]\n",
    "    heads_agnostic.columns=heads_agnostic_cols\n",
    "    heads_agnostic=heads_agnostic.loc[heads_agnostic['head'].isin(unique_head_list)]\n",
    "\n",
    "\n",
    "    modifiers_agnostic=constituents.copy()\n",
    "    modifiers_agnostic_cols=modifiers_agnostic.columns\n",
    "    modifiers_agnostic_cols=['modifier' if 'word' in x else x for x in modifiers_agnostic_cols]\n",
    "    modifiers_agnostic.columns=modifiers_agnostic_cols\n",
    "    modifiers_agnostic=modifiers_agnostic.loc[modifiers_agnostic.modifier.isin(unique_mod_list)]\n",
    "    \n",
    "    print('Calculating features')\n",
    "    \n",
    "\n",
    "    \n",
    "    print('CompoundAware features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_bw_rows(df):\n",
    "    df_orig=df.copy()\n",
    "    df_shifted=df.shift().copy()\n",
    "    denom_df_orig=(df_orig**2).sum(axis=1)\n",
    "    denom_df_shifted=(df_shifted**2).sum(axis=1)\n",
    "    denominator=np.sqrt(denom_df_orig*denom_df_shifted)\n",
    "    numerator=(df_orig*df_shifted).sum(axis=1)\n",
    "    if df.index.nlevels==3:\n",
    "        cosine_sim_df=(numerator/denominator).reset_index(level=[0,1],drop=True)\n",
    "    else:\n",
    "        cosine_sim_df=(numerator/denominator).reset_index(level=[0],drop=True)        \n",
    "    cosine_sim_df.dropna(inplace=True)\n",
    "    cosine_sim_df=cosine_sim_df.to_frame()\n",
    "    return cosine_sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_features(compounds,modifiers,heads,compound_list_df):\n",
    "    \n",
    "    compounds_pivot=pd.pivot_table(compounds, values='count', index=['modifier','head', 'time'],\n",
    "                       columns=['context'], aggfunc=\"sum\",fill_value=0)\n",
    "    modifiers_pivot=pd.pivot_table(modifiers, values='count', index=['modifier','time'],\n",
    "                       columns=['context'], aggfunc=\"sum\",fill_value=0)\n",
    "    heads_pivot=pd.pivot_table(heads, values='count', index=['head','time'],\n",
    "                       columns=['context'], aggfunc=\"sum\",fill_value=0)\n",
    "    \n",
    "    change_compounds_df=compounds_pivot.groupby(level=[0,1]).apply(cosine_bw_rows)\n",
    "    change_compounds_df.columns=['change_comp']\n",
    "\n",
    "    change_modifiers_df=modifiers_pivot.groupby(level=[0]).apply(cosine_bw_rows)\n",
    "    change_modifiers_df.columns=['change_mod']\n",
    "    change_heads_df=heads_pivot.groupby(level=[0]).apply(cosine_bw_rows)\n",
    "    change_heads_df.columns=['change_head']\n",
    "    \n",
    "    \n",
    "    changed_df=pd.merge(change_compounds_df.reset_index(),compound_list_df,on=['modifier','head','time'],how='right')\n",
    "    changed_df=pd.merge(changed_df,change_modifiers_df.reset_index(),on=['modifier','time'],how='right')\n",
    "    \n",
    "    changed_df=pd.merge(changed_df,change_heads_df.reset_index(),on=['head','time'])\n",
    "    return changed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>time</th>\n",
       "      <th>perc_token_modifier</th>\n",
       "      <th>perc_type_modifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academy</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>academy</td>\n",
       "      <td>1830</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.041901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>academy</td>\n",
       "      <td>1840</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>0.052464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>academy</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.010834</td>\n",
       "      <td>0.073388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>academy</td>\n",
       "      <td>1860</td>\n",
       "      <td>0.015582</td>\n",
       "      <td>0.086483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4832</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.244970</td>\n",
       "      <td>0.297218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1980</td>\n",
       "      <td>0.449932</td>\n",
       "      <td>0.419554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>zebra</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.607710</td>\n",
       "      <td>0.491024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>zebra</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.502937</td>\n",
       "      <td>0.453294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>zebra</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.369851</td>\n",
       "      <td>0.446516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4837 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     modifier  time  perc_token_modifier  perc_type_modifier\n",
       "0     academy  1820             0.003118            0.034500\n",
       "1     academy  1830             0.003979            0.041901\n",
       "2     academy  1840             0.005512            0.052464\n",
       "3     academy  1850             0.010834            0.073388\n",
       "4     academy  1860             0.015582            0.086483\n",
       "...       ...   ...                  ...                 ...\n",
       "4832    zebra  1970             0.244970            0.297218\n",
       "4833    zebra  1980             0.449932            0.419554\n",
       "4834    zebra  1990             0.607710            0.491024\n",
       "4835    zebra  2000             0.502937            0.453294\n",
       "4836    zebra  2010             0.369851            0.446516\n",
       "\n",
       "[4837 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_modifiers_features_df=(modifiers_aware.groupby(['modifier','time'])['count'].agg(perc_token_modifier='sum', perc_type_modifier='size')/modifiers_agnostic.groupby(['modifier','time'])['count'].agg(perc_token_modifier='sum', perc_type_modifier='size')).reset_index()\n",
    "num_modifiers_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>time</th>\n",
       "      <th>perc_token_head</th>\n",
       "      <th>perc_type_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>account</td>\n",
       "      <td>1820</td>\n",
       "      <td>0.273542</td>\n",
       "      <td>0.301059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>account</td>\n",
       "      <td>1830</td>\n",
       "      <td>0.267762</td>\n",
       "      <td>0.317516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>account</td>\n",
       "      <td>1840</td>\n",
       "      <td>0.272242</td>\n",
       "      <td>0.319656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>account</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.267778</td>\n",
       "      <td>0.329674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>account</td>\n",
       "      <td>1860</td>\n",
       "      <td>0.269994</td>\n",
       "      <td>0.329282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>year</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.399238</td>\n",
       "      <td>0.639785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>year</td>\n",
       "      <td>1980</td>\n",
       "      <td>0.381366</td>\n",
       "      <td>0.635348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4891</th>\n",
       "      <td>year</td>\n",
       "      <td>1990</td>\n",
       "      <td>0.363366</td>\n",
       "      <td>0.634835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4892</th>\n",
       "      <td>year</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.351876</td>\n",
       "      <td>0.637860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>year</td>\n",
       "      <td>2010</td>\n",
       "      <td>0.313654</td>\n",
       "      <td>0.636292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4894 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         head  time  perc_token_head  perc_type_head\n",
       "0     account  1820         0.273542        0.301059\n",
       "1     account  1830         0.267762        0.317516\n",
       "2     account  1840         0.272242        0.319656\n",
       "3     account  1850         0.267778        0.329674\n",
       "4     account  1860         0.269994        0.329282\n",
       "...       ...   ...              ...             ...\n",
       "4889     year  1970         0.399238        0.639785\n",
       "4890     year  1980         0.381366        0.635348\n",
       "4891     year  1990         0.363366        0.634835\n",
       "4892     year  2000         0.351876        0.637860\n",
       "4893     year  2010         0.313654        0.636292\n",
       "\n",
       "[4894 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads_features_df=(heads_aware.groupby(['head','time'])['count'].agg(perc_token_head='sum', perc_type_head='size')/heads_agnostic.groupby(['head','time'])['count'].agg(perc_token_head='sum', perc_type_head='size')).reset_index()\n",
    "num_heads_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        modifier        head  perc_token_comp:1820  perc_token_comp:1830  \\\n",
       " 0        academy       award                   NaN              0.515152   \n",
       " 1           acid        test              1.000000              0.909091   \n",
       " 2          agony        aunt                   NaN                   NaN   \n",
       " 3        ancient     history              0.997131              0.994727   \n",
       " 4    application        form              1.000000              0.926680   \n",
       " ..           ...         ...                   ...                   ...   \n",
       " 278        white      spirit              0.946360              0.834395   \n",
       " 279       winter    solstice              1.000000              1.000000   \n",
       " 280         word    painting                   NaN                   NaN   \n",
       " 281        world  conference              1.000000                   NaN   \n",
       " 282        zebra    crossing                   NaN                   NaN   \n",
       " \n",
       "      perc_token_comp:1840  perc_token_comp:1850  perc_token_comp:1860  \\\n",
       " 0                     NaN                   NaN                   NaN   \n",
       " 1                0.557823              0.843658              0.735714   \n",
       " 2                     NaN                   NaN                   NaN   \n",
       " 3                0.993532              0.994004              0.991248   \n",
       " 4                0.943396              0.975845              0.989336   \n",
       " ..                    ...                   ...                   ...   \n",
       " 278              0.957655              0.956989              0.923387   \n",
       " 279              1.000000              1.000000              0.999359   \n",
       " 280              1.000000              1.000000              1.000000   \n",
       " 281              1.000000              0.666667              0.666667   \n",
       " 282                   NaN                   NaN                   NaN   \n",
       " \n",
       "      perc_token_comp:1870  perc_token_comp:1880  perc_token_comp:1890  ...  \\\n",
       " 0                0.791667              1.000000              1.000000  ...   \n",
       " 1                0.501795              0.465191              0.595886  ...   \n",
       " 2                     NaN                   NaN                   NaN  ...   \n",
       " 3                0.990481              0.990166              0.990183  ...   \n",
       " 4                0.993808              0.979484              0.968750  ...   \n",
       " ..                    ...                   ...                   ...  ...   \n",
       " 278              0.835000              0.925072              0.926554  ...   \n",
       " 279              1.000000              0.993035                   NaN  ...   \n",
       " 280              1.000000              1.000000              1.000000  ...   \n",
       " 281              1.000000              0.971429              1.000000  ...   \n",
       " 282                   NaN                   NaN                   NaN  ...   \n",
       " \n",
       "      perc_type_modifier:2010  avgModifier  stdevModifier   avgHead  stdevHead  \\\n",
       " 0                   0.170736     3.130400       1.632200  4.695700   0.764800   \n",
       " 1                   0.555842     0.714286       1.097307  3.896552   1.241379   \n",
       " 2                   0.044977     1.862069       1.224017  0.433333   0.558768   \n",
       " 3                   0.819059     2.000000       1.527500  2.842100   2.115100   \n",
       " 4                   0.178305     4.766667       0.422953  4.862069   0.344828   \n",
       " ..                       ...          ...            ...       ...        ...   \n",
       " 278                 0.813871     1.538500       1.240300  2.038500   1.949000   \n",
       " 279                 0.587605     5.000000       0.000000  4.681800   1.086100   \n",
       " 280                 0.302789     3.187500       1.558600  0.437500   0.727400   \n",
       " 281                 0.404139     3.875000       1.361300  4.291700   1.267600   \n",
       " 282                 0.446516     0.758621       0.624509  4.607143   0.859372   \n",
       " \n",
       "      compositionality  stdevHeadModifier  is_adj          compound  \\\n",
       " 0            3.521700           1.473100   False     academy_award   \n",
       " 1            1.222222           1.257079   False         acid_test   \n",
       " 2            0.758621           0.857228   False        agony_aunt   \n",
       " 3            1.947400           1.580200    True   ancient_history   \n",
       " 4            4.800000           0.476095   False  application_form   \n",
       " ..                ...                ...     ...               ...   \n",
       " 278          1.307700           1.257600    True      white_spirit   \n",
       " 279          4.545500           1.335500   False   winter_solstice   \n",
       " 280          1.625000           1.310200   False     word_painting   \n",
       " 281          3.958300           1.366700   False  world_conference   \n",
       " 282          1.250000           1.022078   False    zebra_crossing   \n",
       " \n",
       "           source  \n",
       " 0    cordeiro100  \n",
       " 1          reddy  \n",
       " 2          reddy  \n",
       " 3     cordeiro90  \n",
       " 4          reddy  \n",
       " ..           ...  \n",
       " 278  cordeiro100  \n",
       " 279  cordeiro100  \n",
       " 280   cordeiro90  \n",
       " 281  cordeiro100  \n",
       " 282        reddy  \n",
       " \n",
       " [283 rows x 131 columns],\n",
       "         modifier        head  perc_token_comp:1820  perc_token_comp:1830  \\\n",
       " 0        academy       award              1.000000              0.515152   \n",
       " 1           acid        test              1.000000              0.909091   \n",
       " 2          agony        aunt              1.000000              0.994238   \n",
       " 3        ancient     history              0.997131              0.994727   \n",
       " 4    application        form              1.000000              0.926680   \n",
       " ..           ...         ...                   ...                   ...   \n",
       " 278        white      spirit              0.946360              0.834395   \n",
       " 279       winter    solstice              1.000000              1.000000   \n",
       " 280         word    painting              1.000000              0.994238   \n",
       " 281        world  conference              1.000000              0.994238   \n",
       " 282        zebra    crossing              1.000000              0.994238   \n",
       " \n",
       "      perc_token_comp:1840  perc_token_comp:1850  perc_token_comp:1860  \\\n",
       " 0                0.994959              0.996805              0.994069   \n",
       " 1                0.557823              0.843658              0.735714   \n",
       " 2                0.994959              0.996805              0.994069   \n",
       " 3                0.993532              0.994004              0.991248   \n",
       " 4                0.943396              0.975845              0.989336   \n",
       " ..                    ...                   ...                   ...   \n",
       " 278              0.957655              0.956989              0.923387   \n",
       " 279              1.000000              1.000000              0.999359   \n",
       " 280              1.000000              1.000000              1.000000   \n",
       " 281              1.000000              0.666667              0.666667   \n",
       " 282              0.994959              0.996805              0.994069   \n",
       " \n",
       "      perc_token_comp:1870  perc_token_comp:1880  perc_token_comp:1890  ...  \\\n",
       " 0                0.791667              1.000000              1.000000  ...   \n",
       " 1                0.501795              0.465191              0.595886  ...   \n",
       " 2                0.993247              0.992490              0.988546  ...   \n",
       " 3                0.990481              0.990166              0.990183  ...   \n",
       " 4                0.993808              0.979484              0.968750  ...   \n",
       " ..                    ...                   ...                   ...  ...   \n",
       " 278              0.835000              0.925072              0.926554  ...   \n",
       " 279              1.000000              0.993035              0.988546  ...   \n",
       " 280              1.000000              1.000000              1.000000  ...   \n",
       " 281              1.000000              0.971429              1.000000  ...   \n",
       " 282              0.993247              0.992490              0.988546  ...   \n",
       " \n",
       "      perc_type_modifier:2010  avgModifier  stdevModifier   avgHead  stdevHead  \\\n",
       " 0                   0.170736     3.130400       1.632200  4.695700   0.764800   \n",
       " 1                   0.555842     0.714286       1.097307  3.896552   1.241379   \n",
       " 2                   0.044977     1.862069       1.224017  0.433333   0.558768   \n",
       " 3                   0.819059     2.000000       1.527500  2.842100   2.115100   \n",
       " 4                   0.178305     4.766667       0.422953  4.862069   0.344828   \n",
       " ..                       ...          ...            ...       ...        ...   \n",
       " 278                 0.813871     1.538500       1.240300  2.038500   1.949000   \n",
       " 279                 0.587605     5.000000       0.000000  4.681800   1.086100   \n",
       " 280                 0.302789     3.187500       1.558600  0.437500   0.727400   \n",
       " 281                 0.404139     3.875000       1.361300  4.291700   1.267600   \n",
       " 282                 0.446516     0.758621       0.624509  4.607143   0.859372   \n",
       " \n",
       "      compositionality  stdevHeadModifier  is_adj          compound  \\\n",
       " 0            3.521700           1.473100   False     academy_award   \n",
       " 1            1.222222           1.257079   False         acid_test   \n",
       " 2            0.758621           0.857228   False        agony_aunt   \n",
       " 3            1.947400           1.580200    True   ancient_history   \n",
       " 4            4.800000           0.476095   False  application_form   \n",
       " ..                ...                ...     ...               ...   \n",
       " 278          1.307700           1.257600    True      white_spirit   \n",
       " 279          4.545500           1.335500   False   winter_solstice   \n",
       " 280          1.625000           1.310200   False     word_painting   \n",
       " 281          3.958300           1.366700   False  world_conference   \n",
       " 282          1.250000           1.022078   False    zebra_crossing   \n",
       " \n",
       "           source  \n",
       " 0    cordeiro100  \n",
       " 1          reddy  \n",
       " 2          reddy  \n",
       " 3     cordeiro90  \n",
       " 4          reddy  \n",
       " ..           ...  \n",
       " 278  cordeiro100  \n",
       " 279  cordeiro100  \n",
       " 280   cordeiro90  \n",
       " 281  cordeiro100  \n",
       " 282        reddy  \n",
       " \n",
       " [283 rows x 131 columns])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_compounds_features_df=(compounds_aware.groupby(['modifier','head','time'])['count'].agg(perc_token_comp='sum', perc_type_comp='size')/compounds_agnostic.groupby(['modifier','head','time'])['count'].agg(perc_token_comp='sum', perc_type_comp='size')).reset_index()\n",
    "num_compounds_features_df=pd.merge(num_compounds_features_df,num_modifiers_features_df,on=['modifier','time'])\n",
    "num_compounds_features_df=pd.merge(num_compounds_features_df,num_heads_features_df,on=['head','time'])\n",
    "merge_comp_ratings(num_compounds_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>head</th>\n",
       "      <th>time</th>\n",
       "      <th>change_comp</th>\n",
       "      <th>change_mod</th>\n",
       "      <th>change_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academy</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.310599</td>\n",
       "      <td>0.918114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blind</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.995921</td>\n",
       "      <td>0.918114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.848085</td>\n",
       "      <td>0.918114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cash</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.907187</td>\n",
       "      <td>0.918114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disability</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.349618</td>\n",
       "      <td>0.918114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279397</th>\n",
       "      <td>winter</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1900</td>\n",
       "      <td>0.970616</td>\n",
       "      <td>0.995596</td>\n",
       "      <td>0.999839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279398</th>\n",
       "      <td>word</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960697</td>\n",
       "      <td>0.999839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279399</th>\n",
       "      <td>winter</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1910</td>\n",
       "      <td>0.983041</td>\n",
       "      <td>0.985240</td>\n",
       "      <td>0.999717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279400</th>\n",
       "      <td>word</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.940345</td>\n",
       "      <td>0.999717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279401</th>\n",
       "      <td>word</td>\n",
       "      <td>phone</td>\n",
       "      <td>1850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.981061</td>\n",
       "      <td>0.910776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279402 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          modifier      head  time  change_comp  change_mod  change_head\n",
       "0          academy     award  1830          NaN    0.310599     0.918114\n",
       "1            blind     award  1830          NaN    0.995921     0.918114\n",
       "2             case     award  1830          NaN    0.848085     0.918114\n",
       "3             cash     award  1830          NaN    0.907187     0.918114\n",
       "4       disability     award  1830          NaN    0.349618     0.918114\n",
       "...            ...       ...   ...          ...         ...          ...\n",
       "279397      winter  solstice  1900     0.970616    0.995596     0.999839\n",
       "279398        word  solstice  1900          NaN    0.960697     0.999839\n",
       "279399      winter  solstice  1910     0.983041    0.985240     0.999717\n",
       "279400        word  solstice  1910          NaN    0.940345     0.999717\n",
       "279401        word     phone  1850          NaN    0.981061     0.910776\n",
       "\n",
       "[279402 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_aware_df=temporal_features(compounds_aware,modifiers_aware,heads_aware,all_comps_aware)\n",
    "change_aware_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>head</th>\n",
       "      <th>time</th>\n",
       "      <th>change_comp</th>\n",
       "      <th>change_mod</th>\n",
       "      <th>change_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academy</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.910445</td>\n",
       "      <td>0.981156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blind</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993280</td>\n",
       "      <td>0.981156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>case</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.993940</td>\n",
       "      <td>0.981156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cash</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.821940</td>\n",
       "      <td>0.981156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>disability</td>\n",
       "      <td>award</td>\n",
       "      <td>1830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.903540</td>\n",
       "      <td>0.981156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290607</th>\n",
       "      <td>winter</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1900</td>\n",
       "      <td>0.911541</td>\n",
       "      <td>0.993283</td>\n",
       "      <td>0.999591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290608</th>\n",
       "      <td>word</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.991863</td>\n",
       "      <td>0.999591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290609</th>\n",
       "      <td>winter</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1910</td>\n",
       "      <td>0.976609</td>\n",
       "      <td>0.981344</td>\n",
       "      <td>0.999290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290610</th>\n",
       "      <td>word</td>\n",
       "      <td>solstice</td>\n",
       "      <td>1910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.982114</td>\n",
       "      <td>0.999290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290611</th>\n",
       "      <td>word</td>\n",
       "      <td>phone</td>\n",
       "      <td>1850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997557</td>\n",
       "      <td>0.986283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>290612 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          modifier      head  time  change_comp  change_mod  change_head\n",
       "0          academy     award  1830          NaN    0.910445     0.981156\n",
       "1            blind     award  1830          NaN    0.993280     0.981156\n",
       "2             case     award  1830          NaN    0.993940     0.981156\n",
       "3             cash     award  1830          NaN    0.821940     0.981156\n",
       "4       disability     award  1830          NaN    0.903540     0.981156\n",
       "...            ...       ...   ...          ...         ...          ...\n",
       "290607      winter  solstice  1900     0.911541    0.993283     0.999591\n",
       "290608        word  solstice  1900          NaN    0.991863     0.999591\n",
       "290609      winter  solstice  1910     0.976609    0.981344     0.999290\n",
       "290610        word  solstice  1910          NaN    0.982114     0.999290\n",
       "290611        word     phone  1850          NaN    0.997557     0.986283\n",
       "\n",
       "[290612 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_agnostic_df=temporal_features(compounds_agnostic,modifiers_agnostic,heads_agnostic,all_comps_agnostic)\n",
    "change_agnostic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_ratings_aware_df_na,cur_ratings_aware_df_med=merge_comp_ratings(change_aware_df)\n",
    "cur_ratings_agnostic_df_na,cur_ratings_agnostic_df_med=merge_comp_ratings(change_agnostic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving feature datasets\n"
     ]
    }
   ],
   "source": [
    "print('Saving feature datasets')\n",
    "\n",
    "\n",
    "cur_ratings_aware_df_na.to_csv(f'{args.outputdir}/temporal_CompoundAware_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_na.csv',sep='\\t',index=False)\n",
    "cur_ratings_aware_df_med.to_csv(f'{args.outputdir}/temporal_CompoundAware_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_med.csv',sep='\\t',index=False)\n",
    "\n",
    "cur_ratings_agnostic_df_na.to_csv(f'{args.outputdir}/temporal_CompoundAgnostic_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_na.csv',sep='\\t',index=False)\n",
    "cur_ratings_agnostic_df_med.to_csv(f'{args.outputdir}/temporal_CompoundAgnostic_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_med.csv',sep='\\t',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
