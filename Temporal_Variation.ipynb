{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import pickle as pkl\n",
    "\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\", font_scale = 2.5)\n",
    "sns.set_context(rc={\"lines.markersize\": 17, \"lines.linewidth\": 2})\n",
    "\n",
    "import matplotlib.pyplot as plt      \n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Compute temporal variation features from sparse dataset for google version')\n",
    "\n",
    "parser.add_argument('--inputdir',type=str,\n",
    "                    help='Provide directory where features are located')\n",
    "parser.add_argument('--outputdir',type=str,\n",
    "                    help='Where should the output be stored?')\n",
    "parser.add_argument('--tag', action='store_true',\n",
    "                    help='Should the POS tag be kept?')\n",
    "parser.add_argument('--ppmi', action='store_true',\n",
    "                    help='Should co-occurence matrix be converted to PPMI values')\n",
    "parser.add_argument('--temporal',  type=int,\n",
    "                    help='Value to bin the temporal information: 10000 (remove temporal information), 1 (no binning), 10 (binning to decades), 20 (binning each 20 years) or 50 (binning each 50 years)')\n",
    "parser.add_argument('--cutoff', type=int, default=0,\n",
    "                    help='Cut-off frequency for each compound per time period : none (0), 20, 50 and 100')\n",
    "args = parser.parse_args('--inputdir /datanaco/dharp/compounds/datasets/ --outputdir /data/dharp/compounds/datasets/features/ --temporal 10'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>head</th>\n",
       "      <th>avgModifier</th>\n",
       "      <th>stdevModifier</th>\n",
       "      <th>avgHead</th>\n",
       "      <th>stdevHead</th>\n",
       "      <th>compositionality</th>\n",
       "      <th>stdevHeadModifier</th>\n",
       "      <th>is_adj</th>\n",
       "      <th>compound</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end</td>\n",
       "      <td>user</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>1.117537</td>\n",
       "      <td>4.866667</td>\n",
       "      <td>0.339935</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.871165</td>\n",
       "      <td>False</td>\n",
       "      <td>end_user</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>firing</td>\n",
       "      <td>line</td>\n",
       "      <td>1.607143</td>\n",
       "      <td>1.654848</td>\n",
       "      <td>1.892857</td>\n",
       "      <td>1.496169</td>\n",
       "      <td>1.703704</td>\n",
       "      <td>1.717337</td>\n",
       "      <td>False</td>\n",
       "      <td>firing_line</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>game</td>\n",
       "      <td>plan</td>\n",
       "      <td>2.821429</td>\n",
       "      <td>1.964935</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>3.827586</td>\n",
       "      <td>1.233693</td>\n",
       "      <td>False</td>\n",
       "      <td>game_plan</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>application</td>\n",
       "      <td>form</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>0.422953</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.476095</td>\n",
       "      <td>False</td>\n",
       "      <td>application_form</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snail</td>\n",
       "      <td>mail</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>4.586207</td>\n",
       "      <td>1.099129</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>1.020596</td>\n",
       "      <td>False</td>\n",
       "      <td>snail_mail</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>wedding</td>\n",
       "      <td>day</td>\n",
       "      <td>4.764700</td>\n",
       "      <td>0.562300</td>\n",
       "      <td>4.058800</td>\n",
       "      <td>1.434900</td>\n",
       "      <td>4.941200</td>\n",
       "      <td>0.242500</td>\n",
       "      <td>False</td>\n",
       "      <td>wedding_day</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>white</td>\n",
       "      <td>noise</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>1.112300</td>\n",
       "      <td>4.043500</td>\n",
       "      <td>1.429500</td>\n",
       "      <td>1.173900</td>\n",
       "      <td>1.230400</td>\n",
       "      <td>True</td>\n",
       "      <td>white_noise</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>white</td>\n",
       "      <td>spirit</td>\n",
       "      <td>1.538500</td>\n",
       "      <td>1.240300</td>\n",
       "      <td>2.038500</td>\n",
       "      <td>1.949000</td>\n",
       "      <td>1.307700</td>\n",
       "      <td>1.257600</td>\n",
       "      <td>True</td>\n",
       "      <td>white_spirit</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>winter</td>\n",
       "      <td>solstice</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.681800</td>\n",
       "      <td>1.086100</td>\n",
       "      <td>4.545500</td>\n",
       "      <td>1.335500</td>\n",
       "      <td>False</td>\n",
       "      <td>winter_solstice</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>world</td>\n",
       "      <td>conference</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>1.361300</td>\n",
       "      <td>4.291700</td>\n",
       "      <td>1.267600</td>\n",
       "      <td>3.958300</td>\n",
       "      <td>1.366700</td>\n",
       "      <td>False</td>\n",
       "      <td>world_conference</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>287 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        modifier        head  avgModifier  stdevModifier   avgHead  stdevHead  \\\n",
       "0            end        user     3.866667       1.117537  4.866667   0.339935   \n",
       "1         firing        line     1.607143       1.654848  1.892857   1.496169   \n",
       "2           game        plan     2.821429       1.964935  4.862069   0.344828   \n",
       "3    application        form     4.766667       0.422953  4.862069   0.344828   \n",
       "4          snail        mail     0.600000       0.800000  4.586207   1.099129   \n",
       "..           ...         ...          ...            ...       ...        ...   \n",
       "97       wedding         day     4.764700       0.562300  4.058800   1.434900   \n",
       "98         white       noise     0.652200       1.112300  4.043500   1.429500   \n",
       "99         white      spirit     1.538500       1.240300  2.038500   1.949000   \n",
       "100       winter    solstice     5.000000       0.000000  4.681800   1.086100   \n",
       "101        world  conference     3.875000       1.361300  4.291700   1.267600   \n",
       "\n",
       "     compositionality  stdevHeadModifier  is_adj          compound  \\\n",
       "0            4.250000           0.871165   False          end_user   \n",
       "1            1.703704           1.717337   False       firing_line   \n",
       "2            3.827586           1.233693   False         game_plan   \n",
       "3            4.800000           0.476095   False  application_form   \n",
       "4            1.310345           1.020596   False        snail_mail   \n",
       "..                ...                ...     ...               ...   \n",
       "97           4.941200           0.242500   False       wedding_day   \n",
       "98           1.173900           1.230400    True       white_noise   \n",
       "99           1.307700           1.257600    True      white_spirit   \n",
       "100          4.545500           1.335500   False   winter_solstice   \n",
       "101          3.958300           1.366700   False  world_conference   \n",
       "\n",
       "          source  \n",
       "0          reddy  \n",
       "1          reddy  \n",
       "2          reddy  \n",
       "3          reddy  \n",
       "4          reddy  \n",
       "..           ...  \n",
       "97   cordeiro100  \n",
       "98   cordeiro100  \n",
       "99   cordeiro100  \n",
       "100  cordeiro100  \n",
       "101  cordeiro100  \n",
       "\n",
       "[287 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddy_df=pd.read_csv('data/reddy_90.txt',sep='\\t')\n",
    "reddy_df['source']='reddy'\n",
    "cordeiro90_df=pd.read_csv('data/cordeiro_90.txt',sep='\\t')\n",
    "cordeiro90_df['source']='cordeiro90'\n",
    "cordeiro100_df=pd.read_csv('data/cordeiro_100.txt',sep='\\t')\n",
    "cordeiro100_df['source']='cordeiro100'\n",
    "\n",
    "    \n",
    "comp_ratings_df=pd.concat([reddy_df,cordeiro90_df,cordeiro100_df])\n",
    "#comp_ratings_df.drop_duplicates(inplace=True)\n",
    "comp_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testset_tagger(df):\n",
    "\n",
    "    #### NOUN NOUN\n",
    "    \n",
    "    copy_df_1=df.copy()\n",
    "    copy_df_1.modifier=copy_df_1.modifier+'_NOUN'\n",
    "    copy_df_1['head']=copy_df_1['head']+'_NOUN'\n",
    "\n",
    "    ### PROPN NOUN\n",
    "\n",
    "    copy_df_2=df.copy()\n",
    "    copy_df_2.modifier=copy_df_2.modifier+'_PROPN'\n",
    "    copy_df_2['head']=copy_df_2['head']+'_NOUN'\n",
    "    \n",
    "    ### NOUN PROPN\n",
    "\n",
    "    copy_df_3=df.copy()\n",
    "    copy_df_3.modifier=copy_df_3.modifier+'_NOUN'\n",
    "    copy_df_3['head']=copy_df_3['head']+'_PROPN'\n",
    "    \n",
    "    ### PROPN PROPN    \n",
    "\n",
    "    copy_df_4=df.copy()\n",
    "    copy_df_4.modifier=copy_df_4.modifier+'_PROPN'\n",
    "    copy_df_4['head']=copy_df_4['head']+'_PROPN'\n",
    "    \n",
    "   \n",
    "    ### ADJ/NOUN NOUN\n",
    "    \n",
    "    copy_df_5=df.copy()\n",
    "    \n",
    "    copy_df_5.loc[copy_df_5.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_5.loc[copy_df_5.is_adj==False,\"modifier\"]+=\"_NOUN\"\n",
    "    copy_df_5['head']=copy_df_5['head']+'_NOUN'   \n",
    "    \n",
    "    \n",
    "    ### ADJ/NOUN PROPN\n",
    "    \n",
    "    copy_df_6=df.copy()\n",
    "    copy_df_6.loc[copy_df_6.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_6.loc[copy_df_6.is_adj==False,\"modifier\"]+=\"_NOUN\"\n",
    "    copy_df_6['head']=copy_df_6['head']+'_PROPN'  \n",
    "\n",
    "    \n",
    "    #### ADJ/PROPN NOUN\n",
    "    \n",
    "    copy_df_7=df.copy()\n",
    "    copy_df_7.loc[copy_df_7.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_7.loc[copy_df_7.is_adj==False,\"modifier\"]+=\"_PROPN\"\n",
    "    copy_df_7['head']=copy_df_7['head']+'_NOUN' \n",
    "    \n",
    "    \n",
    "    #### ADJ/PROPN PROPN\n",
    "    \n",
    "    copy_df_8=df.copy()\n",
    "    copy_df_8.loc[copy_df_8.is_adj==True,\"modifier\"]+=\"_ADJ\"\n",
    "    copy_df_8.loc[copy_df_8.is_adj==False,\"modifier\"]+=\"_PROPN\"\n",
    "    copy_df_8['head']=copy_df_8['head']+'_PROPN' \n",
    "    \n",
    "    \n",
    "    complete_df=pd.concat([copy_df_1,copy_df_2,copy_df_3,copy_df_4,copy_df_5,copy_df_6,copy_df_7,copy_df_8],ignore_index=True)\n",
    "                           \n",
    "    return complete_df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>head</th>\n",
       "      <th>avgModifier</th>\n",
       "      <th>stdevModifier</th>\n",
       "      <th>avgHead</th>\n",
       "      <th>stdevHead</th>\n",
       "      <th>compositionality</th>\n",
       "      <th>stdevHeadModifier</th>\n",
       "      <th>is_adj</th>\n",
       "      <th>compound</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end_NOUN</td>\n",
       "      <td>user_NOUN</td>\n",
       "      <td>3.866667</td>\n",
       "      <td>1.117537</td>\n",
       "      <td>4.866667</td>\n",
       "      <td>0.339935</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>0.871165</td>\n",
       "      <td>False</td>\n",
       "      <td>end_user</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>firing_NOUN</td>\n",
       "      <td>line_NOUN</td>\n",
       "      <td>1.607143</td>\n",
       "      <td>1.654848</td>\n",
       "      <td>1.892857</td>\n",
       "      <td>1.496169</td>\n",
       "      <td>1.703704</td>\n",
       "      <td>1.717337</td>\n",
       "      <td>False</td>\n",
       "      <td>firing_line</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>game_NOUN</td>\n",
       "      <td>plan_NOUN</td>\n",
       "      <td>2.821429</td>\n",
       "      <td>1.964935</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>3.827586</td>\n",
       "      <td>1.233693</td>\n",
       "      <td>False</td>\n",
       "      <td>game_plan</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>application_NOUN</td>\n",
       "      <td>form_NOUN</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>0.422953</td>\n",
       "      <td>4.862069</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.476095</td>\n",
       "      <td>False</td>\n",
       "      <td>application_form</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snail_NOUN</td>\n",
       "      <td>mail_NOUN</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>4.586207</td>\n",
       "      <td>1.099129</td>\n",
       "      <td>1.310345</td>\n",
       "      <td>1.020596</td>\n",
       "      <td>False</td>\n",
       "      <td>snail_mail</td>\n",
       "      <td>reddy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>radioactive_ADJ</td>\n",
       "      <td>waste_PROPN</td>\n",
       "      <td>4.916700</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>4.583300</td>\n",
       "      <td>0.668600</td>\n",
       "      <td>4.583300</td>\n",
       "      <td>0.793000</td>\n",
       "      <td>True</td>\n",
       "      <td>radioactive_waste</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>rainy_ADJ</td>\n",
       "      <td>season_PROPN</td>\n",
       "      <td>4.590900</td>\n",
       "      <td>0.854100</td>\n",
       "      <td>3.772700</td>\n",
       "      <td>1.659900</td>\n",
       "      <td>4.227300</td>\n",
       "      <td>1.066000</td>\n",
       "      <td>True</td>\n",
       "      <td>rainy_season</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>social_ADJ</td>\n",
       "      <td>insurance_PROPN</td>\n",
       "      <td>3.551700</td>\n",
       "      <td>1.325200</td>\n",
       "      <td>3.103400</td>\n",
       "      <td>1.819400</td>\n",
       "      <td>2.827600</td>\n",
       "      <td>1.691800</td>\n",
       "      <td>True</td>\n",
       "      <td>social_insurance</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>white_ADJ</td>\n",
       "      <td>noise_PROPN</td>\n",
       "      <td>0.652200</td>\n",
       "      <td>1.112300</td>\n",
       "      <td>4.043500</td>\n",
       "      <td>1.429500</td>\n",
       "      <td>1.173900</td>\n",
       "      <td>1.230400</td>\n",
       "      <td>True</td>\n",
       "      <td>white_noise</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>white_ADJ</td>\n",
       "      <td>spirit_PROPN</td>\n",
       "      <td>1.538500</td>\n",
       "      <td>1.240300</td>\n",
       "      <td>2.038500</td>\n",
       "      <td>1.949000</td>\n",
       "      <td>1.307700</td>\n",
       "      <td>1.257600</td>\n",
       "      <td>True</td>\n",
       "      <td>white_spirit</td>\n",
       "      <td>cordeiro100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1292 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              modifier             head  avgModifier  stdevModifier   avgHead  \\\n",
       "0             end_NOUN        user_NOUN     3.866667       1.117537  4.866667   \n",
       "1          firing_NOUN        line_NOUN     1.607143       1.654848  1.892857   \n",
       "2            game_NOUN        plan_NOUN     2.821429       1.964935  4.862069   \n",
       "3     application_NOUN        form_NOUN     4.766667       0.422953  4.862069   \n",
       "4           snail_NOUN        mail_NOUN     0.600000       0.800000  4.586207   \n",
       "...                ...              ...          ...            ...       ...   \n",
       "1701   radioactive_ADJ      waste_PROPN     4.916700       0.288700  4.583300   \n",
       "1702         rainy_ADJ     season_PROPN     4.590900       0.854100  3.772700   \n",
       "1707        social_ADJ  insurance_PROPN     3.551700       1.325200  3.103400   \n",
       "1718         white_ADJ      noise_PROPN     0.652200       1.112300  4.043500   \n",
       "1719         white_ADJ     spirit_PROPN     1.538500       1.240300  2.038500   \n",
       "\n",
       "      stdevHead  compositionality  stdevHeadModifier  is_adj  \\\n",
       "0      0.339935          4.250000           0.871165   False   \n",
       "1      1.496169          1.703704           1.717337   False   \n",
       "2      0.344828          3.827586           1.233693   False   \n",
       "3      0.344828          4.800000           0.476095   False   \n",
       "4      1.099129          1.310345           1.020596   False   \n",
       "...         ...               ...                ...     ...   \n",
       "1701   0.668600          4.583300           0.793000    True   \n",
       "1702   1.659900          4.227300           1.066000    True   \n",
       "1707   1.819400          2.827600           1.691800    True   \n",
       "1718   1.429500          1.173900           1.230400    True   \n",
       "1719   1.949000          1.307700           1.257600    True   \n",
       "\n",
       "               compound       source  \n",
       "0              end_user        reddy  \n",
       "1           firing_line        reddy  \n",
       "2             game_plan        reddy  \n",
       "3      application_form        reddy  \n",
       "4            snail_mail        reddy  \n",
       "...                 ...          ...  \n",
       "1701  radioactive_waste  cordeiro100  \n",
       "1702       rainy_season  cordeiro100  \n",
       "1707   social_insurance  cordeiro100  \n",
       "1718        white_noise  cordeiro100  \n",
       "1719       white_spirit  cordeiro100  \n",
       "\n",
       "[1292 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ratings_df=testset_tagger(comp_ratings_df)\n",
    "comp_ratings_df.drop_duplicates(inplace=True)\n",
    "comp_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_decades_compound(dec_list,input_dir,unique_mod_list,unique_head_list,ctype='compound'):\n",
    "\n",
    "    if os.path.exists(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\"):\n",
    "        print('Reading file')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\")\n",
    "        \n",
    "    elif os.path.exists(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl\") and args.temporal!=10000:\n",
    "        print(f'Reading decades file {ctype}s/10_{dec_list[0]}_{tag_str}.pkl')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl\")\n",
    "        \n",
    "        print(f'Reducing to {args.temporal}')\n",
    "        complete_df['time']=complete_df['time']-complete_df['time']%args.temporal\n",
    "\n",
    "        complete_df=complete_df.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\")\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_list=[]\n",
    "\n",
    "        for dec in dec_list:\n",
    "            print(dec)\n",
    "            cur_df=pd.read_pickle(f'{input_dir}/{ctype}s/{dec}.pkl')\n",
    "            \n",
    "            if not args.tag:\n",
    "                cur_df=compound_tag_remover(cur_df)\n",
    "            cur_df['time']=dec\n",
    "            cur_df['time']=cur_df['time']-cur_df['time']%args.temporal\n",
    "            df_list.append(cur_df)\n",
    "\n",
    "        print('Done reading compound dataframes')\n",
    "        complete_df=pd.concat(df_list,ignore_index=True)\n",
    "\n",
    "        if args.temporal!=10:\n",
    "            complete_df=complete_df.groupby(['modifier','head','time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\")\n",
    "        \n",
    "    reduced_complete_df=complete_df.loc[(complete_df.modifier.isin(unique_mod_list))&(complete_df['head'].isin(unique_head_list))]            \n",
    "    return reduced_complete_df\n",
    "\n",
    "\n",
    "def process_decades_constituent(dec_list,input_dir,unique_constituent_list,ctype='word'):\n",
    "        \n",
    "    if os.path.exists(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\"):\n",
    "        print('Reading file')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\")\n",
    "        \n",
    "    elif os.path.exists(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl\") and args.temporal!=10000:\n",
    "        print(f'Reading decades file {ctype}s/10_{dec_list[0]}_{tag_str}.pkl')\n",
    "        complete_df=pd.read_pickle(f\"{input_dir}/{ctype}s/10_{dec_list[0]}_{tag_str}.pkl\")\n",
    "        \n",
    "        print(f'Reducing to {args.temporal}')\n",
    "        complete_df['time']=complete_df['time']-complete_df['time']%args.temporal\n",
    "        complete_df=complete_df.groupby([ctype,'time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\")\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        df_list=[]\n",
    "\n",
    "        for dec in dec_list:\n",
    "            cur_df=pd.read_pickle(f'{input_dir}/{ctype}s/{dec}.pkl')\n",
    "            if not args.tag:\n",
    "                cur_df=constituent_tag_remover(cur_df,ctype)\n",
    "            cur_df['time']=dec\n",
    "            cur_df['time']=cur_df['time']-cur_df['time']%args.temporal\n",
    "            df_list.append(cur_df)\n",
    "\n",
    "        print(f'Done reading {ctype} dataframes')\n",
    "        complete_df=pd.concat(df_list,ignore_index=True)\n",
    "        \n",
    "        if args.temporal!=10:\n",
    "            complete_df=complete_df.groupby([ctype,'time','context'])['count'].sum().to_frame().reset_index()\n",
    "        \n",
    "        print(\"Saving file\")\n",
    "        complete_df.to_pickle(f\"{input_dir}/{ctype}s/{args.temporal}_{dec_list[0]}_{tag_str}.pkl\")\n",
    "\n",
    "    if ctype=='modifier':\n",
    "        reduced_complete_df=complete_df.loc[complete_df.modifier.isin(unique_constituent_list)]\n",
    "    elif ctype=='head':\n",
    "        reduced_complete_df=complete_df.loc[complete_df['head'].isin(unique_constituent_list)]\n",
    "    else:\n",
    "        reduced_complete_df=complete_df.loc[complete_df.word.isin(unique_constituent_list)]\n",
    "\n",
    "    return reduced_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compound_tag_remover(compounds):\n",
    "    \n",
    "    print('Removing tags for compound dataset')\n",
    "    compounds['head']=compounds['head'].str.replace('_NOUN|_PROPN','',regex=True)\n",
    "    compounds.modifier=compounds.modifier.str.replace('_NOUN|_PROPN|_ADJ','',regex=True)\n",
    "    \n",
    "    compounds=compounds.groupby(['modifier','head','context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "    return compounds\n",
    "\n",
    "\n",
    "def constituent_tag_remover(constituents,ctype='word'):\n",
    "    \n",
    "    print(f'Removing tags for {ctype} dataset')\n",
    "    constituents[ctype]=constituents[ctype].str.replace('_NOUN|_PROPN|_ADJ','',regex=True)\n",
    "    \n",
    "    constituents=constituents.groupby([ctype,'context'])['count'].sum().to_frame().reset_index()\n",
    "\n",
    "    return constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cutoff_compound(df):\n",
    "\n",
    "    df=df.loc[df.groupby(['modifier','head','time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_cutoff_constituent(df,ctype='word'):\n",
    "\n",
    "    df=df.loc[df.groupby([ctype,'time'])['count'].transform('sum').gt(args.cutoff)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ppmi(ppmi_df):\n",
    "    \n",
    "    ppmi_cols=ppmi_df.columns.tolist()\n",
    "    ppmi_cols=['XY' if 'count' in x else x for x in ppmi_cols]\n",
    "    ppmi_df.columns=ppmi_cols\n",
    "\n",
    "    ppmi_time_counts=ppmi_df.groupby('time')['XY'].sum().to_frame()\n",
    "    ppmi_time_counts.columns=['N']\n",
    "\n",
    "\n",
    "    Y_star=ppmi_df.groupby(['context','time'])['XY'].sum().to_frame()\n",
    "    Y_star.columns=['Y']\n",
    "\n",
    "    ppmi_df=pd.merge(ppmi_df,Y_star.reset_index(),on=['context','time'])\n",
    "    \n",
    "    X_cols=[x for x in ppmi_cols if x not in ['context','XY'] ]\n",
    "\n",
    "\n",
    "    X_star=ppmi_df.groupby(X_cols)['XY'].sum().to_frame()\n",
    "    X_star.columns=['X']\n",
    "\n",
    "    ppmi_df=pd.merge(ppmi_df,X_star.reset_index(),on=X_cols)\n",
    "    ppmi_df=pd.merge(ppmi_df,ppmi_time_counts.reset_index(),on=['time'])\n",
    "    ppmi_df['count']=np.log2((ppmi_df['XY']*ppmi_df['N'])/(ppmi_df['X']*ppmi_df['Y']))\n",
    "    ppmi_df=ppmi_df.loc[ppmi_df['count']>=0]\n",
    "    ppmi_df.drop(['XY','X','Y','N'],axis=1,inplace=True)\n",
    "    \n",
    "    return ppmi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_features(compounds,modifiers,heads,not_found_compounds_df):\n",
    "    \n",
    "    mod_cols=modifiers.columns.tolist()\n",
    "    mod_cols=['count' if 'count' in x else x for x in mod_cols]\n",
    "    modifiers.columns=mod_cols\n",
    "\n",
    "    head_cols=heads.columns.tolist()\n",
    "    head_cols=['count' if 'count' in x else x for x in head_cols]\n",
    "    heads.columns=head_cols\n",
    "\n",
    "    comp_cols=compounds.columns.tolist()\n",
    "    comp_cols=['count' if 'count' in x else x for x in comp_cols]\n",
    "    compounds.columns=comp_cols\n",
    "        \n",
    "    compound_denom=compounds.copy()\n",
    "    compound_denom['count']=compound_denom['count']**2\n",
    "    compound_denom=compound_denom.groupby(['modifier','head','time'])['count'].sum().to_frame()\n",
    "    compound_denom['count']=np.sqrt(compound_denom['count'])\n",
    "    compound_denom.columns=['compound_denom']\n",
    "\n",
    "    modifier_denom=modifiers.copy()\n",
    "    modifier_denom['count']=modifier_denom['count']**2\n",
    "    modifier_denom=modifier_denom.groupby(['modifier','time'])['count'].sum().to_frame()\n",
    "    modifier_denom['count']=np.sqrt(modifier_denom['count'])\n",
    "    modifier_denom.columns=['modifier_denom']\n",
    "\n",
    "    head_denom=heads.copy()\n",
    "    head_denom['count']=head_denom['count']**2\n",
    "    head_denom=head_denom.groupby(['head','time'])['count'].sum().to_frame()\n",
    "    head_denom['count']=np.sqrt(head_denom['count'])\n",
    "    head_denom.columns=['head_denom']\n",
    "\n",
    "    mod_cols=modifiers.columns.tolist()\n",
    "    mod_cols=['mod_count' if 'count' in x else x for x in mod_cols]\n",
    "    modifiers.columns=mod_cols\n",
    "\n",
    "    head_cols=heads.columns.tolist()\n",
    "    head_cols=['head_count' if 'count' in x else x for x in head_cols]\n",
    "    heads.columns=head_cols\n",
    "\n",
    "    comp_cols=compounds.columns.tolist()\n",
    "    comp_cols=['comp_count' if 'count' in x else x for x in comp_cols]\n",
    "    compounds.columns=comp_cols\n",
    "    \n",
    "    print('Calculating cosine features')\n",
    "\n",
    "    print('compound_modifier_sim')\n",
    "    compound_modifier_sim=pd.merge(compounds,modifiers,on=[\"modifier\",\"context\",'time'])\n",
    "    compound_modifier_sim['numerator']=compound_modifier_sim['comp_count']*compound_modifier_sim['mod_count']\n",
    "    compound_modifier_sim=compound_modifier_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "    compound_modifier_sim=pd.merge(compound_modifier_sim.reset_index(),compound_denom.reset_index(),on=[\"modifier\",\"head\",'time'])\n",
    "    compound_modifier_sim=pd.merge(compound_modifier_sim,modifier_denom.reset_index(),on=['modifier','time'])\n",
    "    compound_modifier_sim['sim_with_modifier']=compound_modifier_sim['numerator']/(compound_modifier_sim['compound_denom']*compound_modifier_sim['modifier_denom'])\n",
    "    compound_modifier_sim.drop(['numerator','compound_denom','modifier_denom'],axis=1,inplace=True)\n",
    "\n",
    "    print('compound_head_sim')\n",
    "    compound_head_sim=pd.merge(compounds,heads,on=[\"head\",\"context\",'time'])\n",
    "    compound_head_sim['numerator']=compound_head_sim['comp_count']*compound_head_sim['head_count']\n",
    "    compound_head_sim=compound_head_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "    compound_head_sim=pd.merge(compound_head_sim.reset_index(),compound_denom.reset_index(),on=[\"modifier\",\"head\",'time'])\n",
    "    compound_head_sim=pd.merge(compound_head_sim,head_denom.reset_index(),on=['head','time'])\n",
    "    compound_head_sim['sim_with_head']=compound_head_sim['numerator']/(compound_head_sim['compound_denom']*compound_head_sim['head_denom'])\n",
    "    compound_head_sim.drop(['numerator','compound_denom','head_denom'],axis=1,inplace=True)\n",
    "    \n",
    "    cosine_sim_feat=pd.merge(compound_modifier_sim,compound_head_sim,on=['modifier','head','time'])\n",
    "    \n",
    "    print('constituent_sim')\n",
    "\n",
    "    constituent_sim=pd.merge(heads,compounds,on=[\"head\",\"context\",\"time\"])\n",
    "    #constituent_sim.drop('comp_count',axis=1,inplace=True)\n",
    "    constituent_sim=pd.merge(constituent_sim,modifiers,on=[\"modifier\",\"context\",\"time\"])\n",
    "    constituent_sim['numerator']=constituent_sim['head_count']*constituent_sim['mod_count']\n",
    "    constituent_sim=constituent_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "    constituent_sim=pd.merge(constituent_sim.reset_index(),head_denom.reset_index(),on=[\"head\",\"time\"])\n",
    "    constituent_sim=pd.merge(constituent_sim,modifier_denom.reset_index(),on=[\"modifier\",\"time\"])\n",
    "    constituent_sim['sim_bw_constituents']=constituent_sim['numerator']/(constituent_sim['head_denom']*constituent_sim['modifier_denom'])\n",
    "    constituent_sim.drop(['numerator','modifier_denom','head_denom'],axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    not_found_constituent_sim=pd.merge(not_found_compounds_df,heads,on=[\"head\",'time'])\n",
    "    not_found_constituent_sim=pd.merge(not_found_constituent_sim,modifiers,on=[\"modifier\",'context','time'])\n",
    "    not_found_constituent_sim['numerator']=not_found_constituent_sim['head_count']*not_found_constituent_sim['mod_count']\n",
    "    not_found_constituent_sim=not_found_constituent_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "    not_found_constituent_sim=pd.merge(not_found_constituent_sim.reset_index(),head_denom.reset_index(),on=[\"head\",'time'])\n",
    "    not_found_constituent_sim=pd.merge(not_found_constituent_sim,modifier_denom.reset_index(),on=[\"modifier\",'time'])\n",
    "    not_found_constituent_sim['sim_bw_constituents']=not_found_constituent_sim['numerator']/(not_found_constituent_sim['head_denom']*not_found_constituent_sim['modifier_denom'])\n",
    "    not_found_constituent_sim.drop(['numerator','modifier_denom','head_denom'],axis=1,inplace=True)\n",
    "    \n",
    "    constituent_sim=pd.concat([constituent_sim,not_found_constituent_sim])\n",
    "\n",
    "    \n",
    "    cosine_sim_feat=pd.merge(cosine_sim_feat,constituent_sim,on=['modifier','head','time'],how='right')\n",
    "    print('Cordeiro features')\n",
    "\n",
    "    cosine_sim_feat['beta']=(cosine_sim_feat['sim_with_modifier']-cosine_sim_feat['sim_with_head']*cosine_sim_feat['sim_bw_constituents'])/\\\n",
    "    ((cosine_sim_feat['sim_with_modifier']+cosine_sim_feat['sim_with_head'])*(1-cosine_sim_feat['sim_bw_constituents']))\n",
    "    cosine_sim_feat.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    na_values = {\"beta\": 0.5}\n",
    "    cosine_sim_feat.fillna(value=na_values,inplace=True)\n",
    "\n",
    "    cosine_sim_feat['geom_mean_sim']=np.sqrt(cosine_sim_feat['sim_with_modifier']*cosine_sim_feat['sim_with_head'])\n",
    "    cosine_sim_feat['arith_mean_sim']=cosine_sim_feat[['sim_with_modifier', 'sim_with_head']].mean(axis=1)\n",
    "    \n",
    "    cpf_head_df=pd.merge(heads,head_denom.reset_index(),on=[\"head\",'time'])\n",
    "    cpf_head_df['head_value']=cpf_head_df['head_count']/cpf_head_df['head_denom']\n",
    "    cpf_head_df.drop(['head_count','head_denom'],axis=1,inplace=True)\n",
    "\n",
    "    cpf_modifier_df=pd.merge(modifiers,modifier_denom.reset_index(),on=[\"modifier\",'time'])\n",
    "    cpf_modifier_df['modifier_value']=cpf_modifier_df['mod_count']/cpf_modifier_df['modifier_denom']\n",
    "    cpf_modifier_df.drop(['mod_count','modifier_denom'],axis=1,inplace=True)\n",
    "    \n",
    "    cpf_sim=pd.merge(cpf_head_df,compounds,on=[\"head\",\"context\",\"time\"])\n",
    "    cpf_sim=pd.merge(cpf_sim,cpf_modifier_df,on=[\"modifier\",\"context\",\"time\"])\n",
    "    cpf_sim=pd.merge(cpf_sim,cosine_sim_feat[['modifier','head','beta','time']],on=[\"modifier\",'head','time'])\n",
    "\n",
    "    beta=0.0\n",
    "    cpf_sim['cp_0']=(beta*(cpf_sim['head_value'])/((cpf_sim['head_value']**2).sum()))+((1-beta)*cpf_sim['modifier_value'])\n",
    "\n",
    "    beta=0.25\n",
    "    cpf_sim['cp_25']=(beta*cpf_sim['head_value'])+((1-beta)*cpf_sim['modifier_value'])\n",
    "\n",
    "    beta=0.5\n",
    "    cpf_sim['cp_50']=(beta*cpf_sim['head_value'])+((1-beta)*cpf_sim['modifier_value'])\n",
    "\n",
    "    beta=0.75\n",
    "    cpf_sim['cp_75']=(beta*cpf_sim['head_value'])+((1-beta)*cpf_sim['modifier_value'])\n",
    "\n",
    "    beta=1\n",
    "    cpf_sim['cp_100']=(beta*cpf_sim['head_value'])+((1-beta)*cpf_sim['modifier_value'])\n",
    "\n",
    "    cpf_sim['cp_beta']=(cpf_sim['beta']*cpf_sim['head_value'])+((1-cpf_sim['beta'])*cpf_sim['modifier_value'])\n",
    "\n",
    "    temp_cdf_df=cpf_sim[['modifier','head','time','cp_0','cp_25','cp_50','cp_75','cp_100','cp_beta']].copy()\n",
    "    temp_cdf_df['denom_cp_0']=temp_cdf_df['cp_0']**2\n",
    "    temp_cdf_df['denom_cp_25']=temp_cdf_df['cp_25']**2\n",
    "    temp_cdf_df['denom_cp_50']=temp_cdf_df['cp_50']**2\n",
    "    temp_cdf_df['denom_cp_75']=temp_cdf_df['cp_75']**2\n",
    "    temp_cdf_df['denom_cp_100']=temp_cdf_df['cp_100']**2\n",
    "    temp_cdf_df['denom_cp_beta']=temp_cdf_df['cp_beta']**2\n",
    "\n",
    "    cdf_denom=temp_cdf_df.groupby(['modifier','head','time'])[['denom_cp_0','denom_cp_25','denom_cp_50','denom_cp_75','denom_cp_100','denom_cp_beta']].sum()\n",
    "    cdf_denom['denom_cp_0']=np.sqrt(cdf_denom['denom_cp_0'])\n",
    "    cdf_denom['denom_cp_25']=np.sqrt(cdf_denom['denom_cp_25'])\n",
    "    cdf_denom['denom_cp_50']=np.sqrt(cdf_denom['denom_cp_50'])\n",
    "    cdf_denom['denom_cp_75']=np.sqrt(cdf_denom['denom_cp_75'])\n",
    "    cdf_denom['denom_cp_100']=np.sqrt(cdf_denom['denom_cp_100'])\n",
    "    cdf_denom['denom_cp_beta']=np.sqrt(cdf_denom['denom_cp_beta'])\n",
    "\n",
    "    cpf_sim['num_cp_0']=cpf_sim['comp_count']*cpf_sim['cp_0']\n",
    "    cpf_sim['num_cp_25']=cpf_sim['comp_count']*cpf_sim['cp_25']\n",
    "    cpf_sim['num_cp_50']=cpf_sim['comp_count']*cpf_sim['cp_50']\n",
    "    cpf_sim['num_cp_75']=cpf_sim['comp_count']*cpf_sim['cp_75']\n",
    "    cpf_sim['num_cp_100']=cpf_sim['comp_count']*cpf_sim['cp_100']\n",
    "    cpf_sim['num_cp_beta']=cpf_sim['comp_count']*cpf_sim['cp_beta']\n",
    "\n",
    "    cpf_sim=cpf_sim.groupby(['modifier','head','time'])[['num_cp_0','num_cp_25','num_cp_50','num_cp_75','num_cp_100','num_cp_beta']].sum()\n",
    "    cpf_sim=pd.merge(cpf_sim,cdf_denom,on=['modifier','head','time'])\n",
    "    cpf_sim=pd.merge(cpf_sim,compound_denom.reset_index(),on=[\"modifier\",\"head\",\"time\"])\n",
    "    cpf_sim['sim_cpf_0']=cpf_sim['num_cp_0']/(cpf_sim['denom_cp_0']*cpf_sim['compound_denom'])\n",
    "    cpf_sim['sim_cpf_25']=cpf_sim['num_cp_25']/(cpf_sim['denom_cp_25']*cpf_sim['compound_denom'])\n",
    "    cpf_sim['sim_cpf_50']=cpf_sim['num_cp_50']/(cpf_sim['denom_cp_50']*cpf_sim['compound_denom'])\n",
    "    cpf_sim['sim_cpf_75']=cpf_sim['num_cp_75']/(cpf_sim['denom_cp_75']*cpf_sim['compound_denom'])\n",
    "    cpf_sim['sim_cpf_100']=cpf_sim['num_cp_100']/(cpf_sim['denom_cp_100']*cpf_sim['compound_denom'])\n",
    "    cpf_sim['sim_cpf_beta']=cpf_sim['num_cp_beta']/(cpf_sim['denom_cp_beta']*cpf_sim['compound_denom'])\n",
    "\n",
    "    cpf_sim=cpf_sim[['modifier','head','time','sim_cpf_0','sim_cpf_25','sim_cpf_50','sim_cpf_75','sim_cpf_100','sim_cpf_beta']].copy()\n",
    "    \n",
    "    cosine_sim_feat=cosine_sim_feat.merge(cpf_sim,on=[\"modifier\",'head','time'],how='left')\n",
    "    \n",
    "    return cosine_sim_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_setting_similarity(compounds,modifiers,heads,compounds_agnostic,modifiers_agnostic,heads_agnostic,compound_list_df):\n",
    "    \n",
    "    \n",
    "    mod_cols=modifiers.columns.tolist()\n",
    "    mod_cols=['count' if 'count' in x else x for x in mod_cols]\n",
    "    modifiers.columns=mod_cols\n",
    "\n",
    "    head_cols=heads.columns.tolist()\n",
    "    head_cols=['count' if 'count' in x else x for x in head_cols]\n",
    "    heads.columns=head_cols\n",
    "\n",
    "    comp_cols=compounds.columns.tolist()\n",
    "    comp_cols=['count' if 'count' in x else x for x in comp_cols]\n",
    "    compounds.columns=comp_cols\n",
    "\n",
    "    \n",
    "    mod_agn_cols=modifiers_agnostic.columns.tolist()\n",
    "    mod_agn_cols=['count' if 'count' in x else x for x in mod_agn_cols]\n",
    "    modifiers_agnostic.columns=mod_agn_cols\n",
    "    \n",
    "    head_agn_cols=heads_agnostic.columns.tolist()\n",
    "    head_agn_cols=['count' if 'count' in x else x for x in head_agn_cols]\n",
    "    heads_agnostic.columns=head_agn_cols\n",
    "    \n",
    "    comp_agn_cols=compounds_agnostic.columns.tolist()\n",
    "    comp_agn_cols=['count' if 'count' in x else x for x in comp_agn_cols]\n",
    "    compounds_agnostic.columns=comp_agn_cols\n",
    "    \n",
    "    print('Calculating denominator values')\n",
    "\n",
    "    compound_denom=compounds.copy()\n",
    "    compound_denom['count']=compound_denom['count']**2\n",
    "    compound_denom=compound_denom.groupby(['modifier','head','time'])['count'].sum().to_frame()\n",
    "    compound_denom['count']=np.sqrt(compound_denom['count'])\n",
    "    compound_denom.columns=['compound_denom']\n",
    "    \n",
    "    compound_agnostic_denom=compounds_agnostic.copy()\n",
    "    compound_agnostic_denom['count']=compound_agnostic_denom['count']**2\n",
    "    compound_agnostic_denom=compound_agnostic_denom.groupby(['modifier','head','time'])['count'].sum().to_frame()\n",
    "    compound_agnostic_denom['count']=np.sqrt(compound_agnostic_denom['count'])\n",
    "    compound_agnostic_denom.columns=['compound_agn_denom']\n",
    "    \n",
    "\n",
    "    modifier_denom=modifiers.copy()\n",
    "    modifier_denom['count']=modifier_denom['count']**2\n",
    "    modifier_denom=modifier_denom.groupby(['modifier','time'])['count'].sum().to_frame()\n",
    "    modifier_denom['count']=np.sqrt(modifier_denom['count'])\n",
    "    modifier_denom.columns=['modifier_denom']\n",
    "    \n",
    "    modifier_agnostic_denom=modifiers_agnostic.copy()\n",
    "    modifier_agnostic_denom['count']=modifier_agnostic_denom['count']**2\n",
    "    modifier_agnostic_denom=modifier_agnostic_denom.groupby(['modifier','time'])['count'].sum().to_frame()\n",
    "    modifier_agnostic_denom['count']=np.sqrt(modifier_agnostic_denom['count'])\n",
    "    modifier_agnostic_denom.columns=['modifier_agn_denom']\n",
    "    \n",
    "    \n",
    "    head_denom=heads.copy()\n",
    "    head_denom['count']=head_denom['count']**2\n",
    "    head_denom=head_denom.groupby(['head','time'])['count'].sum().to_frame()\n",
    "    head_denom['count']=np.sqrt(head_denom['count'])\n",
    "    head_denom.columns=['head_denom']\n",
    "    \n",
    "    head_agnostic_denom=heads_agnostic.copy()\n",
    "    head_agnostic_denom['count']=head_agnostic_denom['count']**2\n",
    "    head_agnostic_denom=head_agnostic_denom.groupby(['head','time'])['count'].sum().to_frame()\n",
    "    head_agnostic_denom['count']=np.sqrt(head_agnostic_denom['count'])\n",
    "    head_agnostic_denom.columns=['head_agn_denom'] \n",
    "    \n",
    "    \n",
    "    mod_cols=modifiers.columns.tolist()\n",
    "    mod_cols=['mod_count' if 'count' in x else x for x in mod_cols]\n",
    "    modifiers.columns=mod_cols\n",
    "\n",
    "    head_cols=heads.columns.tolist()\n",
    "    head_cols=['head_count' if 'count' in x else x for x in head_cols]\n",
    "    heads.columns=head_cols\n",
    "\n",
    "    comp_cols=compounds.columns.tolist()\n",
    "    comp_cols=['comp_count' if 'count' in x else x for x in comp_cols]\n",
    "    compounds.columns=comp_cols\n",
    "\n",
    "    \n",
    "    mod_agn_cols=modifiers_agnostic.columns.tolist()\n",
    "    mod_agn_cols=['mod_agn_count' if 'count' in x else x for x in mod_agn_cols]\n",
    "    modifiers_agnostic.columns=mod_agn_cols\n",
    "    \n",
    "    head_agn_cols=heads_agnostic.columns.tolist()\n",
    "    head_agn_cols=['head_agn_count' if 'count' in x else x for x in head_agn_cols]\n",
    "    heads_agnostic.columns=head_agn_cols\n",
    "    \n",
    "    comp_agn_cols=compounds_agnostic.columns.tolist()\n",
    "    comp_agn_cols=['comp_agn_count' if 'count' in x else x for x in comp_agn_cols]\n",
    "    compounds_agnostic.columns=comp_agn_cols\n",
    "\n",
    "    print('Calculating cosine features')\n",
    "\n",
    "    compound_setting_sim=pd.merge(compounds,compounds_agnostic,on=[\"modifier\",'head',\"context\",'time'])\n",
    "    compound_setting_sim['numerator']=compound_setting_sim['comp_count']*compound_setting_sim['comp_agn_count']\n",
    "    compound_setting_sim=compound_setting_sim.groupby(['modifier','head','time'])['numerator'].sum().to_frame()\n",
    "\n",
    "    compound_setting_sim=pd.merge(compound_setting_sim.reset_index(),compound_denom.reset_index(),on=[\"modifier\",\"head\",'time'])\n",
    "    compound_setting_sim=pd.merge(compound_setting_sim,compound_agnostic_denom.reset_index(),on=[\"modifier\",\"head\",'time'])\n",
    "\n",
    "    compound_setting_sim['sim_bw_settings_comp']=compound_setting_sim['numerator']/(compound_setting_sim['compound_denom']*compound_setting_sim['compound_agn_denom'])\n",
    "    \n",
    "    compound_setting_sim=pd.merge(compound_setting_sim,compound_list_df,on=[\"modifier\",'head','time'],how='outer')\n",
    "\n",
    "    compound_setting_sim.set_index(['modifier','head','time'],inplace=True)\n",
    "    compound_setting_sim.drop(['numerator','compound_denom','compound_agn_denom'],axis=1,inplace=True)\n",
    "\n",
    "    head_setting_sim=pd.merge(heads,heads_agnostic,on=['head',\"context\",'time'])\n",
    "    head_setting_sim['numerator']=head_setting_sim['head_count']*head_setting_sim['head_agn_count']\n",
    "    head_setting_sim=head_setting_sim.groupby(['head','time'])['numerator'].sum().to_frame()\n",
    "\n",
    "    head_setting_sim=pd.merge(head_setting_sim.reset_index(),head_denom.reset_index(),on=[\"head\",'time'])\n",
    "    head_setting_sim=pd.merge(head_setting_sim,head_agnostic_denom.reset_index(),on=[\"head\",'time'])\n",
    "\n",
    "    head_setting_sim['sim_bw_settings_head']=head_setting_sim['numerator']/(head_setting_sim['head_denom']*head_setting_sim['head_agn_denom'])\n",
    "    head_setting_sim.set_index(['head','time'],inplace=True)\n",
    "    head_setting_sim.drop(['numerator','head_denom','head_agn_denom'],axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    compound_setting_sim=pd.merge(compound_setting_sim.reset_index(),head_setting_sim.reset_index(),on=[\"head\",'time'])\n",
    "\n",
    "\n",
    "    modifier_setting_sim=pd.merge(modifiers,modifiers_agnostic,on=['modifier',\"context\",'time'])\n",
    "    modifier_setting_sim['numerator']=modifier_setting_sim['mod_count']*modifier_setting_sim['mod_agn_count']\n",
    "    modifier_setting_sim=modifier_setting_sim.groupby(['modifier','time'])['numerator'].sum().to_frame()\n",
    "\n",
    "    modifier_setting_sim=pd.merge(modifier_setting_sim.reset_index(),modifier_denom.reset_index(),on=[\"modifier\",'time'])\n",
    "    modifier_setting_sim=pd.merge(modifier_setting_sim,modifier_agnostic_denom.reset_index(),on=[\"modifier\",'time'])\n",
    "\n",
    "    modifier_setting_sim['sim_bw_settings_modifier']=modifier_setting_sim['numerator']/(modifier_setting_sim['modifier_denom']*modifier_setting_sim['modifier_agn_denom'])\n",
    "    modifier_setting_sim.set_index(['modifier','time'],inplace=True)\n",
    "    modifier_setting_sim.drop(['numerator','modifier_denom','modifier_agn_denom'],axis=1,inplace=True)\n",
    "\n",
    "    compound_setting_sim=pd.merge(compound_setting_sim,modifier_setting_sim.reset_index(),on=[\"modifier\",'time'])\n",
    "    \n",
    "    return compound_setting_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_comp_ratings(features_df):\n",
    "\n",
    "    features_df=pd.pivot_table(features_df, index=['modifier','head'], columns=['time'])\n",
    "    features_df_columns_1=features_df.columns.get_level_values(0)\n",
    "    features_df_columns_2=features_df.columns.get_level_values(1)\n",
    "\n",
    "    cur_year=0\n",
    "    new_columns=[]\n",
    "    for year in features_df_columns_2:\n",
    "        new_columns.append(features_df_columns_1[cur_year]+\":\"+str(year))\n",
    "        cur_year+=1\n",
    "\n",
    "    features_df.columns=new_columns\n",
    "    cur_ratings_df_na=features_df.reset_index().merge(comp_ratings_df,on=['modifier','head'])\n",
    "\n",
    "\n",
    "    imputer= SimpleImputer(strategy=\"median\")\n",
    "    df_med=pd.DataFrame(imputer.fit_transform(features_df))\n",
    "    df_med.columns=features_df.columns\n",
    "    df_med.index=features_df.index\n",
    "\n",
    "    cur_ratings_df_med=df_med.reset_index().merge(comp_ratings_df,on=['modifier','head'])\n",
    "    \n",
    "    return cur_ratings_df_na,cur_ratings_df_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extractor_dec(dec_list):\n",
    "    \n",
    "    print(f'Current dec list {dec_list}')\n",
    "    \n",
    "    compounds_agnostic=process_decades_compound(dec_list,f'{args.inputdir}',ctype=\"phrase\")\n",
    "\n",
    "    constituents=process_decades_constituent(dec_list,f'{args.inputdir}',ctype='word')\n",
    "    \n",
    "    \n",
    "    compounds_aware=process_decades_compound(dec_list,f'{args.inputdir}',ctype=\"compound\")\n",
    "\n",
    "    modifiers_aware=process_decades_constituent(dec_list,f'{args.inputdir}',ctype='modifier')\n",
    "\n",
    "    heads_aware=process_decades_constituent(dec_list,f'{args.inputdir}',ctype='head')\n",
    "    \n",
    "    \n",
    "    if args.cutoff==0:\n",
    "        print('No cut-off applied')          \n",
    "    else:\n",
    "        print(f'Cut-off: {args.cutoff}')\n",
    "        print(compounds_aware)\n",
    "\n",
    "        compounds_aware=process_cutoff_compound(compounds_aware)\n",
    "        \n",
    "        print(compounds_aware)\n",
    "\n",
    "        print(compounds_agnostic)\n",
    "\n",
    "        compounds_agnostic=process_cutoff_compound(compounds_agnostic)\n",
    "        \n",
    "        print(compounds_agnostic)\n",
    "\n",
    "        \n",
    "        constituents=process_cutoff_constituent(constituents,ctype='word')\n",
    "        modifiers_aware=process_cutoff_constituent(modifiers_aware,ctype='modifier')\n",
    "        heads_aware=process_cutoff_constituent(heads_aware,ctype='head')\n",
    "\n",
    "    if args.ppmi:\n",
    "        print('Applying PPMI')\n",
    "        compounds_aware=ppmi(compounds_aware)\n",
    "        modifiers_aware=ppmi(modifiers_aware)\n",
    "        heads_aware=ppmi(heads_aware)\n",
    "                        \n",
    "        compounds_agnostic=ppmi(compounds_agnostic)\n",
    "        constituents=ppmi(constituents)\n",
    "    print(compounds_aware)\n",
    "    timespan_list_aware_df=pd.DataFrame(compounds_aware.time.unique())\n",
    "    timespan_list_aware_df.columns=['time']\n",
    "\n",
    "    compound_list_aware_df=comp_ratings_df[['modifier','head']].copy()\n",
    "    compound_list_aware_df=compound_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "\n",
    "    modifier_list_aware_df=comp_ratings_df[['modifier']].drop_duplicates().copy()\n",
    "    modifier_list_aware_df=modifier_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "\n",
    "    head_list_aware_df=comp_ratings_df[['head']].drop_duplicates().copy()\n",
    "    head_list_aware_df=head_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "            \n",
    "    all_comps_aware=compounds_aware[['modifier','head','time']].copy()\n",
    "    all_comps_aware.drop_duplicates(inplace=True)\n",
    "           \n",
    "    all_mods_aware=compounds_aware[['modifier','time']].copy()\n",
    "    all_mods_aware.drop_duplicates(inplace=True)\n",
    "            \n",
    "    all_heads_aware=compounds_aware[['head','time']].copy()\n",
    "    all_heads_aware.drop_duplicates(inplace=True)\n",
    "            \n",
    "    not_found_compounds_aware_df=compound_list_aware_df.merge(all_comps_aware, on=['modifier','head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_compounds_aware_df=not_found_compounds_aware_df.loc[not_found_compounds_aware_df['_merge']=='left_only']\n",
    "    not_found_compounds_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "            \n",
    "    not_found_modifiers_aware_df=modifier_list_aware_df.merge(all_mods_aware, on=['modifier','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_modifiers_aware_df=not_found_modifiers_aware_df.loc[not_found_modifiers_aware_df['_merge']=='left_only']\n",
    "    not_found_modifiers_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "    not_found_heads_aware_df=head_list_aware_df.merge(all_heads_aware, on=['head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_heads_aware_df=not_found_heads_aware_df.loc[not_found_heads_aware_df['_merge']=='left_only']\n",
    "    not_found_heads_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    timespan_list_agnostic_df=pd.DataFrame(compounds_agnostic.time.unique())\n",
    "    timespan_list_agnostic_df.columns=['time']\n",
    "\n",
    "    compound_list_agnostic_df=comp_ratings_df[['modifier','head']].copy()\n",
    "    compound_list_agnostic_df=compound_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "\n",
    "    modifier_list_agnostic_df=comp_ratings_df[['modifier']].drop_duplicates().copy()\n",
    "    modifier_list_agnostic_df=modifier_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "\n",
    "    head_list_agnostic_df=comp_ratings_df[['head']].drop_duplicates().copy()\n",
    "    head_list_agnostic_df=head_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "            \n",
    "    all_comps_agnostic=compounds_agnostic[['modifier','head','time']].copy()\n",
    "    all_comps_agnostic.drop_duplicates(inplace=True)\n",
    "           \n",
    "    all_mods_agnostic=compounds_agnostic[['modifier','time']].copy()\n",
    "    all_mods_agnostic.drop_duplicates(inplace=True)\n",
    "            \n",
    "    all_heads_agnostic=compounds_agnostic[['head','time']].copy()\n",
    "    all_heads_agnostic.drop_duplicates(inplace=True)\n",
    "            \n",
    "    not_found_compounds_agnostic_df=compound_list_agnostic_df.merge(all_comps_agnostic, on=['modifier','head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_compounds_agnostic_df=not_found_compounds_agnostic_df.loc[not_found_compounds_agnostic_df['_merge']=='left_only']\n",
    "    not_found_compounds_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "                    \n",
    "    not_found_modifiers_agnostic_df=modifier_list_agnostic_df.merge(all_mods_agnostic, on=['modifier','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_modifiers_agnostic_df=not_found_modifiers_agnostic_df.loc[not_found_modifiers_agnostic_df['_merge']=='left_only']\n",
    "    not_found_modifiers_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "    not_found_heads_agnostic_df=head_list_agnostic_df.merge(all_heads_agnostic, on=['head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_heads_agnostic_df=not_found_heads_agnostic_df.loc[not_found_heads_agnostic_df['_merge']=='left_only']\n",
    "    not_found_heads_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    heads_agnostic=constituents.copy()\n",
    "    heads_agnostic_cols=heads_agnostic.columns\n",
    "    heads_agnostic_cols=['head' if 'word' in x else x for x in heads_agnostic_cols]\n",
    "    heads_agnostic.columns=heads_agnostic_cols\n",
    "\n",
    "    modifiers_agnostic=constituents.copy()\n",
    "    modifiers_agnostic_cols=modifiers_agnostic.columns\n",
    "    modifiers_agnostic_cols=['modifier' if 'word' in x else x for x in modifiers_agnostic_cols]\n",
    "    modifiers_agnostic.columns=modifiers_agnostic_cols\n",
    "\n",
    "    \n",
    "    print('Calculating features')\n",
    "    \n",
    "    unique_mod_list=comp_ratings_df[['modifier']].drop_duplicates()['modifier'].to_list()\n",
    "    unique_head_list=comp_ratings_df[['head']].drop_duplicates()['head'].to_list() \n",
    "    \n",
    "    print('CompoundAware features')\n",
    "    \n",
    "    \n",
    "    compound_features_aware=calculate_compound_features(compounds_aware,modifiers_aware,heads_aware,all_comps_aware,not_found_compounds_aware_df,not_found_modifiers_aware_df,not_found_heads_aware_df)\n",
    "    compound_features_aware=compound_features_aware.loc[(compound_features_aware.modifier.isin(unique_mod_list))&(compound_features_aware['head'].isin(unique_head_list))]\n",
    "    \n",
    "    reduced_compounds_aware=compounds_aware.loc[(compounds_aware.modifier.isin(unique_mod_list))&(compounds_aware['head'].isin(unique_head_list))]\n",
    "    reduced_modifiers_aware=modifiers_aware.loc[modifiers_aware.modifier.isin(unique_mod_list)]\n",
    "    reduced_heads_aware=heads_aware.loc[heads_aware['head'].isin(unique_head_list)]\n",
    "    \n",
    "    cosine_sim_feat_aware=calculate_cosine_features(reduced_compounds_aware,reduced_modifiers_aware,reduced_heads_aware,not_found_compounds_aware_df)\n",
    "  \n",
    "    \n",
    "    print('CompoundAgnostic features')\n",
    "\n",
    "    compound_features_agnostic=calculate_compound_features(compounds_agnostic,modifiers_agnostic,heads_agnostic,all_comps_agnostic,not_found_compounds_agnostic_df,not_found_modifiers_agnostic_df,not_found_heads_agnostic_df)\n",
    "    compound_features_agnostic=compound_features_agnostic.loc[(compound_features_agnostic.modifier.isin(unique_mod_list))&(compound_features_agnostic['head'].isin(unique_head_list))]\n",
    "\n",
    "    \n",
    "    reduced_compounds_agnostic=compounds_agnostic.loc[(compounds_agnostic.modifier.isin(unique_mod_list))&(compounds_agnostic['head'].isin(unique_head_list))]\n",
    "    reduced_modifiers_agnostic=modifiers_agnostic.loc[modifiers_agnostic.modifier.isin(unique_mod_list)]\n",
    "    reduced_heads_agnostic=heads_agnostic.loc[heads_agnostic['head'].isin(unique_head_list)]\n",
    "    \n",
    "    cosine_sim_feat_agnostic=calculate_cosine_features(reduced_compounds_agnostic,reduced_modifiers_agnostic,reduced_heads_agnostic,not_found_compounds_agnostic_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Combined cosine features')\n",
    "    compound_setting_sim=calculate_setting_similarity(reduced_compounds_aware,reduced_modifiers_aware,reduced_heads_aware,reduced_compounds_agnostic,reduced_modifiers_agnostic,reduced_heads_agnostic,compound_list_agnostic_df)\n",
    "    \n",
    "\n",
    "    print('Combining all compound aware features')\n",
    "    \n",
    "    features_aware_df=pd.merge(cosine_sim_feat_aware,compound_setting_sim,on=['modifier','head','time'],how='outer')\n",
    "    features_aware_df=features_aware_df.merge(compound_features_aware,on=['modifier','head','time'],how='left')\n",
    "\n",
    "    \n",
    "    print('Combining all compound agnostic features')\n",
    "    \n",
    "    features_agnostic_df=pd.merge(cosine_sim_feat_agnostic,compound_setting_sim,on=['modifier','head','time'],how='outer')\n",
    "    features_agnostic_df=features_agnostic_df.merge(compound_features_agnostic,on=['modifier','head','time'],how='left')   \n",
    "    \n",
    "    return features_aware_df,features_agnostic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10_0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dec_list=[[1820,1830,1840,1850,1860,1870,1880,1890],[1900,1910,1920,1930,1940,1950,1960,1970,1980,1990],[2000,2010]]\n",
    "    \n",
    "    \n",
    "if args.ppmi:\n",
    "    ppmi_str=\"PPMI\"\n",
    "else:\n",
    "    ppmi_str=\"RAW\"\n",
    "    \n",
    "if args.tag:\n",
    "    tag_str='Tagged'\n",
    "else:\n",
    "    tag_str='UnTagged'\n",
    "    \n",
    "temp_cutoff_str=str(args.temporal)+'_'+str(args.cutoff)\n",
    "temp_cutoff_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "    unique_mod_list=comp_ratings_df[['modifier']].drop_duplicates()['modifier'].to_list()\n",
    "    unique_head_list=comp_ratings_df[['head']].drop_duplicates()['head'].to_list() \n",
    "    unique_constituent_list=list(set(unique_mod_list+unique_head_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dec list [1820, 1830, 1840, 1850, 1860, 1870, 1880, 1890]\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Current dec list [1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990]\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Current dec list [2000, 2010]\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "Reading file\n"
     ]
    }
   ],
   "source": [
    "compounds_agnostic_list=[]\n",
    "constituents_list=[]\n",
    "compounds_aware_list=[]\n",
    "modifiers_aware_list=[]\n",
    "heads_aware_list=[]\n",
    "for dec_list in total_dec_list:\n",
    "    \n",
    "    print(f'Current dec list {dec_list}')\n",
    "    \n",
    "    cur_compounds_agnostic=process_decades_compound(dec_list,f'{args.inputdir}',unique_mod_list,unique_head_list,ctype=\"phrase\")\n",
    "\n",
    "    cur_constituents=process_decades_constituent(dec_list,f'{args.inputdir}',unique_constituent_list,ctype='word')\n",
    "    \n",
    "    \n",
    "    cur_compounds_aware=process_decades_compound(dec_list,f'{args.inputdir}',unique_mod_list,unique_head_list,ctype=\"compound\")\n",
    "\n",
    "    cur_modifiers_aware=process_decades_constituent(dec_list,f'{args.inputdir}',unique_constituent_list,ctype='modifier')\n",
    "\n",
    "    cur_heads_aware=process_decades_constituent(dec_list,f'{args.inputdir}',unique_constituent_list,ctype='head')\n",
    "    \n",
    "    compounds_agnostic_list.append(cur_compounds_agnostic)\n",
    "    constituents_list.append(cur_constituents)\n",
    "    \n",
    "    compounds_aware_list.append(cur_compounds_aware)\n",
    "    modifiers_aware_list.append(cur_modifiers_aware)\n",
    "    heads_aware_list.append(cur_heads_aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds_agnostic=pd.concat(compounds_agnostic_list,ignore_index=True)\n",
    "constituents=pd.concat(compounds_agnostic_list,ignore_index=True)\n",
    "\n",
    "compounds_aware=pd.concat(compounds_aware_list,ignore_index=True)\n",
    "modifiers_aware=pd.concat(modifiers_aware_list,ignore_index=True)\n",
    "heads_aware=pd.concat(heads_aware_list,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>head</th>\n",
       "      <th>context</th>\n",
       "      <th>count</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [modifier, head, context, count, time]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_compounds_aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dec list [1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990]\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "Done reading compound dataframes\n",
      "Saving file\n",
      "Done reading word dataframes\n",
      "Saving file\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "Done reading compound dataframes\n",
      "Saving file\n",
      "Done reading modifier dataframes\n",
      "Saving file\n",
      "Done reading head dataframes\n",
      "Saving file\n",
      "No cut-off applied\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    if args.cutoff==0:\n",
    "        print('No cut-off applied')          \n",
    "    else:\n",
    "        print(f'Cut-off: {args.cutoff}')\n",
    "        print(compounds_aware)\n",
    "\n",
    "        compounds_aware=process_cutoff_compound(compounds_aware)\n",
    "        \n",
    "        print(compounds_aware)\n",
    "\n",
    "        print(compounds_agnostic)\n",
    "\n",
    "        compounds_agnostic=process_cutoff_compound(compounds_agnostic)\n",
    "        \n",
    "        print(compounds_agnostic)\n",
    "\n",
    "        \n",
    "        constituents=process_cutoff_constituent(constituents,ctype='word')\n",
    "        modifiers_aware=process_cutoff_constituent(modifiers_aware,ctype='modifier')\n",
    "        heads_aware=process_cutoff_constituent(heads_aware,ctype='head')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating features\n",
      "CompoundAware features\n"
     ]
    }
   ],
   "source": [
    "    if args.ppmi:\n",
    "        print('Applying PPMI')\n",
    "        compounds_aware=ppmi(compounds_aware)\n",
    "        modifiers_aware=ppmi(modifiers_aware)\n",
    "        heads_aware=ppmi(heads_aware)\n",
    "                        \n",
    "        compounds_agnostic=ppmi(compounds_agnostic)\n",
    "        constituents=ppmi(constituents)\n",
    "    timespan_list_aware_df=pd.DataFrame(compounds_aware.time.unique())\n",
    "    timespan_list_aware_df.columns=['time']\n",
    "\n",
    "    compound_list_aware_df=comp_ratings_df[['modifier','head']].copy()\n",
    "    compound_list_aware_df=compound_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "\n",
    "    modifier_list_aware_df=comp_ratings_df[['modifier']].drop_duplicates().copy()\n",
    "    modifier_list_aware_df=modifier_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "\n",
    "    head_list_aware_df=comp_ratings_df[['head']].drop_duplicates().copy()\n",
    "    head_list_aware_df=head_list_aware_df.merge(timespan_list_aware_df,how='cross')\n",
    "            \n",
    "    all_comps_aware=compounds_aware[['modifier','head','time']].copy()\n",
    "    all_comps_aware.drop_duplicates(inplace=True)\n",
    "           \n",
    "    all_mods_aware=compounds_aware[['modifier','time']].copy()\n",
    "    all_mods_aware.drop_duplicates(inplace=True)\n",
    "            \n",
    "    all_heads_aware=compounds_aware[['head','time']].copy()\n",
    "    all_heads_aware.drop_duplicates(inplace=True)\n",
    "            \n",
    "    not_found_compounds_aware_df=compound_list_aware_df.merge(all_comps_aware, on=['modifier','head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_compounds_aware_df=not_found_compounds_aware_df.loc[not_found_compounds_aware_df['_merge']=='left_only']\n",
    "    not_found_compounds_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "            \n",
    "    not_found_modifiers_aware_df=modifier_list_aware_df.merge(all_mods_aware, on=['modifier','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_modifiers_aware_df=not_found_modifiers_aware_df.loc[not_found_modifiers_aware_df['_merge']=='left_only']\n",
    "    not_found_modifiers_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "    not_found_heads_aware_df=head_list_aware_df.merge(all_heads_aware, on=['head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_heads_aware_df=not_found_heads_aware_df.loc[not_found_heads_aware_df['_merge']=='left_only']\n",
    "    not_found_heads_aware_df.drop('_merge',axis=1,inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    timespan_list_agnostic_df=pd.DataFrame(compounds_agnostic.time.unique())\n",
    "    timespan_list_agnostic_df.columns=['time']\n",
    "\n",
    "    compound_list_agnostic_df=comp_ratings_df[['modifier','head']].copy()\n",
    "    compound_list_agnostic_df=compound_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "\n",
    "    modifier_list_agnostic_df=comp_ratings_df[['modifier']].drop_duplicates().copy()\n",
    "    modifier_list_agnostic_df=modifier_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "\n",
    "    head_list_agnostic_df=comp_ratings_df[['head']].drop_duplicates().copy()\n",
    "    head_list_agnostic_df=head_list_agnostic_df.merge(timespan_list_agnostic_df,how='cross')\n",
    "            \n",
    "    all_comps_agnostic=compounds_agnostic[['modifier','head','time']].copy()\n",
    "    all_comps_agnostic.drop_duplicates(inplace=True)\n",
    "           \n",
    "    all_mods_agnostic=compounds_agnostic[['modifier','time']].copy()\n",
    "    all_mods_agnostic.drop_duplicates(inplace=True)\n",
    "            \n",
    "    all_heads_agnostic=compounds_agnostic[['head','time']].copy()\n",
    "    all_heads_agnostic.drop_duplicates(inplace=True)\n",
    "            \n",
    "    not_found_compounds_agnostic_df=compound_list_agnostic_df.merge(all_comps_agnostic, on=['modifier','head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_compounds_agnostic_df=not_found_compounds_agnostic_df.loc[not_found_compounds_agnostic_df['_merge']=='left_only']\n",
    "    not_found_compounds_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "                    \n",
    "    not_found_modifiers_agnostic_df=modifier_list_agnostic_df.merge(all_mods_agnostic, on=['modifier','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_modifiers_agnostic_df=not_found_modifiers_agnostic_df.loc[not_found_modifiers_agnostic_df['_merge']=='left_only']\n",
    "    not_found_modifiers_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "            \n",
    "    not_found_heads_agnostic_df=head_list_agnostic_df.merge(all_heads_agnostic, on=['head','time'], how='outer', suffixes=['', '_'], indicator=True)\n",
    "    not_found_heads_agnostic_df=not_found_heads_agnostic_df.loc[not_found_heads_agnostic_df['_merge']=='left_only']\n",
    "    not_found_heads_agnostic_df.drop('_merge',axis=1,inplace=True)\n",
    "    \n",
    "    \n",
    "    heads_agnostic=constituents.copy()\n",
    "    heads_agnostic_cols=heads_agnostic.columns\n",
    "    heads_agnostic_cols=['head' if 'word' in x else x for x in heads_agnostic_cols]\n",
    "    heads_agnostic.columns=heads_agnostic_cols\n",
    "\n",
    "    modifiers_agnostic=constituents.copy()\n",
    "    modifiers_agnostic_cols=modifiers_agnostic.columns\n",
    "    modifiers_agnostic_cols=['modifier' if 'word' in x else x for x in modifiers_agnostic_cols]\n",
    "    modifiers_agnostic.columns=modifiers_agnostic_cols\n",
    "\n",
    "    \n",
    "    print('Calculating features')\n",
    "    \n",
    "    unique_mod_list=comp_ratings_df[['modifier']].drop_duplicates()['modifier'].to_list()\n",
    "    unique_head_list=comp_ratings_df[['head']].drop_duplicates()['head'].to_list() \n",
    "    \n",
    "    print('CompoundAware features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating productivity features\n",
      "Calculating information theory features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: invalid value encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log2\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    " compound_features_aware=calculate_compound_features(compounds_aware,modifiers_aware,heads_aware,all_comps_aware,not_found_compounds_aware_df,not_found_modifiers_aware_df,not_found_heads_aware_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cur_ratings_aware_df_na,cur_ratings_aware_df_med=merge_comp_ratings(features_aware_df)\n",
    "cur_ratings_agnostic_df_na,cur_ratings_agnostic_df_med=merge_comp_ratings(features_agnostic_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving feature datasets\n"
     ]
    }
   ],
   "source": [
    "cur_ratings_aware_df_na,cur_ratings_aware_df_med=merge_comp_ratings(features_aware_df)\n",
    "cur_ratings_agnostic_df_na,cur_ratings_agnostic_df_med=merge_comp_ratings(features_agnostic_df)\n",
    "\n",
    "print('Saving feature datasets')\n",
    "\n",
    "cur_ratings_aware_df_na.loc[:,~cur_ratings_aware_df_na.columns.str.contains('setting')].to_csv(f'{args.outputdir}/features_CompoundAware_woSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_na.csv',sep='\\t',index=False)\n",
    "cur_ratings_aware_df_med.loc[:,~cur_ratings_aware_df_med.columns.str.contains('setting')].to_csv(f'{args.outputdir}/features_CompoundAware_woSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_med.csv',sep='\\t',index=False)\n",
    "\n",
    "cur_ratings_agnostic_df_na.loc[:,~cur_ratings_agnostic_df_na.columns.str.contains('setting')].to_csv(f'{args.outputdir}/features_CompoundAgnostic_woSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_na.csv',sep='\\t',index=False)\n",
    "cur_ratings_agnostic_df_med.loc[:,~cur_ratings_agnostic_df_med.columns.str.contains('setting')].to_csv(f'{args.outputdir}/features_CompoundAgnostic_woSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_med.csv',sep='\\t',index=False)\n",
    "      \n",
    "\n",
    "cur_ratings_aware_df_na.to_csv(f'{args.outputdir}/features_CompoundAware_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_na.csv',sep='\\t',index=False)\n",
    "cur_ratings_aware_df_med.to_csv(f'{args.outputdir}/features_CompoundAware_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_med.csv',sep='\\t',index=False)\n",
    "\n",
    "cur_ratings_agnostic_df_na.to_csv(f'{args.outputdir}/features_CompoundAgnostic_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_na.csv',sep='\\t',index=False)\n",
    "cur_ratings_agnostic_df_med.to_csv(f'{args.outputdir}/features_CompoundAgnostic_withSetting_{ppmi_str}_{tag_str}_{temp_cutoff_str}_med.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/dharp/compounds/datasets/features/'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.outputdir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
