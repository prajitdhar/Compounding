{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import fastparquet\n",
    "import spacy\n",
    "import fasttext\n",
    "import glob, os\n",
    "import re\n",
    "from os.path import isfile\n",
    "\n",
    "large_files=['a_','an','of','to','in','ad','wh','be','ha','is','co','wa','he','no','it','wi','fo','re','as','on','we','punctuation','th','ma','pr','ar','ip','sh','ca','so','hi','bu','al','se','de','by','wo','st','fr','di','mo','su','at','or','yo','me','li','pa','do','ex','le','pe','po','if','ne','fi','un','fa','sa','ch','la','lo','ac','ho','mu','go','si','en','ev','tr']\n",
    "\n",
    "keep_string=r\"(.+_(NOUN|ADV|VERB|ADJ|X|PRT|CONJ|PRON|DET|ADP|NUM|\\.)|_END_)\\s*\"\n",
    "fmodel = fasttext.load_model('/data/dharp/packages/lid.176.bin')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "nn='(?!NOUN).*'\n",
    "comp='NOUN\\sNOUN'\n",
    "word='.*'\n",
    "\n",
    "n1=f'^{comp}\\s{nn}\\s{comp}$'\n",
    "n2=f'^{comp}\\s{nn}\\s{word}\\s{word}$'\n",
    "n3=f'^{nn}\\s{comp}\\s{nn}\\s{word}$'\n",
    "n4=f'^{word}\\s{nn}\\s{comp}\\s{nn}$'\n",
    "n5=f'^{word}\\s{word}\\s{nn}\\s{comp}$'\n",
    "\n",
    "\n",
    "def delist_lang(lst):\n",
    "    lang_lst=[]\n",
    "    for i,lang in enumerate(lst):\n",
    "        if not lang:\n",
    "            lang_lst.append(None)\n",
    "        else:\n",
    "            lang_lst.append(lang[0])\n",
    "    return lang_lst\n",
    "\n",
    "\n",
    "def significance(lst):\n",
    "    significance_list=[]\n",
    "    for l in lst:\n",
    "        if len(l)>1:\n",
    "            significance_list.append(abs(l[0]-l[1])/np.mean(l[0]+l[1])>0.1)\n",
    "            #print(f'{conf[0]} {conf[1]} {abs(conf[0]-conf[1])/np.mean(conf[0]+conf[1])>0.1}')\n",
    "        else:\n",
    "            significance_list.append(True)\n",
    "    return significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_maker(sent_lst):\n",
    "    ret_sents=[]\n",
    "    g_pos=[]\n",
    "    for sent in sent_lst:\n",
    "        cur_words=[]\n",
    "        pos_sent=[]\n",
    "        for word_pos in sent.split(' '):\n",
    "            word,pos=word_pos.rsplit('_',1)\n",
    "            cur_words.append(word)\n",
    "            pos_sent.append(pos)\n",
    "            cur_sent=' '.join(cur_words)\n",
    "            cur_pos=' '.join(pos_sent)\n",
    "            cur_sent=re.sub('_', '', cur_sent)\n",
    "        ret_sents.append(cur_sent)\n",
    "        g_pos.append(cur_pos)\n",
    "    return ret_sents,g_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_lemma_reducer(sent):\n",
    "    ner_sent=[]\n",
    "    lemma=[]\n",
    "    pos=[]\n",
    "    parse=[]\n",
    "    ner=[]\n",
    "    parsed_sent=nlp(sent)\n",
    "    for token in parsed_sent:\n",
    "        parse.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "\n",
    "    #print(parse)\n",
    "    parse_sent=' '.join(parse)\n",
    "    lemma_sent=' '.join(lemma)\n",
    "    pos_sent=' '.join(pos)\n",
    "    if parsed_sent.ents:\n",
    "        for ent in parsed_sent.ents:\n",
    "            cur_ner='_'.join([str(ent.start_char), str(ent.end_char), ent.label_])\n",
    "            ner.append(cur_ner)\n",
    "    else:\n",
    "        ner.append('')\n",
    "    ner_sent=' '.join(ner)\n",
    "    \n",
    "    return parse_sent,ner_sent,lemma_sent,pos_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_tagger(parsed_sent):\n",
    "    labels,confs=fmodel.predict(parsed_sent,k=-1,threshold=0.1)\n",
    "    lang_list=delist_lang(labels)    \n",
    "    significance_list=significance(confs)\n",
    "    assert len(lang_list)==len(significance_list)\n",
    "    return lang_list,significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(df):\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    df['sent']=sent_maker(df.old_index)\n",
    "    \n",
    "    results=np.vectorize(ner_lemma_reducer)(df.sent.values)\n",
    "    results_df=pd.DataFrame(results)\n",
    "    results_df=results_df.transpose()\n",
    "    results_df.columns=['parse_sent','ner_sent','lemma_sent','pos_sent']\n",
    "\n",
    "    index_df=pd.concat([df,results_df],axis=1,ignore_index=False)\n",
    "    lang_list,significance_list=lang_tagger(index_df.parse_sent.values.tolist())\n",
    "    \n",
    "    index_df['lang']=lang_list\n",
    "    index_df['lang_conf']=significance_list\n",
    "    index_df.lang=index_df.lang.str.split('_',n=4).str[-1]\n",
    "    index_df=index_df.loc[(index_df.lang=='en') &(index_df.lang_conf==True)]\n",
    "    index_df['nwords']=index_df.pos_sent.str.count(' ').add(1)\n",
    "    index_df=index_df.loc[index_df.nwords==5]\n",
    "    index_df.lemma_sent=index_df.lemma_sent.str.lower()\n",
    "    index_df.pos_sent=index_df.pos_sent.str.replace('PROPN','NOUN')\n",
    "    index_df['comp_class']=0\n",
    "\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n1),'comp_class']=1\n",
    "    index_df.loc[~(index_df.pos_sent.str.contains(n1))& index_df.pos_sent.str.contains(n2),'comp_class']=2\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n3),'comp_class']=3\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n4),'comp_class']=4\n",
    "    index_df.loc[~(index_df.pos_sent.str.contains(n1))& index_df.pos_sent.str.contains(n5),'comp_class']=5\n",
    "\n",
    "    index_df.drop(['parse_sent','sent','lang','lang_conf','nwords'],axis=1,inplace=True)\n",
    "    #print(index_df)\n",
    "    return index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def large_df_processor(letter):\n",
    "    \n",
    "    CHUNKSIZE = 800_000_000\n",
    "    num_partitions = 100\n",
    "    total_df_shape=0\n",
    "    df_list=[]\n",
    "    path_loc=\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-5gram-20120701-\"+letter+\".gz\"\n",
    "    dfs   = pd.read_csv(path_loc, compression='gzip', header=None, sep=\"\\t\", quoting=csv.QUOTE_NONE,usecols=[0,1,2],chunksize=CHUNKSIZE)    \n",
    "    for i,df in enumerate(dfs):\n",
    "        \n",
    "        print(f'Split num {i+1}')        \n",
    "        cur_time=time.time()\n",
    "        df.columns=['fivegram_pos','year','count']\n",
    "        df=df.loc[df.year>=1800]\n",
    "        index_df=df.groupby(['fivegram_pos'])['count'].sum().reset_index()\n",
    "        index_df.columns=['old_index','total_count']\n",
    "        index_df=index_df.loc[index_df.old_index.str.match(\"^\"+keep_string*5+\"$\",na=False)]\n",
    "\n",
    "        df_split = np.array_split(index_df, num_partitions)\n",
    "        pool = Pool(num_partitions)\n",
    "        print('Started parallelization')\n",
    "        results=pool.map_async(trial,df_split)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        curr_df_list=results.get()\n",
    "        #df_list.extend(curr_df_list)\n",
    "        index_df=pd.concat(curr_df_list,ignore_index=True)\n",
    "        print(f'Total time taken for split num {i+1}: {round(time.time()-cur_time)} secs')        \n",
    "\n",
    "        ntypes=index_df.shape[0]\n",
    "        ntokens=index_df.total_count.sum()\n",
    "\n",
    "        types_perc=round(ntypes/df.shape[0]*100,3)\n",
    "        print(f'Number of types: {ntypes}, perc. of unique types (decade agnostic): {types_perc}%')\n",
    "\n",
    "        print(f'Number of tokens: {ntokens}, ratio of tokens to types: {round(ntokens/ntypes,3)}')\n",
    "\n",
    "        ncomptypes=np.sum(index_df.comp_class!=0)\n",
    "        ncomptypes_perc=round(ncomptypes/ntypes*100,3)\n",
    "        print(f'Number of compounds types: {ncomptypes}, perc. of compound types: {ncomptypes_perc}%')\n",
    "\n",
    "        comp_count=index_df.loc[index_df.comp_class!=0,'total_count'].sum()\n",
    "        comp_count_perc=round(comp_count/ntokens*100,3)\n",
    "        print(f'Compound count: {comp_count}, perc. of compound tokens: {comp_count_perc}%')\n",
    "\n",
    "        words_df=index_df.loc[index_df.pos_sent.str.contains('NOUN')].reset_index(drop=True)\n",
    "        words_df['nner']=words_df.ner_sent.str.count(' ').add(1)\n",
    "        words_df['nX']=words_df.pos_sent.str.count('X')-words_df.pos_sent.str.count('AUX')\n",
    "        words_df=words_df.loc[~(words_df.nX>=3)]\n",
    "        words_df=words_df.loc[words_df.nner<2]        \n",
    "\n",
    "        words=pd.merge(df,words_df,left_on='fivegram_pos',right_on='old_index',how='right')\n",
    "        words=words.groupby(['lemma_sent','year','pos_sent','comp_class','ner_sent'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "\n",
    "        words.to_pickle(f'/data/dharp/compounds/datasets/google/{letter}{i+1}.pkl')\n",
    "        #phrases_df=words_df.loc[words_df.pos_sent.str.contains('NOUN NOUN')].reset_index(drop=True)\n",
    "        #phrases=pd.merge(df,phrases_df,left_on='fivegram_pos',right_on='old_index',how='right')\n",
    "        #phrases=phrases.groupby(['lemma_sent','year','pos_sent','comp_class','ner_sent'])['count'].sum().to_frame()\n",
    "        #phrases.reset_index(inplace=True)\n",
    "\n",
    "        #comp_df=phrases_df.loc[phrases_df.comp_class!=0].reset_index(drop=True)\n",
    "        #compounds=pd.merge(df,comp_df,left_on='fivegram_pos',right_on='old_index',how='right')\n",
    "        #compounds=compounds.groupby(['lemma_sent','year','pos_sent','comp_class','ner_sent'])['count'].sum().to_frame()\n",
    "        #compounds.reset_index(inplace=True)\n",
    "\n",
    "        print(f'Total time taken for letter {letter}: {round(time.time()-cur_time)} secs')\n",
    "        with open(f'/data/dharp/compounds/datasets/stats/{letter}{i+1}.txt','w') as f:\n",
    "            f.write(f'{letter}\\t{i+1}\\t{ntypes}\\t{ntokens}\\t{ncomptypes}\\t{comp_count}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split num 1\n",
      "Started parallelization\n",
      "Total time taken for split num 1: 3 secs\n",
      "Number of types: 15817, perc. of unique types (decade agnostic): 2.602%\n",
      "Number of tokens: 1901799, ratio of tokens to types: 120.238\n",
      "Number of compounds types: 3817, perc. of compound types: 24.132%\n",
      "Compound count: 464731, perc. of compound tokens: 24.436%\n",
      "Total time taken for letter za: 4 secs\n"
     ]
    }
   ],
   "source": [
    "large_df_processor('za')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "        num_partitions=200\n",
    "        i=0\n",
    "        path_loc=\"http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-5gram-20120701-za.gz\"\n",
    "        df   = pd.read_csv(path_loc, compression='gzip', header=None, sep=\"\\t\", quoting=csv.QUOTE_NONE,usecols=[0,1,2])    \n",
    "        cur_time=time.time()\n",
    "        df.columns=['fivegram_pos','year','count']\n",
    "        df=df.loc[df.year>=1800]\n",
    "        index_df=df.groupby(['fivegram_pos'])['count'].sum().reset_index()\n",
    "        index_df.columns=['old_index','total_count']\n",
    "        index_df=index_df.loc[index_df.old_index.str.match(\"^\"+keep_string*5+\"$\",na=False)]\n",
    "        df=index_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df.reset_index(inplace=True,drop=True)\n",
    "    ret_lst=sent_maker(df.old_index)\n",
    "    df['sent']=ret_lsts[0]\n",
    "    df['g_pos']=ret_lsts[1]\n",
    "    \n",
    "    results=np.vectorize(ner_lemma_reducer)(df.sent.values)\n",
    "    results_df=pd.DataFrame(results)\n",
    "    results_df=results_df.transpose()\n",
    "    results_df.columns=['parse_sent','ner_sent','lemma_sent','pos_sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    index_df=pd.concat([df,results_df],axis=1,ignore_index=False)\n",
    "    lang_list,significance_list=lang_tagger(index_df.parse_sent.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "    index_df['lang']=lang_list\n",
    "    index_df['lang_conf']=significance_list\n",
    "    index_df.lang=index_df.lang.str.split('_',n=4).str[-1]\n",
    "    index_df=index_df.loc[(index_df.lang=='en') &(index_df.lang_conf==True)]\n",
    "    index_df['nwords']=index_df.pos_sent.str.count(' ').add(1)\n",
    "    index_df=index_df.loc[index_df.nwords==5]\n",
    "    index_df.lemma_sent=index_df.lemma_sent.str.lower()\n",
    "    index_df.pos_sent=index_df.pos_sent.str.replace('PROPN','NOUN')\n",
    "    index_df.pos_sent=index_df.pos_sent.str.replace('AUX','VERB')\n",
    "    index_df.g_pos=index_df.g_pos.str.replace('.','PUNCT',regex=False)\n",
    "\n",
    "    index_df['comp_class']=0\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n1),'comp_class']=1\n",
    "    index_df.loc[~(index_df.pos_sent.str.contains(n1))& index_df.pos_sent.str.contains(n2),'comp_class']=2\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n3),'comp_class']=3\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n4),'comp_class']=4\n",
    "    index_df.loc[~(index_df.pos_sent.str.contains(n1))& index_df.pos_sent.str.contains(n5),'comp_class']=5\n",
    "\n",
    "    #index_df.drop(['parse_sent','sent','lang','lang_conf','nwords'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOUN ADP NOUN PUNCT ']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df.loc[index_df.sent.str.contains('END')][:1].g_pos.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_index</th>\n",
       "      <th>total_count</th>\n",
       "      <th>ner_sent</th>\n",
       "      <th>lemma_sent</th>\n",
       "      <th>pos_sent</th>\n",
       "      <th>comp_class</th>\n",
       "      <th>nner</th>\n",
       "      <th>nX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z.A.B._NOUN Zeman_NOUN ,_. The_DET Break_ADJ</td>\n",
       "      <td>46</td>\n",
       "      <td>0_12_PERSON</td>\n",
       "      <td>z.a.b. zeman , the break</td>\n",
       "      <td>NOUN NOUN PUNCT DET NOUN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z.A._NOUN ,_. The_DET Myth_NOUN of_ADP</td>\n",
       "      <td>49</td>\n",
       "      <td>0_4_GPE</td>\n",
       "      <td>z.a. , the myth of</td>\n",
       "      <td>NOUN PUNCT DET NOUN ADP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZABC_NOUN is_VERB a_DET right_ADJ angle_NOUN</td>\n",
       "      <td>61</td>\n",
       "      <td>0_4_ORG</td>\n",
       "      <td>zabc be a right angle</td>\n",
       "      <td>NOUN AUX DET ADJ NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZABEL_ADJ -_. Concerto_NOUN for_ADP Harp_NOUN</td>\n",
       "      <td>56</td>\n",
       "      <td>0_16_PERSON</td>\n",
       "      <td>zabel - concerto for harp</td>\n",
       "      <td>NOUN PUNCT NOUN ADP NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ZABEL_NOUN ,_. Editor_NOUN :_. Literary_ADJ</td>\n",
       "      <td>103</td>\n",
       "      <td>0_5_ORG</td>\n",
       "      <td>zabel , editor : literary</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT ADJ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8337</th>\n",
       "      <td>zazen_NOUN ,_. or_CONJ sitting_VERB meditation...</td>\n",
       "      <td>85</td>\n",
       "      <td></td>\n",
       "      <td>zazen , or sit meditation</td>\n",
       "      <td>NOUN PUNCT CCONJ VERB NOUN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8338</th>\n",
       "      <td>zazen_NOUN I_PRON speak_VERB of_ADP is_VERB</td>\n",
       "      <td>50</td>\n",
       "      <td></td>\n",
       "      <td>zazen -pron- speak of be</td>\n",
       "      <td>NOUN PRON VERB ADP AUX</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>zazen_VERB ,_. in_ADP a_DET brisk_ADJ</td>\n",
       "      <td>42</td>\n",
       "      <td>0_5_CARDINAL</td>\n",
       "      <td>zazen , in a brisk</td>\n",
       "      <td>NOUN PUNCT ADP DET ADJ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8341</th>\n",
       "      <td>zazzera_NOUN ,_. which_DET is_VERB as_ADV</td>\n",
       "      <td>65</td>\n",
       "      <td>0_7_ORG</td>\n",
       "      <td>zazzera , which be as</td>\n",
       "      <td>NOUN PUNCT DET AUX SCONJ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8342</th>\n",
       "      <td>zazzera_NOUN for_ADP the_DET betrothal_NOUN be...</td>\n",
       "      <td>65</td>\n",
       "      <td>0_7_ORG</td>\n",
       "      <td>zazzera for the betrothal before</td>\n",
       "      <td>NOUN ADP DET NOUN ADV</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5218 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              old_index  total_count  \\\n",
       "1          Z.A.B._NOUN Zeman_NOUN ,_. The_DET Break_ADJ           46   \n",
       "2                Z.A._NOUN ,_. The_DET Myth_NOUN of_ADP           49   \n",
       "3          ZABC_NOUN is_VERB a_DET right_ADJ angle_NOUN           61   \n",
       "4         ZABEL_ADJ -_. Concerto_NOUN for_ADP Harp_NOUN           56   \n",
       "5           ZABEL_NOUN ,_. Editor_NOUN :_. Literary_ADJ          103   \n",
       "...                                                 ...          ...   \n",
       "8337  zazen_NOUN ,_. or_CONJ sitting_VERB meditation...           85   \n",
       "8338        zazen_NOUN I_PRON speak_VERB of_ADP is_VERB           50   \n",
       "8339              zazen_VERB ,_. in_ADP a_DET brisk_ADJ           42   \n",
       "8341          zazzera_NOUN ,_. which_DET is_VERB as_ADV           65   \n",
       "8342  zazzera_NOUN for_ADP the_DET betrothal_NOUN be...           65   \n",
       "\n",
       "          ner_sent                        lemma_sent  \\\n",
       "1      0_12_PERSON          z.a.b. zeman , the break   \n",
       "2          0_4_GPE                z.a. , the myth of   \n",
       "3          0_4_ORG             zabc be a right angle   \n",
       "4      0_16_PERSON         zabel - concerto for harp   \n",
       "5          0_5_ORG         zabel , editor : literary   \n",
       "...            ...                               ...   \n",
       "8337                       zazen , or sit meditation   \n",
       "8338                        zazen -pron- speak of be   \n",
       "8339  0_5_CARDINAL                zazen , in a brisk   \n",
       "8341       0_7_ORG             zazzera , which be as   \n",
       "8342       0_7_ORG  zazzera for the betrothal before   \n",
       "\n",
       "                        pos_sent  comp_class  nner  nX  \n",
       "1       NOUN NOUN PUNCT DET NOUN           2     1   0  \n",
       "2        NOUN PUNCT DET NOUN ADP           0     1   0  \n",
       "3          NOUN AUX DET ADJ NOUN           0     1   0  \n",
       "4       NOUN PUNCT NOUN ADP NOUN           0     1   0  \n",
       "5      NOUN PUNCT NOUN PUNCT ADJ           0     1   0  \n",
       "...                          ...         ...   ...  ..  \n",
       "8337  NOUN PUNCT CCONJ VERB NOUN           0     1   0  \n",
       "8338      NOUN PRON VERB ADP AUX           0     1   0  \n",
       "8339      NOUN PUNCT ADP DET ADJ           0     1   0  \n",
       "8341    NOUN PUNCT DET AUX SCONJ           0     1   0  \n",
       "8342       NOUN ADP DET NOUN ADV           0     1   0  \n",
       "\n",
       "[5218 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_split = np.array_split(index_df, num_partitions)\n",
    "        pool = Pool(num_partitions)\n",
    "        print('Started parallelization')\n",
    "        results=pool.map_async(trial,df_split)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        \n",
    "        curr_df_list=results.get()\n",
    "        #df_list.extend(curr_df_list)\n",
    "        index_df=pd.concat(curr_df_list,ignore_index=True)\n",
    "        print(f'Total time taken for split num {i+1}: {round(time.time()-cur_time)} secs')        \n",
    "\n",
    "        ntypes=index_df.shape[0]\n",
    "        ntokens=index_df.total_count.sum()\n",
    "\n",
    "        types_perc=round(ntypes/df.shape[0]*100,3)\n",
    "        print(f'Number of types: {ntypes}, perc. of unique types (decade agnostic): {types_perc}%')\n",
    "\n",
    "        print(f'Number of tokens: {ntokens}, ratio of tokens to types: {round(ntokens/ntypes,3)}')\n",
    "\n",
    "        ncomptypes=np.sum(index_df.comp_class!=0)\n",
    "        ncomptypes_perc=round(ncomptypes/ntypes*100,3)\n",
    "        print(f'Number of compounds types: {ncomptypes}, perc. of compound types: {ncomptypes_perc}%')\n",
    "\n",
    "        comp_count=index_df.loc[index_df.comp_class!=0,'total_count'].sum()\n",
    "        comp_count_perc=round(comp_count/ntokens*100,3)\n",
    "        print(f'Compound count: {comp_count}, perc. of compound tokens: {comp_count_perc}%')\n",
    "\n",
    "        words_df=index_df.loc[index_df.pos_sent.str.contains('NOUN')].reset_index(drop=True)\n",
    "        words_df['nner']=words_df.ner_sent.str.count(' ').add(1)\n",
    "        words_df['nX']=words_df.pos_sent.str.count('X')-words_df.pos_sent.str.count('AUX')\n",
    "        words_df=words_df.loc[~(words_df.nX>=3)]\n",
    "        words_df=words_df.loc[words_df.nner<2]        \n",
    "\n",
    "        words=pd.merge(df,words_df,left_on='fivegram_pos',right_on='old_index',how='right')\n",
    "        words=words.groupby(['lemma_sent','year','pos_sent','comp_class','ner_sent'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
