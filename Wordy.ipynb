{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join,getsize\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import pickle as pkl\n",
    "\n",
    "import warnings\n",
    "#warnings.simplefilter(action='ignore', category=ResourceWarning)\n",
    "np.random.seed(seed=1991)\n",
    "from itertools import chain\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Program to run coha compounder for a particular file and setting')\n",
    "\n",
    "parser.add_argument('--input', type=str,\n",
    "                    help='location of the tsv file')\n",
    "\n",
    "parser.add_argument('--data', type=str,\n",
    "                    help='location of the hdf5 file')\n",
    "\n",
    "parser.add_argument('--word', action='store_true',\n",
    "                    help='Extracting context for words only?')\n",
    "\n",
    "parser.add_argument('--output', type=str,\n",
    "                    help='directory to save dataset in')\n",
    "\n",
    "parser.add_argument('--chunksize', type=int,default=50_000_000,\n",
    "                    help='Value of chunksize to read datasets in. If 0 is given, no chunks are created')\n",
    "\n",
    "\n",
    "args = parser.parse_args('--input ../Compounding/coha_compounds/coha_fivegrams.tsv --word --output ../Compounding/coha_compounds/ --chunksize 0'.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextwords=pkl.load( open( \"data/coha_context.pkl\", \"rb\" ) )\n",
    "\n",
    "#batched_pkl_files=pkl.load(open('../data/batched_pkl_files.pkl','rb'))\n",
    "\n",
    "#spelling_replacement={'context':br_to_us_dict,'modifier':br_to_us_dict,'head':br_to_us_dict,'word':br_to_us_dict}\n",
    "\n",
    "#words_list=pkl.load(open('novel_compound_predictor/words_list.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_word=r'.+_.+'\n",
    "any_noun=r'.+_nn1'\n",
    "proper_noun=r'[a-z.-]+_nn1'\n",
    "content_word=r'(noun|nn2|jj|rr|v)'\n",
    "space=r'\\s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relemjoin(df,col_name):\n",
    "    new_col=col_name.split('_')[0]\n",
    "    new_col_pos=new_col[0]+\"_pos\"\n",
    "    df[new_col]=df[col_name].str.split('_', 1).str[0]\n",
    "    df[new_col_pos]=\"noun\"\n",
    "    #df[new_col]=np.vectorize(lemma_maker)(df[new_col], df[new_col_pos])\n",
    "    #df.replace(spelling_replacement,inplace=True)\n",
    "    df[new_col]=df[new_col]+\"_noun\"\n",
    "    return df\n",
    "    \n",
    "def syntactic_reducer(df,align,level=None):\n",
    "    if len(df) == 0:\n",
    "        print(\"Am here\")\n",
    "        return df\n",
    "    if align==\"right\":\n",
    "        if level==\"word\":\n",
    "            #t1=time.time()\n",
    "            df=df.loc[df.fivegram_pos.str.match(r\"^\"+any_noun+space+(any_word+space)*3+any_word+\"$\")]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df['word_pos'],df['r1_pos'],df['r2_pos'],df['r3_pos'],_=df['fivegram_pos'].str.split(space).str\n",
    "            #df=df.query('word_pos == @word_list')\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            #print(time.time()-t1)\n",
    "            return df\n",
    "        else:\n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[-a-z]+_.+\\s+[-a-z]+_.+\\s+[-a-z]+_.+$')]\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+(any_noun+space)*2+(any_word+space)*2+any_word+'$')]\n",
    "\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[-a-z]+_noun\\s+[-a-z]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+(any_noun+space)*3+(any_word+space)+any_word+'$')]\n",
    "            \n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+(proper_noun+space)*2+(any_word+space)*2+any_word+'$')]\n",
    "\n",
    "            try:\n",
    "                phrases[['modifier_pos','head_pos','r1_pos','r2_pos','r3_pos']]=phrases['fivegram_pos'].str.split(space, expand=True)\n",
    "                cdsm[['modifier_pos','head_pos','r1_pos','r2_pos','r3_pos']]=cdsm['fivegram_pos'].str.split(space, expand=True)\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            \n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['r1_pos','r2_pos','r3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','r1_pos','r2_pos','r3_pos'])\n",
    "\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid1\":\n",
    "        if level==\"word\":\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            df=df.loc[df.fivegram_pos.str.match(r\"^\"+any_word+space+any_noun+space+(any_word+space)*2+any_word+\"$\")]\n",
    "\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            \n",
    "            df[['l1_pos','word_pos','r1_pos','r2_pos','r3_pos']]=df['fivegram_pos'].str.split(space, expand=True)\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','r1_pos','r2_pos','r3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+any_word+space+(any_noun+space)*2+any_word+space+any_word+'$')]\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "\n",
    "            \n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+(any_noun+space)*4+any_word+'$')]\n",
    "            \n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+any_word+space+(proper_noun+space)*2+any_word+space+any_word+'$')]\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                phrases[['l1_pos','modifier_pos','head_pos','r1_pos','r2_pos']]=phrases['fivegram_pos'].str.split(space, expand=True)\n",
    "                cdsm[['l1_pos','modifier_pos','head_pos','r1_pos','r2_pos']]=cdsm['fivegram_pos'].str.split(space, expand=True)\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','r1_pos','r2_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','l1_pos','r1_pos','r2_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','l1_pos','r1_pos','r2_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "    \n",
    "            \n",
    "    elif align==\"mid2\":\n",
    "        if level==\"word\":\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+\\s+[a-z-]+_.+$')]\n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*2+any_noun+space+any_word+space+any_word+'$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "           \n",
    "            df[['l1_pos','l2_pos','word_pos','r1_pos','r2_pos']]=df['fivegram_pos'].str.split(space, expand=True,n=4)\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            \n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "            \n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*2+(any_noun+space)*2+any_word+'$')]\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "\n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+any_word+space+(any_noun+space)*3+any_word+'$')]\n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+(any_word+space)*2+(proper_noun+space)*2+any_word+'$')]\n",
    "            try:\n",
    "                phrases[['l1_pos','l2_pos','modifier_pos','head_pos','r1_pos']]=phrases['fivegram_pos'].str.split(space, expand=True)\n",
    "                cdsm[['l1_pos','l2_pos','modifier_pos','head_pos','r1_pos']]=cdsm['fivegram_pos'].str.split(space, expand=True)\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','r1_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','l1_pos','l2_pos','r1_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','l1_pos','l2_pos','r1_pos'])\n",
    "            return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            \n",
    "    elif align==\"mid3\":\n",
    "        #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_.+$')]\n",
    "        df=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*3+any_noun+space+any_word+'$')]\n",
    "        if len(df)==0:\n",
    "            return df\n",
    "\n",
    "        df[['l1_pos','l2_pos','word_pos','r1_pos','r2_pos']]=df['fivegram_pos'].str.split(space, expand=True, n=4)\n",
    "        df=relemjoin(df,'word_pos')\n",
    "        df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','l2_pos','r1_pos','r2_pos'])\n",
    "        return df\n",
    "        \n",
    "    elif align==\"left\":\n",
    "        \n",
    "        if level==\"word\":\n",
    "            #df=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            df=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*4+any_noun+'$')]\n",
    "            if len(df) == 0:\n",
    "                return df\n",
    "            _,df['l1_pos'],df['l2_pos'],df['l3_pos'],df['word_pos']=df['fivegram_pos'].str.split(space).str\n",
    "            df=relemjoin(df,'word_pos')\n",
    "            df=pd.melt(df,id_vars=['word','year','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            return df\n",
    "        else:\n",
    "            #phrases=df.loc[df.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            phrases=df.loc[df.fivegram_pos.str.match(r'^'+(any_word+space)*3+any_noun+space+any_noun+'$')]\n",
    "            #cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^[a-z-]+_.+\\s+[a-z-]+_.+\\s+[a-z-]+_noun\\s+[a-z-]+_noun\\s+[a-z-]+_noun$')]\n",
    "            \n",
    "            \n",
    "            cdsm=phrases.loc[~phrases.fivegram_pos.str.match(r'^'+(any_word+space)*2+(any_noun+space)*2+any_noun+'$')]\n",
    "            cdsm=cdsm.loc[cdsm.fivegram_pos.str.match(r'^'+(any_word+space)*3+proper_noun+space+proper_noun+'$')]\n",
    "            \n",
    "            try:\n",
    "                phrases[['l1_pos','l2_pos','l3_pos','modifier_pos','head_pos']]=phrases['fivegram_pos'].str.split(space, expand=True)\n",
    "                cdsm[['l1_pos','l2_pos','l3_pos','modifier_pos','head_pos']]=cdsm['fivegram_pos'].str.split(space, expand=True)\n",
    "            except ValueError:\n",
    "                phrases=pd.DataFrame()\n",
    "                compounds=pd.DataFrame()\n",
    "                modifiers=pd.DataFrame()\n",
    "                heads=pd.DataFrame()\n",
    "                return phrases,compounds,modifiers,heads\n",
    "            \n",
    "            phrases=relemjoin(phrases,'modifier_pos')\n",
    "            phrases=relemjoin(phrases,'head_pos')\n",
    "            cdsm=relemjoin(cdsm,'modifier_pos')\n",
    "            cdsm=relemjoin(cdsm,'head_pos')\n",
    "            \n",
    "            phrases=pd.melt(phrases,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            compounds=pd.melt(cdsm,id_vars=['modifier','head','year','count'],value_vars=['l1_pos','l2_pos','l3_pos'])\n",
    "            modifiers=pd.melt(cdsm,id_vars=['modifier','year','count'],value_vars=['head','l1_pos','l2_pos','l3_pos'])\n",
    "            heads=pd.melt(cdsm,id_vars=['head','year','count'],value_vars=['modifier','l1_pos','l2_pos','l3_pos'])\n",
    "            return phrases,compounds,modifiers,heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_reducer(df):\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    df[\"variable\"]=df[\"variable\"].str.replace(r\"_pos\",\"\")\n",
    "    df.dropna(inplace=True)\n",
    "    df[[\"context\",\"context_pos\"]]=df['value'].str.split('_', 1, expand=True)\n",
    "    \n",
    "    df=df.loc[df.context_pos.str.match(r\"^\"+content_word)]\n",
    "    # Rename parts of speech to match the pos label\n",
    "    # from the Google Ngrams data in @contextwords\n",
    "    df.context_pos = df.context_pos.str.replace(\"rr\", \"adv\")\n",
    "    df.context_pos = df.context_pos.str.replace(\"nn2\", \"noun\")\n",
    "    df.context_pos = df.context_pos.str.replace(\"^v.+\", \"verb\")\n",
    "    df.context_pos = df.context_pos.str.replace(\"jj\", \"adj\")\n",
    "    #df.replace(adv_replacement,inplace=True)\n",
    "    #df['context_pos']=df['context_pos'].str[0]\n",
    "    if len(df)==0:\n",
    "        return df\n",
    "    #df['context']=np.vectorize(lemma_maker)(df['context'], df['context_pos'])\n",
    "    #df.replace(spelling_replacement,inplace=True)\n",
    "    df['context']=df['context']+\"_\"+df['context_pos']\n",
    "\n",
    "    df.query('context in @contextwords',inplace=True)\n",
    "    df.drop(['variable','value','context_pos'],axis=1,inplace=True)\n",
    "    #df.reset_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def cdsm_word_reducer(df):\n",
    "    rightgram=syntactic_reducer(df,align=\"right\",level=\"word\")\n",
    "    rightgram=context_reducer(rightgram)\n",
    "    \n",
    "    mid1gram=syntactic_reducer(df,align=\"mid1\",level=\"word\")\n",
    "    mid1gram=context_reducer(mid1gram)\n",
    "    \n",
    "    mid2gram=syntactic_reducer(df,align=\"mid2\",level=\"word\")\n",
    "    mid2gram=context_reducer(mid2gram)\n",
    "    \n",
    "    mid3gram=syntactic_reducer(df,align=\"mid3\",level=\"word\")\n",
    "    mid3gram=context_reducer(mid3gram)\n",
    "    \n",
    "    leftgram=syntactic_reducer(df,align=\"left\",level=\"word\")\n",
    "    leftgram=context_reducer(leftgram)    \n",
    "    \n",
    "    words_df=pd.concat([rightgram,mid1gram,mid2gram,mid3gram,leftgram],ignore_index=True,sort=False)\n",
    "    words_df.dropna(inplace=True)\n",
    "    #words_df=words_df.query('word in @words_list')\n",
    "    words_df=words_df.groupby(['word','context','year'])['count'].sum().to_frame()\n",
    "    words_df.reset_index(inplace=True)\n",
    "    words_df.year=words_df.year.astype(\"int32\")\n",
    "    #print(words_df.shape)\n",
    "    return words_df\n",
    "\n",
    "def cdsm_reducer(df):\n",
    "    phrase_rightgram,compound_rightgram,modifier_rightgram,head_rightgram=syntactic_reducer(df,align=\"right\")\n",
    "    phrase_rightgram=context_reducer(phrase_rightgram)\n",
    "    compound_rightgram=context_reducer(compound_rightgram)\n",
    "    modifier_rightgram=context_reducer(modifier_rightgram)\n",
    "    head_rightgram=context_reducer(head_rightgram)\n",
    "\n",
    "\n",
    "    phrase_mid1gram,compound_mid1gram,modifier_mid1gram,head_mid1gram=syntactic_reducer(df,align=\"mid1\")\n",
    "    phrase_mid1gram=context_reducer(phrase_mid1gram)\n",
    "    compound_mid1gram=context_reducer(compound_mid1gram)\n",
    "    modifier_mid1gram=context_reducer(modifier_mid1gram)\n",
    "    head_mid1gram=context_reducer(head_mid1gram)\n",
    " \n",
    "\n",
    "    phrase_mid2gram,compound_mid2gram,modifier_mid2gram,head_mid2gram=syntactic_reducer(df,align=\"mid2\")\n",
    "    phrase_mid2gram=context_reducer(phrase_mid2gram)\n",
    "    compound_mid2gram=context_reducer(compound_mid2gram)\n",
    "    modifier_mid2gram=context_reducer(modifier_mid2gram)\n",
    "    head_mid2gram=context_reducer(head_mid2gram)\n",
    "    \n",
    "    phrase_leftgram,compound_leftgram,modifier_leftgram,head_leftgram=syntactic_reducer(df,align=\"left\")\n",
    "    phrase_leftgram=context_reducer(phrase_leftgram)\n",
    "    compound_leftgram=context_reducer(compound_leftgram)\n",
    "    modifier_leftgram=context_reducer(modifier_leftgram)\n",
    "    head_leftgram=context_reducer(head_leftgram)\n",
    "    \n",
    "    \n",
    "    phrases=pd.concat([phrase_rightgram,phrase_mid1gram,phrase_mid2gram,phrase_leftgram],ignore_index=True,sort=False)\n",
    "    compounds=pd.concat([compound_rightgram,compound_mid1gram,compound_mid2gram,compound_leftgram],ignore_index=True,sort=False)\n",
    "    modifiers=pd.concat([modifier_rightgram,modifier_mid1gram,modifier_mid2gram,modifier_leftgram],ignore_index=True,sort=False)\n",
    "    heads=pd.concat([head_rightgram,head_mid1gram,head_mid2gram,head_leftgram],ignore_index=True,sort=False)\n",
    "\n",
    "    \n",
    "    phrases.dropna(inplace=True)\n",
    "    phrases=phrases.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "    phrases.reset_index(inplace=True)\n",
    "    phrases.year=phrases.year.astype(\"int32\")\n",
    "    \n",
    "    compounds.dropna(inplace=True)\n",
    "    compounds=compounds.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "    compounds.reset_index(inplace=True)\n",
    "    compounds.year=compounds.year.astype(\"int32\")\n",
    "    \n",
    "    modifiers.dropna(inplace=True)\n",
    "    modifiers=modifiers.groupby(['modifier','context','year'])['count'].sum().to_frame()\n",
    "    modifiers.reset_index(inplace=True)\n",
    "    modifiers.year=modifiers.year.astype(\"int32\")\n",
    "    \n",
    "    heads.dropna(inplace=True)\n",
    "    heads=heads.groupby(['head','context','year'])['count'].sum().to_frame()\n",
    "    heads.reset_index(inplace=True)\n",
    "    heads.year=heads.year.astype(\"int32\")\n",
    "    return compounds,modifiers,heads,phrases\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df,save_loc,num_cores):\n",
    "    num_partitions = num_cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_cores)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if not args.word:\n",
    "\n",
    "        results=pool.map_async(cdsm_reducer,df_split)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        results=results.get()\n",
    "\n",
    "        \n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        compound_list = [ result[0] for result in results]\n",
    "        compounds=pd.concat(compound_list,ignore_index=True)\n",
    "        compounds=compounds.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(args.output+\"/compounds.csv\"):\n",
    "            compounds.to_csv(args.output+\"/compounds.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            compounds.to_csv(args.output+\"/compounds.csv\", mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "        \n",
    "        modifier_list = [ result[1] for result in results]\n",
    "        modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "        modifiers=modifiers.groupby(['modifier','context','year'])['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(args.output+\"/modifiers.csv\"):\n",
    "            modifiers.to_csv(args.output+\"/modifiers.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            modifiers.to_csv(args.output+\"/modifiers.csv\", mode='a',sep=\"\\t\",header=False,index=False)\n",
    "        \n",
    "        head_list = [ result[2] for result in results]\n",
    "        heads=pd.concat(head_list,ignore_index=True)\n",
    "        heads=heads.groupby(['head','context','year'])['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(args.output+\"/heads.csv\"):\n",
    "            heads.to_csv(args.output+\"/heads.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            heads.to_csv(args.output+\"/heads.csv\", mode='a',sep=\"\\t\",header=False,index=False)\n",
    "            \n",
    "        phrase_list = [ result[3] for result in results]\n",
    "        phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "        phrases=phrases.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "        phrases.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(args.output+\"/phrases.csv\"):\n",
    "            phrases.to_csv(args.output+\"/phrases.csv\",sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            phrases.to_csv(args.output+\"/phrases.csv\", mode='a',sep=\"\\t\",header=False,index=False)\n",
    "\n",
    "    else:\n",
    "        words_list=[]\n",
    "        results=pool.map_async(cdsm_word_reducer,df_split)\n",
    "  \n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        words_list=results.get()\n",
    "        words = pd.concat(words_list,ignore_index=True,sort=False)\n",
    "        words=words.groupby(['word','context','year'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(words.shape)\n",
    "                \n",
    "        if not isfile(save_loc):\n",
    "            words.to_csv(save_loc,sep=\"\\t\",index=False,header=True)\n",
    "        else:\n",
    "            words.to_csv(save_loc, mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "    print(\"Done concatenations \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=args.input\n",
    "df=pd.read_csv(args.input, sep=\"\\t\",header=None)\n",
    "df.columns = [\"fivegram_pos\", \"year\", \"count\", \"volume\"]\n",
    "str_num=file_name.split('/')[-1].split('.')[0].split('_')[-1]\n",
    "if args.word:\n",
    "    output_file=args.output+'/words.csv'\n",
    "else:\n",
    "    output_file=args.output+'/context.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users0/pageljs/dh/build/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n"
     ]
    }
   ],
   "source": [
    "rightgram=syntactic_reducer(df,align=\"right\",level=\"word\")\n",
    "#rightgram=context_reducer(rightgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid2gram=syntactic_reducer(df,align=\"mid2\",level=\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid2gram.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "testme=context_reducer(mid2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>context</th>\n",
       "      <th>context_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100116</th>\n",
       "      <td>the_noun</td>\n",
       "      <td>1850</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>%s</td>\n",
       "      <td>%s</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444856</th>\n",
       "      <td>'_noun</td>\n",
       "      <td>1910</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>three-months</td>\n",
       "      <td>three-months</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900345</th>\n",
       "      <td>of_noun</td>\n",
       "      <td>1940</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>passenger-miles</td>\n",
       "      <td>passenger-miles</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425348</th>\n",
       "      <td>\"_noun</td>\n",
       "      <td>1970</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>ws</td>\n",
       "      <td>ws</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958467</th>\n",
       "      <td>of_noun</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>decibels</td>\n",
       "      <td>decibels</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044375</th>\n",
       "      <td>,_noun</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>plug-ins</td>\n",
       "      <td>plug-ins</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159212</th>\n",
       "      <td>;_noun</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>decibels</td>\n",
       "      <td>decibels</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168429</th>\n",
       "      <td>without_noun</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>l1</td>\n",
       "      <td>decibels</td>\n",
       "      <td>decibels</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607812</th>\n",
       "      <td>_noun</td>\n",
       "      <td>1900</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>ohms</td>\n",
       "      <td>ohms</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787320</th>\n",
       "      <td>_noun</td>\n",
       "      <td>1920</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>three-times</td>\n",
       "      <td>three-times</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3397416</th>\n",
       "      <td>_noun</td>\n",
       "      <td>1960</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>eight-months</td>\n",
       "      <td>eight-months</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563860</th>\n",
       "      <td>_noun</td>\n",
       "      <td>1960</td>\n",
       "      <td>1</td>\n",
       "      <td>l2</td>\n",
       "      <td>maritimes</td>\n",
       "      <td>maritimes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word  year  count variable            value          context  \\\n",
       "100116       the_noun  1850      1       l1               %s               %s   \n",
       "444856         '_noun  1910      1       l1     three-months     three-months   \n",
       "900345        of_noun  1940      1       l1  passenger-miles  passenger-miles   \n",
       "1425348        \"_noun  1970      1       l1               ws               ws   \n",
       "1958467       of_noun  1990      1       l1         decibels         decibels   \n",
       "2044375        ,_noun  2000      1       l1         plug-ins         plug-ins   \n",
       "2159212        ;_noun  2000      1       l1         decibels         decibels   \n",
       "2168429  without_noun  2000      1       l1         decibels         decibels   \n",
       "2607812         _noun  1900      1       l2             ohms             ohms   \n",
       "2787320         _noun  1920      1       l2      three-times      three-times   \n",
       "3397416         _noun  1960      1       l2     eight-months     eight-months   \n",
       "3563860         _noun  1960      1       l2        maritimes        maritimes   \n",
       "\n",
       "        context_pos  \n",
       "100116         None  \n",
       "444856         None  \n",
       "900345         None  \n",
       "1425348        None  \n",
       "1958467        None  \n",
       "2044375        None  \n",
       "2159212        None  \n",
       "2168429        None  \n",
       "2607812        None  \n",
       "2787320        None  \n",
       "3397416        None  \n",
       "3563860        None  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testme[testme.isna().any(axis=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
