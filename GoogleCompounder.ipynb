{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import fastparquet\n",
    "import pickle\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from os.path import isfile\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Program to run google compounder for a particular file and setting')\n",
    "\n",
    "parser.add_argument('--data', type=str,\n",
    "                    help='location of the pickle file')\n",
    "\n",
    "parser.add_argument('--word', action='store_true',\n",
    "                    help='Extracting context for words only?')\n",
    "\n",
    "parser.add_argument('--output', type=str,\n",
    "                    help='directory to save dataset in')\n",
    "\n",
    "\n",
    "args = parser.parse_args('--data /data/dharp/compounds/datasets/entire_df_v3/ --output /data/dharp/compounds/datasets --word'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53207"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_list = pickle.load( open( f'{args.output}/contexts/contexts_top50k.pkl', \"rb\" ) )\n",
    "len(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_id_vars=['modifier','head','year','count','num_comp','comp_ner_sent']\n",
    "modifier_id_vars=['modifier','year','count','num_comp','comp_ner_sent']\n",
    "head_id_vars=['head','year','count','num_comp','comp_ner_sent']\n",
    "word_id_vars=['word','year','count','num_comp','comp_ner_sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='.+_.+'\n",
    "comp='.+_(?:NOUN|PROPN)\\s.+_(?:NOUN|PROPN)'\n",
    "\n",
    "\n",
    "p2=f'^{comp}\\s{word}\\s{word}\\s{word}$'\n",
    "p3=f'^{word}\\s{comp}\\s{word}\\s{word}$'\n",
    "p4=f'^{word}\\s{word}\\s{comp}\\s{word}$'\n",
    "p5=f'^{word}\\s{word}\\s{word}\\s{comp}$'\n",
    "\n",
    "phrase_dict={2:p2,3:p3,4:p4,5:p5}\n",
    "\n",
    "noun='.+_(?:NOUN|PROPN)'\n",
    "\n",
    "n1=f'^{noun}\\s{word}\\s{word}\\s{word}\\s{word}$'\n",
    "n2=f'^{word}\\s{noun}\\s{word}\\s{word}\\s{word}$'\n",
    "n3=f'^{word}\\s{word}\\s{noun}\\s{word}\\s{word}$'\n",
    "n4=f'^{word}\\s{word}\\s{word}\\s{noun}\\s{word}$'\n",
    "n5=f'^{word}\\s{word}\\s{word}\\s{word}\\s{noun}$'\n",
    "\n",
    "word_dict={1:n1,2:n2,3:n3,4:n4,5:n5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_side_parser(df,phrases=False): # N N _ _ _ \n",
    "    cur_df=df.copy()\n",
    "    if not phrases:\n",
    "\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "        \n",
    "\n",
    "def mid1_parser(df,phrases=False): # _ N N _ _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "def mid2_parser(df,phrases=False): # _ _ N N _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "def right_side_parser(df,phrases=False): # _ _ _ N N\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df,phrases=False):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # N N _ N N\n",
    "        compound_left_df,modifier_left_df,head_left_df=left_side_parser(df)\n",
    "        compound_right_df,modifier_right_df,head_right_df=right_side_parser(df)\n",
    "        \n",
    "        final_compound_df=pd.concat([compound_left_df,compound_right_df],ignore_index=True)\n",
    "        final_modifier_df=pd.concat([modifier_left_df,modifier_right_df],ignore_index=True)\n",
    "        final_head_df=pd.concat([head_left_df,head_right_df],ignore_index=True)\n",
    "           \n",
    "    elif pattern==2: # N N _ _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=left_side_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=left_side_parser(df,phrases=True)\n",
    "\n",
    "    elif pattern==3: # _ N N _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid1_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid1_parser(df,phrases=True)\n",
    "    \n",
    "    elif pattern==4: # _ _ N N _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid2_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid2_parser(df,phrases=True)\n",
    "        \n",
    "    elif pattern==5: # _ _ _ N N\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=right_side_parser(df)   \n",
    "        else:\n",
    "            final_phrases_df=right_side_parser(df,phrases=True)\n",
    "\n",
    "            \n",
    "\n",
    "    if not phrases:\n",
    "        return final_compound_df,final_modifier_df,final_head_df\n",
    "    else:\n",
    "        return final_phrases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_extracter(df,phrases=False):\n",
    "    \n",
    "    comp_df_list=[]\n",
    "    head_df_list=[]\n",
    "    mod_df_list=[]\n",
    "    phrase_df_list=[]\n",
    "    if not phrases:\n",
    "        \n",
    "        for i in range(1,6):\n",
    "            \n",
    "            if df.loc[df.comp_class==i].shape[0]!=0:\n",
    "                cur_comp_df,cur_mod_df,cur_head_df=syntactic_reducer(df.loc[df.comp_class==i])\n",
    "\n",
    "                comp_df_list.append(cur_comp_df)\n",
    "                mod_df_list.append(cur_mod_df)\n",
    "                head_df_list.append(cur_head_df)\n",
    "    \n",
    "        compounds=pd.concat(comp_df_list,ignore_index=True,sort=False)\n",
    "        modifiers=pd.concat(mod_df_list,ignore_index=True,sort=False)\n",
    "        heads=pd.concat(head_df_list,ignore_index=True,sort=False)\n",
    "\n",
    "        \n",
    "        compounds.dropna(inplace=True)\n",
    "        compounds=compounds.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "\n",
    "        modifiers.dropna(inplace=True)\n",
    "        modifiers=modifiers.groupby(['modifier','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        heads.dropna(inplace=True)\n",
    "        heads=heads.groupby(['head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        return compounds,modifiers,heads\n",
    "    \n",
    "    else:\n",
    "        for i in range(2,6):\n",
    "            cur_df=df.loc[df.lemma_pos.str.contains(phrase_dict[i])].copy()\n",
    "            if cur_df.shape[0]!=0:\n",
    "                cur_df.comp_class=i\n",
    "                cur_phrase_df=syntactic_reducer(cur_df,phrases=True)\n",
    "                phrase_df_list.append(cur_phrase_df)\n",
    "                \n",
    "                \n",
    "        if phrase_df_list==[]:\n",
    "            return None\n",
    "        else:\n",
    "            phrases=pd.concat(phrase_df_list,ignore_index=True,sort=False)\n",
    "\n",
    "            phrases.dropna(inplace=True)\n",
    "            phrases=phrases.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            phrases.reset_index(inplace=True)\n",
    "\n",
    "            return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_reducer(df):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # N _ _ _ _\n",
    "        df[['word','w1','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "    elif pattern==2: # _ N _ _ _\n",
    "        df[['w1','word','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==3: # _ _ N _ _\n",
    "        df[['w1','w2','word','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==4: # _ _ _ N _\n",
    "        df[['w1','w2','w3','word','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==5: # _ _ _ _ N\n",
    "        df[['w1','w2','w3','w4','word']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w2','w3','w4'],value_name='context')\n",
    "\n",
    "    return word_df\n",
    "\n",
    "\n",
    "def word_extractor(df):\n",
    "    word_df_list=[]\n",
    "\n",
    "    for i in range(1,6):\n",
    "        cur_df=df.loc[df.lemma_pos.str.contains(word_dict[i])].copy()\n",
    "        if cur_df.shape[0]!=0:\n",
    "            cur_df.comp_class=i\n",
    "            cur_word_df=word_reducer(cur_df)\n",
    "            word_df_list.append(cur_word_df)\n",
    "\n",
    "    words=pd.concat(word_df_list,ignore_index=True,sort=False)\n",
    "        \n",
    "    words.dropna(inplace=True)\n",
    "    words=words.groupby(['word','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "    words.reset_index(inplace=True)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df,phrases=False):\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_partitions)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if not args.word:\n",
    "        \n",
    "        if not phrases:\n",
    "            #Processing heads, modifiers and compounds for Compound Aware\n",
    "\n",
    "            results=pool.map_async(compound_extracter,df_split)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            results=results.get()\n",
    "\n",
    "            print(\"Done parallelizing\")\n",
    "            compound_list = [ result[0] for result in results]\n",
    "            compounds=pd.concat(compound_list,ignore_index=True)\n",
    "            compounds=compounds.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            compounds.reset_index(inplace=True)\n",
    "\n",
    "            modifier_list = [ result[1] for result in results]\n",
    "            modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "            modifiers=modifiers.groupby(['modifier','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            modifiers.reset_index(inplace=True)\n",
    "\n",
    "            head_list = [ result[2] for result in results]\n",
    "            heads=pd.concat(head_list,ignore_index=True)\n",
    "            heads=heads.groupby(['head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            heads.reset_index(inplace=True)\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return compounds,modifiers,heads\n",
    "        else:\n",
    "            \n",
    "            #Processing phrases for Compound Agnostic\n",
    "            results=pool.starmap_async(compound_extracter,zip(df_split,repeat(phrases)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            phrase_list=results.get()\n",
    "            \n",
    "            print(\"Done parallelizing\")\n",
    "            \n",
    "            phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "            phrases=phrases.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "            phrases.reset_index(inplace=True)\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return phrases\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Processing words for Compound Agnostic\n",
    "\n",
    "        words_list=[]\n",
    "        results=pool.map_async(word_extractor,df_split)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        words_list=results.get()\n",
    "\n",
    "        print(\"Done parallelizing\")\n",
    "        \n",
    "        words = pd.concat(words_list,ignore_index=True)\n",
    "        words=words.groupby(['word','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_processor(f):   \n",
    "    cur_fname=f.split('.')[0].split('/')[-1]\n",
    "    print(f'Current parquet file: {f}')\n",
    "    cur_parq=fastparquet.ParquetFile(f)\n",
    "\n",
    "    print(f'Number of partitions: {len(cur_parq.row_groups)}')\n",
    "    compounds_list=[]\n",
    "    modifiers_list=[]\n",
    "    heads_list=[]\n",
    "    phrases_list=[]\n",
    "    words_list=[]\n",
    "\n",
    "    \n",
    "    for i,cur_df in enumerate(cur_parq.iter_row_groups()):\n",
    "        print(f'Partition {i+1} out of {len(cur_parq.row_groups)}\\n')\n",
    "        cur_df.drop(['pos_sent'],axis=1,inplace=True)\n",
    "        \n",
    "        if not args.word:\n",
    "            reduced_df=cur_df.loc[cur_df.comp_class!=0].reset_index(drop=True)\n",
    "            cur_compounds,cur_modifiers,cur_heads=parallelize_dataframe(reduced_df)\n",
    "            compounds_list.append(cur_compounds)\n",
    "            modifiers_list.append(cur_modifiers)\n",
    "            heads_list.append(cur_heads)\n",
    "\n",
    "            #Gathering phrases\n",
    "            #Removing compound classes as they are already gathered\n",
    "            #Use old comp_class to now store phrase_class\n",
    "            print(\"Phrases\")\n",
    "\n",
    "\n",
    "            cur_phrases=parallelize_dataframe(cur_df,phrases=True)\n",
    "            #print(cur_phrases.shape[0])\n",
    "            phrases_list.append(cur_phrases)\n",
    "            \n",
    "        else:\n",
    "            print(\"Words\")\n",
    "            cur_words=parallelize_dataframe(cur_df)\n",
    "            words_list.append(cur_words)\n",
    "\n",
    "    if not args.word:\n",
    "\n",
    "        compounds=pd.concat(compounds_list,ignore_index=True)\n",
    "        comp_before=compounds.shape[0]\n",
    "        compounds=compounds.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        comp_after=compounds.shape[0]\n",
    "\n",
    "        print(f\"Compound before : {comp_before}, after : {comp_after} Change in percentage : {(comp_before-comp_after)/comp_before*100:0.2f}%\")\n",
    "\n",
    "        compounds.reset_index(inplace=True)\n",
    "        compounds.to_pickle(f'{args.output}/compounds/{cur_fname}.pkl')\n",
    "\n",
    "        modifiers=pd.concat(modifiers_list,ignore_index=True)\n",
    "        mod_before=modifiers.shape[0]\n",
    "        modifiers=modifiers.groupby(['modifier','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        mod_after=modifiers.shape[0]\n",
    "\n",
    "        print(f\"Modifier before : {mod_before}, after : {mod_after} Change in percentage : {(mod_before-mod_after)/mod_before*100:0.2f}%\")\n",
    "\n",
    "        modifiers.reset_index(inplace=True)\n",
    "        modifiers.to_pickle(f'{args.output}/modifiers/{cur_fname}.pkl')\n",
    "\n",
    "        heads=pd.concat(heads_list,ignore_index=True)\n",
    "        head_before=heads.shape[0]\n",
    "        heads=heads.groupby(['head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        head_after=heads.shape[0]\n",
    "\n",
    "        print(f\"Head before : {head_before}, after : {head_after} Change in percentage : {(head_before-head_after)/head_before*100:0.2f}%\")\n",
    "\n",
    "        heads.reset_index(inplace=True)\n",
    "        heads.to_pickle(f'{args.output}/heads/{cur_fname}.pkl')\n",
    "\n",
    "        phrases=pd.concat(phrases_list,ignore_index=True)\n",
    "        phr_before=phrases.shape[0]\n",
    "        phrases=phrases.groupby(['modifier','head','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        phr_after=phrases.shape[0]\n",
    "\n",
    "        print(f\"Phrase before : {phr_before}, after : {phr_after} Change in percentage : {(phr_before-phr_after)/phr_before*100:0.2f}%\")\n",
    "\n",
    "        phrases.reset_index(inplace=True)\n",
    "        phrases.to_pickle(f'{args.output}/phrases/{cur_fname}.pkl')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        words=pd.concat(words_list,ignore_index=True)\n",
    "        words_before=words.shape[0]\n",
    "        words=words.groupby(['word','num_comp','context','year','comp_ner_sent'])['count'].sum().to_frame()\n",
    "        words_after=words.shape[0]\n",
    "\n",
    "        print(f\"Words before : {words_before}, after : {words_after} Change in percentage : {(words_before-words_after)/words_before*100:0.2f}%\")\n",
    "\n",
    "        words.reset_index(inplace=True)\n",
    "        words.to_pickle(f'{args.output}/words/{cur_fname}.pkl')\n",
    "        \n",
    "\n",
    "    print(\"Done with file \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parquet file: /data/dharp/compounds/datasets/entire_df_v3/df_115.parq\n"
     ]
    }
   ],
   "source": [
    "f='/data/dharp/compounds/datasets/entire_df_v3/df_115.parq'\n",
    "cur_fname=f.split('.')[0].split('/')[-1]\n",
    "print(f'Current parquet file: {f}')\n",
    "cur_parq=fastparquet.ParquetFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parquet file: /data/dharp/compounds/datasets/entire_df_v3/df_115.parq\n",
      "Number of partitions: 9\n",
      "Partition 1 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 294 secs\n",
      "Partition 2 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 67 secs\n",
      "Partition 3 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 82 secs\n",
      "Partition 4 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 72 secs\n",
      "Partition 5 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 61 secs\n",
      "Partition 6 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 75 secs\n",
      "Partition 7 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 66 secs\n",
      "Partition 8 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 66 secs\n",
      "Partition 9 out of 9\n",
      "\n",
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 73 secs\n",
      "Words before : 442705949, after : 352421743 Change in percentage : 20.39%\n",
      "Done with file \n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_processor(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Phr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mPhr\u001b[49m\u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2.4\u001b[39m, comp \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.9\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Phr' is not defined"
     ]
    }
   ],
   "source": [
    "Phr- 2.4, comp - 1.9 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
