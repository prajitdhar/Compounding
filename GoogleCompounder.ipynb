{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import fastparquet\n",
    "import pickle\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Program to run google compounder for a particular file and setting')\n",
    "\n",
    "parser.add_argument('--data', type=str,\n",
    "                    help='location of the pickle file')\n",
    "\n",
    "parser.add_argument('--word', action='store_true',\n",
    "                    help='Extracting context for words only?')\n",
    "\n",
    "parser.add_argument('--output', type=str,\n",
    "                    help='directory to save dataset in')\n",
    "\n",
    "\n",
    "args = parser.parse_args('--data /datanaco/dharp/compounds/datasets/googleV3/ --output /data/dharp/compounds/datasets --word'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64360"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_list = pickle.load( open( f'{args.output}/word_lists/contexts_top50k_old.pkl', \"rb\" ) )\n",
    "len(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1318647"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constituent_list = pickle.load( open( f'{args.output}/word_lists/constituents.pkl', \"rb\" ) )\n",
    "len(constituent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_parquet_files=[]\n",
    "for filename in glob.glob(f'{args.data}*parq'):\n",
    "    all_parquet_files.append(filename)\n",
    "len(all_parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_id_vars=['modifier','head','count','is_comp','comp_ner_sent']\n",
    "modifier_id_vars=['modifier','count']#,'num_comp','comp_ner_sent']\n",
    "head_id_vars=['head','count']#,'num_comp','comp_ner_sent']\n",
    "word_id_vars=['word','count']#,'num_comp','comp_ner_sent']\n",
    "\n",
    "\n",
    "\n",
    "compound_id_context_vars=['modifier','head','context','is_comp','comp_ner_sent']\n",
    "modifier_id_context_vars=['modifier','context']#,'num_comp','comp_ner_sent']\n",
    "head_id_context_vars=['head','context']#,'num_comp','comp_ner_sent']\n",
    "word_id_context_vars=['word','context']#,'num_comp','comp_ner_sent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "2,7\n",
    "\n",
    "3,8\n",
    "\n",
    "4,9\n",
    "\n",
    "5,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='.+_.+'\n",
    "comp='.+_(?:NOUN|PROPN)\\s.+_(?:NOUN|PROPN)'\n",
    "\n",
    "\n",
    "p2=f'^{comp}\\s{word}\\s{word}\\s{word}$'\n",
    "p3=f'^{word}\\s{comp}\\s{word}\\s{word}$'\n",
    "p4=f'^{word}\\s{word}\\s{comp}\\s{word}$'\n",
    "p5=f'^{word}\\s{word}\\s{word}\\s{comp}$'\n",
    "\n",
    "adj_noun='.+_(?:ADJ|NOUN|PROPN)'\n",
    "\n",
    "an1=f'^{adj_noun}\\s{word}\\s{word}\\s{word}\\s{word}$'\n",
    "an2=f'^{word}\\s{adj_noun}\\s{word}\\s{word}\\s{word}$'\n",
    "an3=f'^{word}\\s{word}\\s{adj_noun}\\s{word}\\s{word}$'\n",
    "an4=f'^{word}\\s{word}\\s{word}\\s{adj_noun}\\s{word}$'\n",
    "an5=f'^{word}\\s{word}\\s{word}\\s{word}\\s{adj_noun}$'\n",
    "\n",
    "word_dict={1:an1,2:an2,3:an3,4:an4,5:an5}\n",
    "\n",
    "### ADJ NOUNs\n",
    "\n",
    "an_comp='.+_ADJ\\s.+_(?:NOUN|PROPN)'\n",
    "adj='.+_ADJ'\n",
    "\n",
    "\n",
    "ap2=f'^{an_comp}\\s{word}\\s{word}\\s{word}$'\n",
    "ap3=f'^{word}\\s{an_comp}\\s{word}\\s{word}$'\n",
    "ap4=f'^{word}\\s{word}\\s{an_comp}\\s{word}$'\n",
    "ap5=f'^{word}\\s{word}\\s{word}\\s{an_comp}$'\n",
    "\n",
    "phrase_dict={2:p2,3:p3,4:p4,5:p5,7:ap2,8:ap3,9:ap4,10:ap5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_side_parser(df,phrases=False): # N N _ _ _ \n",
    "    cur_df=df.copy()\n",
    "    if not phrases:\n",
    "\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "        \n",
    "\n",
    "def mid1_parser(df,phrases=False): # _ N N _ _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "def mid2_parser(df,phrases=False): # _ _ N N _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "def right_side_parser(df,phrases=False): # _ _ _ N N\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True,n=4)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df,phrases=False):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern in [1,6,11,12]: # N N _ N N | A N _ A N | A N _  N N | N N _ A N\n",
    "        compound_left_df,modifier_left_df,head_left_df=left_side_parser(df)\n",
    "        compound_right_df,modifier_right_df,head_right_df=right_side_parser(df)\n",
    "        \n",
    "        final_compound_df=pd.concat([compound_left_df,compound_right_df],ignore_index=True)\n",
    "        final_modifier_df=pd.concat([modifier_left_df,modifier_right_df],ignore_index=True)\n",
    "        final_head_df=pd.concat([head_left_df,head_right_df],ignore_index=True)\n",
    "           \n",
    "    elif pattern in [2,7]: # N N _ _ _ | A N _ _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=left_side_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=left_side_parser(df,phrases=True)\n",
    "\n",
    "    elif pattern in [3,8]: # _ N N _ _ | _ A N _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid1_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid1_parser(df,phrases=True)\n",
    "    \n",
    "    elif pattern in [4,9]: # _ _ N N _ | _ _ A N _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid2_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid2_parser(df,phrases=True)\n",
    "        \n",
    "    elif pattern in [5,10]: # _ _ _ N N | _ _ _ A N\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=right_side_parser(df)   \n",
    "        else:\n",
    "            final_phrases_df=right_side_parser(df,phrases=True)\n",
    "\n",
    "            \n",
    "\n",
    "    if not phrases:\n",
    "        return final_compound_df,final_modifier_df,final_head_df\n",
    "    else:\n",
    "        return final_phrases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_extracter(df,phrases=False):\n",
    "    \n",
    "    comp_df_list=[]\n",
    "    head_df_list=[]\n",
    "    mod_df_list=[]\n",
    "    phrase_df_list=[]\n",
    "    if not phrases:\n",
    "        \n",
    "        for i in range(1,13):\n",
    "            \n",
    "            if df.loc[df.comp_class==i].shape[0]!=0:\n",
    "                cur_comp_df,cur_mod_df,cur_head_df=syntactic_reducer(df.loc[df.comp_class==i])\n",
    "\n",
    "                comp_df_list.append(cur_comp_df)\n",
    "                mod_df_list.append(cur_mod_df)\n",
    "                head_df_list.append(cur_head_df)\n",
    "    \n",
    "        compounds=pd.concat(comp_df_list,ignore_index=True,sort=False)\n",
    "        modifiers=pd.concat(mod_df_list,ignore_index=True,sort=False)\n",
    "        heads=pd.concat(head_df_list,ignore_index=True,sort=False)\n",
    "\n",
    "        \n",
    "        compounds.dropna(inplace=True)\n",
    "        compounds=compounds.loc[compounds.context.isin(context_list)]\n",
    "        compounds=compounds.groupby(compound_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "\n",
    "        modifiers.dropna(inplace=True)\n",
    "        modifiers=modifiers.loc[modifiers.context.isin(context_list)]\n",
    "        modifiers=modifiers.groupby(modifier_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        heads.dropna(inplace=True)\n",
    "        heads=heads.loc[heads.context.isin(context_list)]\n",
    "        heads=heads.groupby(head_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        return compounds,modifiers,heads\n",
    "    \n",
    "    else:\n",
    "        phrase_list=[2,3,4,5,7,8,9,10]\n",
    "        for i in phrase_list:\n",
    "            cur_df=df.loc[df.lemma_pos.str.contains(phrase_dict[i])].copy()\n",
    "            if cur_df.shape[0]!=0:\n",
    "                cur_df.comp_class=i\n",
    "                cur_phrase_df=syntactic_reducer(cur_df,phrases=True)\n",
    "                phrase_df_list.append(cur_phrase_df)\n",
    "                \n",
    "                \n",
    "        if phrase_df_list==[]:\n",
    "            return None\n",
    "        else:\n",
    "            phrases=pd.concat(phrase_df_list,ignore_index=True,sort=False)\n",
    "\n",
    "            phrases.dropna(inplace=True)\n",
    "            phrases=phrases.loc[phrases.context.isin(context_list)]\n",
    "            phrases=phrases.groupby(compound_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "            phrases.reset_index(inplace=True)\n",
    "\n",
    "            return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_reducer(df):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # A|N _ _ _ _\n",
    "        df[['word','w1','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "    elif pattern==2: # _ A|N _ _ _\n",
    "        df[['w1','word','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==3: # _ _ A|N _ _\n",
    "        df[['w1','w2','word','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==4: # _ _ _ A|N _\n",
    "        df[['w1','w2','w3','word','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==5: # _ _ _ _ A|N\n",
    "        df[['w1','w2','w3','w4','word']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w2','w3','w4'],value_name='context')\n",
    "\n",
    "    return word_df\n",
    "\n",
    "\n",
    "def word_extractor(df):\n",
    "    word_df_list=[]\n",
    "\n",
    "    for i in range(1,6):\n",
    "        cur_df=df.loc[df.lemma_pos.str.contains(word_dict[i])].copy()\n",
    "        if cur_df.shape[0]!=0:\n",
    "            cur_df.comp_class=i\n",
    "            cur_word_df=word_reducer(cur_df)\n",
    "            word_df_list.append(cur_word_df)\n",
    "\n",
    "    words=pd.concat(word_df_list,ignore_index=True,sort=False)\n",
    "        \n",
    "    words.dropna(inplace=True)\n",
    "    words=words.loc[words.context.isin(context_list)]\n",
    "    words=words.loc[words.word.isin(constituent_list)]\n",
    "    words=words.groupby(word_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "    words.reset_index(inplace=True)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df,phrases=False):\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_partitions)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if not args.word:\n",
    "        \n",
    "        if not phrases:\n",
    "            #Processing heads, modifiers and compounds for Compound Aware\n",
    "\n",
    "            results=pool.map_async(compound_extracter,df_split)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            results=results.get()\n",
    "\n",
    "            print(\"Done parallelizing\")\n",
    "            compound_list = [ result[0] for result in results]\n",
    "            compounds=pd.concat(compound_list,ignore_index=True)\n",
    "            compounds=compounds.groupby(compound_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "            compounds.reset_index(inplace=True)\n",
    "\n",
    "            modifier_list = [ result[1] for result in results]\n",
    "            modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "            modifiers=modifiers.groupby(modifier_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "            modifiers.reset_index(inplace=True)\n",
    "\n",
    "            head_list = [ result[2] for result in results]\n",
    "            heads=pd.concat(head_list,ignore_index=True)\n",
    "            heads=heads.groupby(head_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "            heads.reset_index(inplace=True)\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return compounds,modifiers,heads\n",
    "        else:\n",
    "            \n",
    "            #Processing phrases for Compound Agnostic\n",
    "            results=pool.starmap_async(compound_extracter,zip(df_split,repeat(phrases)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            phrase_list=results.get()\n",
    "            \n",
    "            print(\"Done parallelizing\")\n",
    "            \n",
    "            phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "            phrases=phrases.groupby(compound_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "            phrases.reset_index(inplace=True)\n",
    "\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return phrases\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Processing words for Compound Agnostic\n",
    "\n",
    "        words_list=[]\n",
    "        results=pool.map_async(word_extractor,df_split)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        words_list=results.get()\n",
    "\n",
    "        print(\"Done parallelizing\")\n",
    "        \n",
    "        words = pd.concat(words_list,ignore_index=True)\n",
    "        words=words.groupby(word_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_processor(f):   \n",
    "    cur_fname=f.split('.')[0].split('/')[-1]\n",
    "    print(f'Current parquet file: {f}')\n",
    "    cur_parq=fastparquet.ParquetFile(f)\n",
    "\n",
    "    print(f'Number of partitions: {len(cur_parq.row_groups)}')\n",
    "    compounds_list=[]\n",
    "    modifiers_list=[]\n",
    "    heads_list=[]\n",
    "    phrases_list=[]\n",
    "    words_list=[]\n",
    "\n",
    "    \n",
    "    for i,cur_df in enumerate(cur_parq.iter_row_groups()):\n",
    "        print(f'Partition {i+1} out of {len(cur_parq.row_groups)}\\n')\n",
    "        cur_df['nwords']=cur_df.lemma_pos.str.count(' ').add(1).copy()\n",
    "        pd.options.mode.chained_assignment = 'warn'\n",
    "        cur_df=cur_df.loc[cur_df.nwords==5]\n",
    "        cur_df.drop(['nwords'],axis=1,inplace=True)\n",
    "        \n",
    "        if not args.word:\n",
    "            reduced_df=cur_df.loc[cur_df.comp_class!=0].reset_index(drop=True)\n",
    "            cur_compounds,cur_modifiers,cur_heads=parallelize_dataframe(reduced_df)\n",
    "            compounds_list.append(cur_compounds)\n",
    "            modifiers_list.append(cur_modifiers)\n",
    "            heads_list.append(cur_heads)\n",
    "\n",
    "            #Gathering phrases\n",
    "            #Removing compound classes as they are already gathered\n",
    "            #Use old comp_class to now store phrase_class\n",
    "            print(\"Phrases\")\n",
    "\n",
    "\n",
    "            cur_phrases=parallelize_dataframe(cur_df,phrases=True)\n",
    "            #print(cur_phrases.shape[0])\n",
    "            phrases_list.append(cur_phrases)\n",
    "            \n",
    "        else:\n",
    "            print(\"Words\")\n",
    "            cur_words=parallelize_dataframe(cur_df)\n",
    "            words_list.append(cur_words)\n",
    "\n",
    "    if not args.word:\n",
    "\n",
    "        compounds=pd.concat(compounds_list,ignore_index=True)\n",
    "        comp_before=compounds.shape[0]\n",
    "        compounds=compounds.groupby(compound_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        comp_after=compounds.shape[0]\n",
    "\n",
    "        print(f\"Compound before : {comp_before}, after : {comp_after} Change in percentage : {(comp_before-comp_after)/comp_before*100:0.2f}%\")\n",
    "\n",
    "        compounds.reset_index(inplace=True)\n",
    "        compounds.to_pickle(f'{args.output}/compounds/{cur_fname}.pkl')\n",
    "\n",
    "        modifiers=pd.concat(modifiers_list,ignore_index=True)\n",
    "        mod_before=modifiers.shape[0]\n",
    "        modifiers=modifiers.groupby(modifier_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        mod_after=modifiers.shape[0]\n",
    "\n",
    "        print(f\"Modifier before : {mod_before}, after : {mod_after} Change in percentage : {(mod_before-mod_after)/mod_before*100:0.2f}%\")\n",
    "\n",
    "        modifiers.reset_index(inplace=True)\n",
    "        modifiers.to_pickle(f'{args.output}/modifiers/{cur_fname}.pkl')\n",
    "\n",
    "        heads=pd.concat(heads_list,ignore_index=True)\n",
    "        head_before=heads.shape[0]\n",
    "        heads=heads.groupby(head_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        head_after=heads.shape[0]\n",
    "\n",
    "        print(f\"Head before : {head_before}, after : {head_after} Change in percentage : {(head_before-head_after)/head_before*100:0.2f}%\")\n",
    "\n",
    "        heads.reset_index(inplace=True)\n",
    "        heads.to_pickle(f'{args.output}/heads/{cur_fname}.pkl')\n",
    "\n",
    "        phrases=pd.concat(phrases_list,ignore_index=True)\n",
    "        phr_before=phrases.shape[0]\n",
    "        phrases=phrases.groupby(compound_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        phr_after=phrases.shape[0]\n",
    "\n",
    "        print(f\"Phrase before : {phr_before}, after : {phr_after} Change in percentage : {(phr_before-phr_after)/phr_before*100:0.2f}%\")\n",
    "\n",
    "        phrases.reset_index(inplace=True)\n",
    "        phrases.to_pickle(f'{args.output}/phrases/{cur_fname}.pkl')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        words=pd.concat(words_list,ignore_index=True)\n",
    "        words_before=words.shape[0]\n",
    "        words=words.groupby(word_id_context_vars,observed=True)['count'].sum().to_frame()\n",
    "        words_after=words.shape[0]\n",
    "\n",
    "        print(f\"Words before : {words_before}, after : {words_after} Change in percentage : {(words_before-words_after)/words_before*100:0.2f}%\")\n",
    "\n",
    "        words.reset_index(inplace=True)\n",
    "        words.to_pickle(f'{args.output}/words/{cur_fname}.pkl')\n",
    "        \n",
    "\n",
    "    print(\"Done with file \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parquet file: /datanaco/dharp/compounds/datasets/googleV3/1800.parq\n"
     ]
    }
   ],
   "source": [
    "f='/datanaco/dharp/compounds/datasets/googleV3/1800.parq'\n",
    "cur_fname=f.split('.')[0].split('/')[-1]\n",
    "print(f'Current parquet file: {f}')\n",
    "cur_parq=fastparquet.ParquetFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first = next(iter(cur_parq.iter_row_groups()))\n",
    "first['nwords']=first.lemma_pos.str.count(' ').add(1).copy()\n",
    "pd.options.mode.chained_assignment = 'warn'\n",
    "first=first.loc[first.nwords==5]\n",
    "first.drop(['nwords','is_comp','comp_ner_sent'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parquet file: /datanaco/dharp/compounds/datasets/googleV3/1800.parq\n",
      "Number of partitions: 5\n",
      "Partition 1 out of 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16106/4281535223.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_df.drop(['nwords'],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 200 secs\n",
      "Partition 2 out of 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16106/4281535223.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_df.drop(['nwords'],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 206 secs\n",
      "Partition 3 out of 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16106/4281535223.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_df.drop(['nwords'],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 203 secs\n",
      "Partition 4 out of 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16106/4281535223.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_df.drop(['nwords'],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 210 secs\n",
      "Partition 5 out of 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16106/4281535223.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cur_df.drop(['nwords'],axis=1,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words\n",
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 208 secs\n",
      "Words before : 50006119, after : 3913011 Change in percentage : 92.17%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert count, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparquet_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[85], line 94\u001b[0m, in \u001b[0;36mparquet_processor\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     90\u001b[0m     words_after\u001b[38;5;241m=\u001b[39mwords\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords before : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords_before\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, after : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords_after\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Change in percentage : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(words_before\u001b[38;5;241m-\u001b[39mwords_after)\u001b[38;5;241m/\u001b[39mwords_before\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m     \u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     words\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/words/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcur_fname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone with file \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:6361\u001b[0m, in \u001b[0;36mDataFrame.reset_index\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6355\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6356\u001b[0m             \u001b[38;5;66;03m# if we have the codes, extract the values with a mask\u001b[39;00m\n\u001b[1;32m   6357\u001b[0m             level_values \u001b[38;5;241m=\u001b[39m algorithms\u001b[38;5;241m.\u001b[39mtake(\n\u001b[1;32m   6358\u001b[0m                 level_values, lab, allow_fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mlev\u001b[38;5;241m.\u001b[39m_na_value\n\u001b[1;32m   6359\u001b[0m             )\n\u001b[0;32m-> 6361\u001b[0m         \u001b[43mnew_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6362\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6364\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlevel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6365\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_duplicates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_duplicates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6368\u001b[0m new_obj\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m new_index\n\u001b[1;32m   6369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n",
      "File \u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:4817\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4812\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_duplicates=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4813\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself.flags.allows_duplicate_labels\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4814\u001b[0m     )\n\u001b[1;32m   4815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_duplicates \u001b[38;5;129;01mand\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m   4816\u001b[0m     \u001b[38;5;66;03m# Should this be a different kind of error??\u001b[39;00m\n\u001b[0;32m-> 4817\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot insert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   4819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc must be int\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert count, already exists"
     ]
    }
   ],
   "source": [
    "parquet_processor(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>context</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!_PROPN</td>\n",
       "      <td>hail_NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>!_PROPN</td>\n",
       "      <td>ship_VERB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#_ADJ</td>\n",
       "      <td>account_NOUN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#_ADJ</td>\n",
       "      <td>already_ADV</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#_ADJ</td>\n",
       "      <td>always_ADV</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503231</th>\n",
       "      <td>■_NOUN</td>\n",
       "      <td>whole_NOUN</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503232</th>\n",
       "      <td>■_NOUN</td>\n",
       "      <td>word_NOUN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503233</th>\n",
       "      <td>■_NOUN</td>\n",
       "      <td>writ_NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503234</th>\n",
       "      <td>■_NOUN</td>\n",
       "      <td>year_NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8503235</th>\n",
       "      <td>■_NOUN</td>\n",
       "      <td>•_NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8503236 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word       context  count\n",
       "0        !_PROPN     hail_NOUN      1\n",
       "1        !_PROPN     ship_VERB      1\n",
       "2          #_ADJ  account_NOUN      2\n",
       "3          #_ADJ   already_ADV      1\n",
       "4          #_ADJ    always_ADV      2\n",
       "...          ...           ...    ...\n",
       "8503231   ■_NOUN    whole_NOUN     18\n",
       "8503232   ■_NOUN     word_NOUN      8\n",
       "8503233   ■_NOUN     writ_NOUN      1\n",
       "8503234   ■_NOUN     year_NOUN      1\n",
       "8503235   ■_NOUN        •_NOUN      1\n",
       "\n",
       "[8503236 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mword_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1_000_000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 33\u001b[0m, in \u001b[0;36mword_extractor\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cur_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     32\u001b[0m         cur_df\u001b[38;5;241m.\u001b[39mcomp_class\u001b[38;5;241m=\u001b[39mi\n\u001b[0;32m---> 33\u001b[0m         cur_word_df\u001b[38;5;241m=\u001b[39m\u001b[43mword_reducer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         word_df_list\u001b[38;5;241m.\u001b[39mappend(cur_word_df)\n\u001b[1;32m     36\u001b[0m words\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat(word_df_list,ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m, in \u001b[0;36mword_reducer\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m     word_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mmelt(df,id_vars\u001b[38;5;241m=\u001b[39mword_id_vars,value_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw3\u001b[39m\u001b[38;5;124m'\u001b[39m],value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pattern\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m: \u001b[38;5;66;03m# _ A|N _ _ _\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw4\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mlemma_pos\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m,expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     word_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mmelt(df,id_vars\u001b[38;5;241m=\u001b[39mword_id_vars,value_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw4\u001b[39m\u001b[38;5;124m'\u001b[39m],value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pattern\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m: \u001b[38;5;66;03m# _ _ A|N _ _\u001b[39;00m\n",
      "File \u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:3968\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3966\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[1;32m   3967\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[0;32m-> 3968\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m   3970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py:4010\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4006\u001b[0m     \u001b[38;5;66;03m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[1;32m   4007\u001b[0m     \u001b[38;5;66;03m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[1;32m   4009\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m-> 4010\u001b[0m         \u001b[43mcheck_key_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4011\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k1, k2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(key, value\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m   4012\u001b[0m             \u001b[38;5;28mself\u001b[39m[k1] \u001b[38;5;241m=\u001b[39m value[k2]\n",
      "File \u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/indexers/utils.py:401\u001b[0m, in \u001b[0;36mcheck_key_length\u001b[0;34m(columns, key, value)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key):\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "\u001b[0;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "word_extractor(first.head(1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting the datasets\n",
      "Starting parallelizing\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/data/dharp/packages/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/data/dharp/packages/miniconda3/lib/python3.9/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"/tmp/ipykernel_16106/3564573068.py\", line 33, in word_extractor\n    cur_word_df=word_reducer(cur_df)\n  File \"/tmp/ipykernel_16106/3564573068.py\", line 4, in word_reducer\n    df[['word','w1','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n  File \"/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3968, in __setitem__\n    self._setitem_array(key, value)\n  File \"/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 4010, in _setitem_array\n    check_key_length(self.columns, key, value)\n  File \"/data/dharp/packages/miniconda3/lib/python3.9/site-packages/pandas/core/indexers/utils.py\", line 401, in check_key_length\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparallelize_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m, in \u001b[0;36mparallelize_dataframe\u001b[0;34m(df, phrases)\u001b[0m\n\u001b[1;32m     64\u001b[0m pool\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     65\u001b[0m pool\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m---> 66\u001b[0m words_list\u001b[38;5;241m=\u001b[39m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone parallelizing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m words \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(words_list,ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/dharp/packages/miniconda3/lib/python3.9/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "parallelize_dataframe(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 82 secs\n"
     ]
    }
   ],
   "source": [
    "cur_comp_df,cur_mod_df,cur_head_df=parallelize_dataframe(first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 204 secs\n"
     ]
    }
   ],
   "source": [
    "cur_phrase_df=parallelize_dataframe(first,phrases=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
