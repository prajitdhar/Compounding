{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import fastparquet\n",
    "import pickle\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Program to run google compounder for a particular file and setting')\n",
    "\n",
    "parser.add_argument('--data', type=str,\n",
    "                    help='location of the pickle file')\n",
    "\n",
    "parser.add_argument('--word', action='store_true',\n",
    "                    help='Extracting context for words only?')\n",
    "\n",
    "parser.add_argument('--output', type=str,\n",
    "                    help='directory to save dataset in')\n",
    "\n",
    "\n",
    "args = parser.parse_args('--data /datanaco/dharp/compounds/datasets/googleV3/ --output /data/dharp/compounds/datasets'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64360"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_list = pickle.load( open( f'{args.output}/contexts/contexts_top50k_new.pkl', \"rb\" ) )\n",
    "len(context_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_parquet_files=[]\n",
    "for filename in glob.glob(f'{args.data}*parq'):\n",
    "    all_parquet_files.append(filename)\n",
    "len(all_parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_id_vars=['modifier','head','year','count','num_comp','comp_ner_sent']\n",
    "modifier_id_vars=['modifier','year','count']#,'num_comp','comp_ner_sent']\n",
    "head_id_vars=['head','year','count']#,'num_comp','comp_ner_sent']\n",
    "word_id_vars=['word','year','count']#,'num_comp','comp_ner_sent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word='.+_.+'\n",
    "comp='.+_(?:NOUN|PROPN)\\s.+_(?:NOUN|PROPN)'\n",
    "\n",
    "\n",
    "p2=f'^{comp}\\s{word}\\s{word}\\s{word}$'\n",
    "p3=f'^{word}\\s{comp}\\s{word}\\s{word}$'\n",
    "p4=f'^{word}\\s{word}\\s{comp}\\s{word}$'\n",
    "p5=f'^{word}\\s{word}\\s{word}\\s{comp}$'\n",
    "\n",
    "phrase_dict={2:p2,3:p3,4:p4,5:p5}\n",
    "\n",
    "noun='.+_(?:NOUN|PROPN)'\n",
    "\n",
    "n1=f'^{noun}\\s{word}\\s{word}\\s{word}\\s{word}$'\n",
    "n2=f'^{word}\\s{noun}\\s{word}\\s{word}\\s{word}$'\n",
    "n3=f'^{word}\\s{word}\\s{noun}\\s{word}\\s{word}$'\n",
    "n4=f'^{word}\\s{word}\\s{word}\\s{noun}\\s{word}$'\n",
    "n5=f'^{word}\\s{word}\\s{word}\\s{word}\\s{noun}$'\n",
    "\n",
    "word_dict={1:n1,2:n2,3:n3,4:n4,5:n5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_side_parser(df,phrases=False): # N N _ _ _ \n",
    "    cur_df=df.copy()\n",
    "    if not phrases:\n",
    "\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "        \n",
    "\n",
    "def mid1_parser(df,phrases=False): # _ N N _ _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "def mid2_parser(df,phrases=False): # _ _ N N _\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df\n",
    "\n",
    "def right_side_parser(df,phrases=False): # _ _ _ N N\n",
    "    cur_df=df.copy()\n",
    "    \n",
    "    if not phrases:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            compound_df=pd.DataFrame()\n",
    "            modifier_df=pd.DataFrame()\n",
    "            head_df=pd.DataFrame()\n",
    "            return compound_df,modifier_df,head_df\n",
    "\n",
    "        compound_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "\n",
    "        modifier_df=pd.melt(cur_df,id_vars=modifier_id_vars,value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "\n",
    "        head_df=pd.melt(cur_df,id_vars=head_id_vars,value_vars=['modifier','w2','w3'],value_name='context')\n",
    "\n",
    "        return compound_df,modifier_df,head_df\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "        except ValueError:\n",
    "            phrase_df=pd.DataFrame()\n",
    "            return phrase_df\n",
    "        \n",
    "        phrase_df=pd.melt(cur_df,id_vars=compound_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "        return phrase_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df,phrases=False):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # N N _ N N\n",
    "        compound_left_df,modifier_left_df,head_left_df=left_side_parser(df)\n",
    "        compound_right_df,modifier_right_df,head_right_df=right_side_parser(df)\n",
    "        \n",
    "        final_compound_df=pd.concat([compound_left_df,compound_right_df],ignore_index=True)\n",
    "        final_modifier_df=pd.concat([modifier_left_df,modifier_right_df],ignore_index=True)\n",
    "        final_head_df=pd.concat([head_left_df,head_right_df],ignore_index=True)\n",
    "           \n",
    "    elif pattern==2: # N N _ _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=left_side_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=left_side_parser(df,phrases=True)\n",
    "\n",
    "    elif pattern==3: # _ N N _ _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid1_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid1_parser(df,phrases=True)\n",
    "    \n",
    "    elif pattern==4: # _ _ N N _\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=mid2_parser(df)\n",
    "        else:\n",
    "            final_phrases_df=mid2_parser(df,phrases=True)\n",
    "        \n",
    "    elif pattern==5: # _ _ _ N N\n",
    "        if not phrases:\n",
    "            final_compound_df,final_modifier_df,final_head_df=right_side_parser(df)   \n",
    "        else:\n",
    "            final_phrases_df=right_side_parser(df,phrases=True)\n",
    "\n",
    "            \n",
    "\n",
    "    if not phrases:\n",
    "        return final_compound_df,final_modifier_df,final_head_df\n",
    "    else:\n",
    "        return final_phrases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_extracter(df,phrases=False):\n",
    "    \n",
    "    comp_df_list=[]\n",
    "    head_df_list=[]\n",
    "    mod_df_list=[]\n",
    "    phrase_df_list=[]\n",
    "    if not phrases:\n",
    "        \n",
    "        for i in range(1,6):\n",
    "            \n",
    "            if df.loc[df.comp_class==i].shape[0]!=0:\n",
    "                cur_comp_df,cur_mod_df,cur_head_df=syntactic_reducer(df.loc[df.comp_class==i])\n",
    "\n",
    "                comp_df_list.append(cur_comp_df)\n",
    "                mod_df_list.append(cur_mod_df)\n",
    "                head_df_list.append(cur_head_df)\n",
    "    \n",
    "        compounds=pd.concat(comp_df_list,ignore_index=True,sort=False)\n",
    "        modifiers=pd.concat(mod_df_list,ignore_index=True,sort=False)\n",
    "        heads=pd.concat(head_df_list,ignore_index=True,sort=False)\n",
    "\n",
    "        \n",
    "        compounds.dropna(inplace=True)\n",
    "        compounds=compounds.groupby(compound_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        return compounds\n",
    "        compounds.reset_index(inplace=True)\n",
    "\n",
    "        modifiers.dropna(inplace=True)\n",
    "        modifiers=modifiers.groupby(modifier_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        heads.dropna(inplace=True)\n",
    "        heads=heads.groupby(head_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        return compounds,modifiers,heads\n",
    "    \n",
    "    else:\n",
    "        for i in range(2,6):\n",
    "            cur_df=df.loc[df.lemma_pos.str.contains(phrase_dict[i])].copy()\n",
    "            if cur_df.shape[0]!=0:\n",
    "                cur_df.comp_class=i\n",
    "                cur_phrase_df=syntactic_reducer(cur_df,phrases=True)\n",
    "                phrase_df_list.append(cur_phrase_df)\n",
    "                \n",
    "                \n",
    "        if phrase_df_list==[]:\n",
    "            return None\n",
    "        else:\n",
    "            phrases=pd.concat(phrase_df_list,ignore_index=True,sort=False)\n",
    "\n",
    "            phrases.dropna(inplace=True)\n",
    "            phrases=phrases.groupby(compound_id_vars,observed=True)['count'].sum().to_frame()\n",
    "            phrases.reset_index(inplace=True)\n",
    "\n",
    "            return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_reducer(df):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # N _ _ _ _\n",
    "        df[['word','w1','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3'],value_name='context')\n",
    "        \n",
    "    elif pattern==2: # _ N _ _ _\n",
    "        df[['w1','word','w2','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==3: # _ _ N _ _\n",
    "        df[['w1','w2','word','w3','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==4: # _ _ _ N _\n",
    "        df[['w1','w2','w3','word','w4']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w1','w2','w3','w4'],value_name='context')\n",
    "\n",
    "    elif pattern==5: # _ _ _ _ N\n",
    "        df[['w1','w2','w3','w4','word']]=df.lemma_pos.str.split(' ',expand=True)\n",
    "        word_df=pd.melt(df,id_vars=word_id_vars,value_vars=['w2','w3','w4'],value_name='context')\n",
    "\n",
    "    return word_df\n",
    "\n",
    "\n",
    "def word_extractor(df):\n",
    "    word_df_list=[]\n",
    "\n",
    "    for i in range(1,6):\n",
    "        cur_df=df.loc[df.lemma_pos.str.contains(word_dict[i])].copy()\n",
    "        if cur_df.shape[0]!=0:\n",
    "            cur_df.comp_class=i\n",
    "            cur_word_df=word_reducer(cur_df)\n",
    "            word_df_list.append(cur_word_df)\n",
    "\n",
    "    words=pd.concat(word_df_list,ignore_index=True,sort=False)\n",
    "        \n",
    "    words.dropna(inplace=True)\n",
    "    words=words.groupby(word_id_vars,observed=True)['count'].sum().to_frame()\n",
    "    words.reset_index(inplace=True)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df,phrases=False):\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_partitions)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if not args.word:\n",
    "        \n",
    "        if not phrases:\n",
    "            #Processing heads, modifiers and compounds for Compound Aware\n",
    "\n",
    "            results=pool.map_async(compound_extracter,df_split)\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            results=results.get()\n",
    "\n",
    "            print(\"Done parallelizing\")\n",
    "            compound_list = [ result[0] for result in results]\n",
    "            compounds=pd.concat(compound_list,ignore_index=True)\n",
    "            compounds=compounds.groupby(compound_id_vars,observed=True)['count'].sum().to_frame()\n",
    "            compounds.reset_index(inplace=True)\n",
    "\n",
    "            modifier_list = [ result[1] for result in results]\n",
    "            modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "            modifiers=modifiers.groupby(modifier_id_vars,observed=True)['count'].sum().to_frame()\n",
    "            modifiers.reset_index(inplace=True)\n",
    "\n",
    "            head_list = [ result[2] for result in results]\n",
    "            heads=pd.concat(head_list,ignore_index=True)\n",
    "            heads=heads.groupby(head_id_vars,observed=True)['count'].sum().to_frame()\n",
    "            heads.reset_index(inplace=True)\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return compounds,modifiers,heads\n",
    "        else:\n",
    "            \n",
    "            #Processing phrases for Compound Agnostic\n",
    "            results=pool.starmap_async(compound_extracter,zip(df_split,repeat(phrases)))\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            phrase_list=results.get()\n",
    "            \n",
    "            print(\"Done parallelizing\")\n",
    "            \n",
    "            phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "            phrases=phrases.groupby(compound_id_vars,observed=True)['count'].sum().to_frame()\n",
    "            phrases.reset_index(inplace=True)\n",
    "            print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "\n",
    "            return phrases\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #Processing words for Compound Agnostic\n",
    "\n",
    "        words_list=[]\n",
    "        results=pool.map_async(word_extractor,df_split)\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        words_list=results.get()\n",
    "\n",
    "        print(\"Done parallelizing\")\n",
    "        \n",
    "        words = pd.concat(words_list,ignore_index=True)\n",
    "        words=words.groupby(word_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_processor(f):   \n",
    "    cur_fname=f.split('.')[0].split('/')[-1]\n",
    "    print(f'Current parquet file: {f}')\n",
    "    cur_parq=fastparquet.ParquetFile(f)\n",
    "\n",
    "    print(f'Number of partitions: {len(cur_parq.row_groups)}')\n",
    "    compounds_list=[]\n",
    "    modifiers_list=[]\n",
    "    heads_list=[]\n",
    "    phrases_list=[]\n",
    "    words_list=[]\n",
    "\n",
    "    \n",
    "    for i,cur_df in enumerate(cur_parq.iter_row_groups()):\n",
    "        print(f'Partition {i+1} out of {len(cur_parq.row_groups)}\\n')\n",
    "        #cur_df.drop(['pos_sent'],axis=1,inplace=True)\n",
    "        \n",
    "        if not args.word:\n",
    "            reduced_df=cur_df.loc[cur_df.comp_class!=0].reset_index(drop=True)\n",
    "            cur_compounds,cur_modifiers,cur_heads=parallelize_dataframe(reduced_df)\n",
    "            compounds_list.append(cur_compounds)\n",
    "            modifiers_list.append(cur_modifiers)\n",
    "            heads_list.append(cur_heads)\n",
    "\n",
    "            #Gathering phrases\n",
    "            #Removing compound classes as they are already gathered\n",
    "            #Use old comp_class to now store phrase_class\n",
    "            print(\"Phrases\")\n",
    "\n",
    "\n",
    "            cur_phrases=parallelize_dataframe(cur_df,phrases=True)\n",
    "            #print(cur_phrases.shape[0])\n",
    "            phrases_list.append(cur_phrases)\n",
    "            \n",
    "        else:\n",
    "            print(\"Words\")\n",
    "            cur_words=parallelize_dataframe(cur_df)\n",
    "            words_list.append(cur_words)\n",
    "\n",
    "    if not args.word:\n",
    "\n",
    "        compounds=pd.concat(compounds_list,ignore_index=True)\n",
    "        comp_before=compounds.shape[0]\n",
    "        compounds=compounds.groupby(compound_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        comp_after=compounds.shape[0]\n",
    "\n",
    "        print(f\"Compound before : {comp_before}, after : {comp_after} Change in percentage : {(comp_before-comp_after)/comp_before*100:0.2f}%\")\n",
    "\n",
    "        compounds.reset_index(inplace=True)\n",
    "        compounds.to_pickle(f'{args.output}/compounds/{cur_fname}.pkl')\n",
    "\n",
    "        modifiers=pd.concat(modifiers_list,ignore_index=True)\n",
    "        mod_before=modifiers.shape[0]\n",
    "        modifiers=modifiers.groupby(modifier_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        mod_after=modifiers.shape[0]\n",
    "\n",
    "        print(f\"Modifier before : {mod_before}, after : {mod_after} Change in percentage : {(mod_before-mod_after)/mod_before*100:0.2f}%\")\n",
    "\n",
    "        modifiers.reset_index(inplace=True)\n",
    "        modifiers.to_pickle(f'{args.output}/modifiers/{cur_fname}.pkl')\n",
    "\n",
    "        heads=pd.concat(heads_list,ignore_index=True)\n",
    "        head_before=heads.shape[0]\n",
    "        heads=heads.groupby(head_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        head_after=heads.shape[0]\n",
    "\n",
    "        print(f\"Head before : {head_before}, after : {head_after} Change in percentage : {(head_before-head_after)/head_before*100:0.2f}%\")\n",
    "\n",
    "        heads.reset_index(inplace=True)\n",
    "        heads.to_pickle(f'{args.output}/heads/{cur_fname}.pkl')\n",
    "\n",
    "        phrases=pd.concat(phrases_list,ignore_index=True)\n",
    "        phr_before=phrases.shape[0]\n",
    "        phrases=phrases.groupby(compound_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        phr_after=phrases.shape[0]\n",
    "\n",
    "        print(f\"Phrase before : {phr_before}, after : {phr_after} Change in percentage : {(phr_before-phr_after)/phr_before*100:0.2f}%\")\n",
    "\n",
    "        phrases.reset_index(inplace=True)\n",
    "        phrases.to_pickle(f'{args.output}/phrases/{cur_fname}.pkl')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        words=pd.concat(words_list,ignore_index=True)\n",
    "        words_before=words.shape[0]\n",
    "        words=words.groupby(word_id_vars,observed=True)['count'].sum().to_frame()\n",
    "        words_after=words.shape[0]\n",
    "\n",
    "        print(f\"Words before : {words_before}, after : {words_after} Change in percentage : {(words_before-words_after)/words_before*100:0.2f}%\")\n",
    "\n",
    "        words.reset_index(inplace=True)\n",
    "        words.to_pickle(f'{args.output}/words/{cur_fname}.pkl')\n",
    "        \n",
    "\n",
    "    print(\"Done with file \\n\")\n",
    "    return compounds,modifiers,heads,phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_processor(args.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current parquet file: /datanaco/dharp/compounds/datasets/googleV3/df_1.parq\n"
     ]
    }
   ],
   "source": [
    "f='/datanaco/dharp/compounds/datasets/googleV3/df_1.parq'\n",
    "cur_fname=f.split('.')[0].split('/')[-1]\n",
    "print(f'Current parquet file: {f}')\n",
    "cur_parq=fastparquet.ParquetFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = next(iter(cur_parq.iter_row_groups()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=first.loc[first['year'].between(2010,2010+10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5=df.loc[df.comp_class==5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c=right_side_parser(df_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",_PUNCT               157527\n",
       "and_CCONJ             157527\n",
       "the_DET               148155\n",
       "their_PRON              3117\n",
       "that_SCONJ              1370\n",
       "to_ADP                  1325\n",
       "then_ADV                 846\n",
       "this_DET                 651\n",
       "that_DET                 339\n",
       "three_NUM                330\n",
       "third_ADJ                160\n",
       "therefore_ADV            149\n",
       "thus_ADV                 147\n",
       "these_DET                139\n",
       "thermal_ADJ              118\n",
       "those_DET                 82\n",
       "there_ADV                 82\n",
       "through_ADP               72\n",
       "thy_PRON                  62\n",
       "though_SCONJ              49\n",
       "thin_ADJ                  31\n",
       "throughout_ADP            27\n",
       "thick_ADJ                 23\n",
       "thou_VERB                 23\n",
       "thou_ADP                  22\n",
       "thereby_ADV               20\n",
       "tiny_ADJ                  18\n",
       "thou_X                    14\n",
       "think_VERB                14\n",
       "they_PRON                 13\n",
       "threaten_VERB             13\n",
       "that_PRON                 12\n",
       "throw_VERB                10\n",
       "therapeutic_ADJ           10\n",
       "this_PRON                  9\n",
       "threaded_ADJ               9\n",
       "thither_PRON               8\n",
       "thirteen_NUM               8\n",
       "thrust_VERB                8\n",
       "tireless_ADJ               8\n",
       "thoracic_ADJ               8\n",
       "therein_ADV                6\n",
       "thaw_VERB                  5\n",
       "therewithal_VERB           5\n",
       "timely_ADJ                 2\n",
       "thirty_NUM                 2\n",
       "thine_ADJ                  2\n",
       "thenceforward_VERB         2\n",
       "thermochemical_ADJ         1\n",
       "tho_ADJ                    1\n",
       "Name: context, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.context.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modifier</th>\n",
       "      <th>head</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "      <th>num_comp</th>\n",
       "      <th>comp_ner_sent</th>\n",
       "      <th>variable</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346563</th>\n",
       "      <td>cambridge_PROPN</td>\n",
       "      <td>university_PROPN</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>w3</td>\n",
       "      <td>then_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346564</th>\n",
       "      <td>cambridge_PROPN</td>\n",
       "      <td>university_PROPN</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>w3</td>\n",
       "      <td>then_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346565</th>\n",
       "      <td>cambridge_PROPN</td>\n",
       "      <td>university_PROPN</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>w3</td>\n",
       "      <td>then_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346566</th>\n",
       "      <td>cambridge_PROPN</td>\n",
       "      <td>university_PROPN</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>w3</td>\n",
       "      <td>then_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346567</th>\n",
       "      <td>cambridge_PROPN</td>\n",
       "      <td>university_PROPN</td>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>ORG</td>\n",
       "      <td>w3</td>\n",
       "      <td>then_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472576</th>\n",
       "      <td>tax_NOUN</td>\n",
       "      <td>revenue_NOUN</td>\n",
       "      <td>2015</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>w3</td>\n",
       "      <td>therefore_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472577</th>\n",
       "      <td>tax_NOUN</td>\n",
       "      <td>revenue_NOUN</td>\n",
       "      <td>2016</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>w3</td>\n",
       "      <td>therefore_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472578</th>\n",
       "      <td>tax_NOUN</td>\n",
       "      <td>revenue_NOUN</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>w3</td>\n",
       "      <td>therefore_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472579</th>\n",
       "      <td>tax_NOUN</td>\n",
       "      <td>revenue_NOUN</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>w3</td>\n",
       "      <td>therefore_ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472580</th>\n",
       "      <td>tax_NOUN</td>\n",
       "      <td>revenue_NOUN</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>w3</td>\n",
       "      <td>therefore_ADV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1721 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               modifier              head  year  count  num_comp  \\\n",
       "346563  cambridge_PROPN  university_PROPN  2010      3      True   \n",
       "346564  cambridge_PROPN  university_PROPN  2011      2      True   \n",
       "346565  cambridge_PROPN  university_PROPN  2012      1      True   \n",
       "346566  cambridge_PROPN  university_PROPN  2013      2      True   \n",
       "346567  cambridge_PROPN  university_PROPN  2014      1      True   \n",
       "...                 ...               ...   ...    ...       ...   \n",
       "472576         tax_NOUN      revenue_NOUN  2015      3      True   \n",
       "472577         tax_NOUN      revenue_NOUN  2016      2      True   \n",
       "472578         tax_NOUN      revenue_NOUN  2017      2      True   \n",
       "472579         tax_NOUN      revenue_NOUN  2018      1      True   \n",
       "472580         tax_NOUN      revenue_NOUN  2019      2      True   \n",
       "\n",
       "       comp_ner_sent variable        context  \n",
       "346563           ORG       w3       then_ADV  \n",
       "346564           ORG       w3       then_ADV  \n",
       "346565           ORG       w3       then_ADV  \n",
       "346566           ORG       w3       then_ADV  \n",
       "346567           ORG       w3       then_ADV  \n",
       "...              ...      ...            ...  \n",
       "472576                     w3  therefore_ADV  \n",
       "472577                     w3  therefore_ADV  \n",
       "472578                     w3  therefore_ADV  \n",
       "472579                     w3  therefore_ADV  \n",
       "472580                     w3  therefore_ADV  \n",
       "\n",
       "[1721 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.loc[a.context.isin(context_list)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
