{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import fastparquet\n",
    "import argparse\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp\n",
    "from os.path import isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Program to run google compounder for a particular file and setting')\n",
    "\n",
    "parser.add_argument('--data', type=str,\n",
    "                    help='location of the pickle file')\n",
    "\n",
    "parser.add_argument('--word', action='store_true',\n",
    "                    help='Extracting context for words only?')\n",
    "\n",
    "parser.add_argument('--output', type=str,\n",
    "                    help='directory to save dataset in')\n",
    "\n",
    "\n",
    "args = parser.parse_args('--data /data/dharp/compounds/datasets/entire_df/ --output /data/dharp/compounds/datasets'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/data/dharp/compounds/datasets/contexts/no_ner_0_50000.txt','r') as f:\n",
    "    contexts=f.read().split(\"\\n\")\n",
    "    contexts=contexts[:-1]\n",
    "len(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_side_parser(df): # N N _ _ _\n",
    "    cur_df=df.copy()\n",
    "\n",
    "    try:\n",
    "        cur_df[['modifier','head','w1','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "    except ValueError:\n",
    "        compound_df=pd.DataFrame()\n",
    "        modifier_df=pd.DataFrame()\n",
    "        head_df=pd.DataFrame()\n",
    "        return compound_df,modifier_df,head_df\n",
    "    \n",
    "    compound_df=pd.melt(cur_df,id_vars=['modifier','head','year','count'],value_vars=['w1','w2','w3'],value_name='context')\n",
    "    compound_df=compound_df.loc[compound_df.context.isin(contexts)]\n",
    "\n",
    "    modifier_df=pd.melt(cur_df,id_vars=['modifier','year','count'],value_vars=['head','w1','w2'],value_name='context')\n",
    "    modifier_df=modifier_df.loc[modifier_df.context.isin(contexts)]\n",
    "    \n",
    "    head_df=pd.melt(cur_df,id_vars=['head','year','count'],value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "    head_df=head_df.loc[head_df.context.isin(contexts)]\n",
    "    \n",
    "    return compound_df,modifier_df,head_df\n",
    "\n",
    "def mid1_parser(df): # _ N N _ _\n",
    "    cur_df=df.copy()\n",
    "    try:\n",
    "        cur_df[['w1','modifier','head','w2','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "    except ValueError:\n",
    "        compound_df=pd.DataFrame()\n",
    "        modifier_df=pd.DataFrame()\n",
    "        head_df=pd.DataFrame()\n",
    "        return compound_df,modifier_df,head_df\n",
    "    \n",
    "    compound_df=pd.melt(cur_df,id_vars=['modifier','head','year','count'],value_vars=['w1','w2','w3'],value_name='context')\n",
    "    compound_df=compound_df.loc[compound_df.context.isin(contexts)]\n",
    "\n",
    "    modifier_df=pd.melt(cur_df,id_vars=['modifier','year','count'],value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "    modifier_df=modifier_df.loc[modifier_df.context.isin(contexts)]\n",
    "    \n",
    "    head_df=pd.melt(cur_df,id_vars=['head','year','count'],value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "    head_df=head_df.loc[head_df.context.isin(contexts)]\n",
    "    \n",
    "    return compound_df,modifier_df,head_df\n",
    "\n",
    "def mid2_parser(df): # _ _ N N _\n",
    "    cur_df=df.copy()\n",
    "    try:\n",
    "        cur_df[['w1','w2','modifier','head','w3']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "    except ValueError:\n",
    "        compound_df=pd.DataFrame()\n",
    "        modifier_df=pd.DataFrame()\n",
    "        head_df=pd.DataFrame()\n",
    "        return compound_df,modifier_df,head_df\n",
    "       \n",
    "    compound_df=pd.melt(cur_df,id_vars=['modifier','head','year','count'],value_vars=['w1','w2','w3'],value_name='context')\n",
    "    compound_df=compound_df.loc[compound_df.context.isin(contexts)]\n",
    "\n",
    "    modifier_df=pd.melt(cur_df,id_vars=['modifier','year','count'],value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "    modifier_df=modifier_df.loc[modifier_df.context.isin(contexts)]\n",
    "    \n",
    "    head_df=pd.melt(cur_df,id_vars=['head','year','count'],value_vars=['modifier','w1','w2','w3'],value_name='context')\n",
    "    head_df=head_df.loc[head_df.context.isin(contexts)]\n",
    "    \n",
    "    return compound_df,modifier_df,head_df\n",
    "\n",
    "def right_side_parser(df): # _ _ _ N N\n",
    "    cur_df=df.copy()\n",
    "    try:\n",
    "        cur_df[['w1','w2','w3','modifier','head']]=cur_df.lemma_pos.str.split(' ',expand=True)\n",
    "    except ValueError:\n",
    "        compound_df=pd.DataFrame()\n",
    "        modifier_df=pd.DataFrame()\n",
    "        head_df=pd.DataFrame()\n",
    "        return compound_df,modifier_df,head_df\n",
    "    \n",
    "    compound_df=pd.melt(cur_df,id_vars=['modifier','head','year','count'],value_vars=['w1','w2','w3'],value_name='context')\n",
    "    compound_df=compound_df.loc[compound_df.context.isin(contexts)]\n",
    "    \n",
    "    modifier_df=pd.melt(cur_df,id_vars=['modifier','year','count'],value_vars=['head','w1','w2','w3'],value_name='context')\n",
    "    modifier_df=modifier_df.loc[modifier_df.context.isin(contexts)]\n",
    "    \n",
    "    head_df=pd.melt(cur_df,id_vars=['head','year','count'],value_vars=['modifier','w2','w3'],value_name='context')\n",
    "    head_df=head_df.loc[head_df.context.isin(contexts)]\n",
    "    \n",
    "    return compound_df,modifier_df,head_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntactic_reducer(df):\n",
    "    pattern=df.iloc[0].comp_class\n",
    "    if pattern==1: # N N _ _ N N\n",
    "        compound_left_df,modifier_left_df,head_left_df=left_side_parser(df)\n",
    "        compound_right_df,modifier_right_df,head_right_df=right_side_parser(df)\n",
    "        \n",
    "        final_compound_df=pd.concat([compound_left_df,compound_right_df],ignore_index=True)\n",
    "        final_modifier_df=pd.concat([modifier_left_df,modifier_right_df],ignore_index=True)\n",
    "        final_head_df=pd.concat([head_left_df,head_right_df],ignore_index=True)\n",
    "           \n",
    "    elif pattern==2: # N N _ _ _\n",
    "        final_compound_df,final_modifier_df,final_head_df=left_side_parser(df)\n",
    "\n",
    "    elif pattern==3: # _ N N _ _\n",
    "        final_compound_df,final_modifier_df,final_head_df=mid1_parser(df)\n",
    "    \n",
    "    elif pattern==4: # _ _ N N _\n",
    "        final_compound_df,final_modifier_df,final_head_df=mid2_parser(df)\n",
    "        \n",
    "    elif pattern==5: # _ _ _ N N\n",
    "        final_compound_df,final_modifier_df,final_head_df=right_side_parser(df)\n",
    "\n",
    "    return final_compound_df,final_modifier_df,final_head_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_extracter(df):\n",
    "    if df.loc[df.comp_class==1].shape[0]!=0:\n",
    "        sides_comp_df,sides_mod_df,sides_head_df=syntactic_reducer(df.loc[df.comp_class==1])\n",
    "    else:\n",
    "        sides_comp_df=pd.DataFrame()\n",
    "        sides_mod_df=pd.DataFrame()\n",
    "        sides_head_df=pd.DataFrame()\n",
    "    \n",
    "    if df.loc[df.comp_class==2].shape[0]!=0:\n",
    "        left_comp_df,left_mod_df,left_head_df=syntactic_reducer(df.loc[df.comp_class==2])\n",
    "    else:\n",
    "        left_comp_df=pd.DataFrame()\n",
    "        left_mod_df=pd.DataFrame()\n",
    "        left_head_df=pd.DataFrame()       \n",
    "        \n",
    "    if df.loc[df.comp_class==3].shape[0]!=0:\n",
    "        mid1_comp_df,mid1_mod_df,mid1_head_df=syntactic_reducer(df.loc[df.comp_class==3])\n",
    "    else:\n",
    "        mid1_comp_df=pd.DataFrame()\n",
    "        mid1_mod_df=pd.DataFrame()\n",
    "        mid1_head_df=pd.DataFrame()\n",
    "        \n",
    "    if df.loc[df.comp_class==4].shape[0]!=0:\n",
    "        mid2_comp_df,mid2_mod_df,mid2_head_df=syntactic_reducer(df.loc[df.comp_class==4])\n",
    "    else:\n",
    "        mid2_comp_df=pd.DataFrame()\n",
    "        mid2_mod_df=pd.DataFrame()\n",
    "        mid2_head_df=pd.DataFrame()\n",
    "\n",
    "    if df.loc[df.comp_class==5].shape[0]!=0:\n",
    "        right_comp_df,right_mod_df,right_head_df=syntactic_reducer(df.loc[df.comp_class==5])\n",
    "        \n",
    "    else:\n",
    "        right_comp_df=pd.DataFrame()\n",
    "        right_mod_df=pd.DataFrame()\n",
    "        right_head_df=pd.DataFrame()\n",
    "\n",
    "    compounds=pd.concat([sides_comp_df,left_comp_df,mid1_comp_df,mid2_comp_df,right_comp_df],ignore_index=True,sort=False)\n",
    "    modifiers=pd.concat([sides_mod_df,left_mod_df,mid1_mod_df,mid2_mod_df,right_mod_df],ignore_index=True,sort=False)\n",
    "    heads=pd.concat([sides_head_df,left_head_df,mid1_head_df,mid2_head_df,right_head_df],ignore_index=True,sort=False)\n",
    "    \n",
    "    if len(compounds)==0:\n",
    "        return compounds,modifiers,heads\n",
    "    \n",
    "    compounds.dropna(inplace=True)\n",
    "    compounds=compounds.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "    compounds.reset_index(inplace=True)\n",
    "    \n",
    "    modifiers.dropna(inplace=True)\n",
    "    modifiers=modifiers.groupby(['modifier','context','year'])['count'].sum().to_frame()\n",
    "    modifiers.reset_index(inplace=True)\n",
    "    \n",
    "    heads.dropna(inplace=True)\n",
    "    heads=heads.groupby(['head','context','year'])['count'].sum().to_frame()\n",
    "    heads.reset_index(inplace=True)\n",
    "    \n",
    "    return compounds,modifiers,heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 135838547 entries, 0 to 135838546\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   lemma_pos   object\n",
      " 1   pos_sent    object\n",
      " 2   year        object\n",
      " 3   comp_class  int64 \n",
      " 4   count       object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 5.1+ GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    110089191\n",
       "5     12755236\n",
       "3      7314162\n",
       "4      5679958\n",
       "Name: comp_class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_df=pd.read_pickle(\"/data/dharp/compounds/datasets/googleV3/833.pkl\")\n",
    "trial_df.info()\n",
    "trial_df.comp_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting the datasets\n",
      "Starting parallelizing\n",
      "Done parallelizing\n",
      "Total time taken 16 secs\n"
     ]
    }
   ],
   "source": [
    "a,b,c=parallelize_dataframe(trial_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     135838547\n",
       "unique        12062\n",
       "top               1\n",
       "freq       49688381\n",
       "Name: count, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_df['count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 342196 entries, 0 to 342195\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   modifier  342196 non-null  object\n",
      " 1   context   342196 non-null  object\n",
      " 2   year      342196 non-null  object\n",
      " 3   count     342196 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 10.4+ MB\n"
     ]
    }
   ],
   "source": [
    "b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df):\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    print(\"Done splitting the datasets\")\n",
    "    pool = Pool(num_partitions)\n",
    "\n",
    "    cur_time=time.time()\n",
    "    print(\"Starting parallelizing\")\n",
    "    if not args.word:\n",
    "\n",
    "        results=pool.map_async(compound_extracter,df_split)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        results=results.get()\n",
    "\n",
    "        \n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        compound_list = [ result[0] for result in results]\n",
    "        compounds=pd.concat(compound_list,ignore_index=True)\n",
    "        compounds=compounds.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "        compounds.reset_index(inplace=True)\n",
    "        \n",
    "        if not isfile(f'{args.output}/compounds.csv'):\n",
    "            compounds.to_csv(f'{args.output}/compounds.csv',sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            compounds.to_csv(f'{args.output}/compounds.csv', mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "        \n",
    "        modifier_list = [ result[1] for result in results]\n",
    "        modifiers=pd.concat(modifier_list,ignore_index=True)\n",
    "        modifiers=modifiers.groupby(['modifier','context','year'])['count'].sum().to_frame()\n",
    "        modifiers.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(f'{args.output}/modifiers.csv'):\n",
    "            modifiers.to_csv(f'{args.output}/modifiers.csv',sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            modifiers.to_csv(f'{args.output}/modifiers.csv', mode='a',sep=\"\\t\",header=False,index=False)\n",
    "        \n",
    "        head_list = [ result[2] for result in results]\n",
    "        heads=pd.concat(head_list,ignore_index=True)\n",
    "        heads=heads.groupby(['head','context','year'])['count'].sum().to_frame()\n",
    "        heads.reset_index(inplace=True)\n",
    "\n",
    "        if not isfile(f'{args.output}/heads.csv'):\n",
    "            heads.to_csv(f'{args.output}/heads.csv',sep=\"\\t\",index=False)\n",
    "        else:\n",
    "            heads.to_csv(f'{args.output}/heads.csv', mode='a',sep=\"\\t\",header=False,index=False)\n",
    "            \n",
    "#        phrase_list = [ result[3] for result in results]\n",
    "#        phrases=pd.concat(phrase_list,ignore_index=True)\n",
    "#        phrases=phrases.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "#        phrases.reset_index(inplace=True)\n",
    "        \n",
    "#        if not isfile(f'{args.output}/phrases.csv'):\n",
    "#            phrases.to_csv(f'{args.output}/phrases.csv',sep=\"\\t\",index=False)\n",
    "#        else:\n",
    "#            phrases.to_csv(f'{args.output}/phrases.csv', mode='a',sep=\"\\t\",header=False,index=False)\n",
    "\n",
    "    else:\n",
    "        words_list=[]\n",
    "        results=pool.map_async(cdsm_word_reducer,df_split)\n",
    "  \n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(\"Done parallelizing\")\n",
    "        print(\"Total time taken\",round(time.time()-cur_time),\"secs\")\n",
    "        words_list=results.get()\n",
    "        words = pd.concat(words_list,ignore_index=True,sort=False)\n",
    "        words=words.groupby(['word','context','year'])['count'].sum().to_frame()\n",
    "        words.reset_index(inplace=True)\n",
    "        print(words.shape)\n",
    "                \n",
    "        if not isfile(f'{args.output}/words.csv'):\n",
    "            words.to_csv(f'{args.output}/words.csv',sep=\"\\t\",index=False,header=True)\n",
    "        else:\n",
    "            words.to_csv(f'{args.output}/words.csv', mode='a',sep=\"\\t\", header=False,index=False)\n",
    "        \n",
    "    print(\"Done concatenations \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_processor(f):   \n",
    "    cur_fname=f.split('.')[0].split('/')[-1]\n",
    "    print(f'Current parquet file: {f}')\n",
    "    cur_parq=fastparquet.ParquetFile(f)\n",
    "    print(f'Number of partitions: {len(cur_parq.row_groups)}')\n",
    "    compounds_list=[]\n",
    "    modifiers_list=[]\n",
    "    heads_list=[]\n",
    "\n",
    "    for i,cur_df in enumerate(cur_parq.iter_row_groups()):\n",
    "        print(f'Partition {i+1} out of {len(cur_parq.row_groups)}')\n",
    "        cur_df.year=cur_df.year.astype(\"int32\")\n",
    "        cur_df=cur_df.loc[cur_df.comp_class!=0].reset_index(drop=True)\n",
    "        cur_compounds,cur_modifiers,cur_heads=parallelize_dataframe(cur_df,num_cores)\n",
    "        compounds_list.append(cur_compounds)\n",
    "        modifiers_list.append(cur_modifiers)\n",
    "        heads_list.append(cur_heads)\n",
    "\n",
    "        \n",
    "    compounds=pd.concat(compounds_list,ignore_index=True)\n",
    "    compounds=compounds.groupby(['modifier','head','context','year'])['count'].sum().to_frame()\n",
    "    compounds.reset_index(inplace=True)\n",
    "    \n",
    "    compounds.to_parquet(\n",
    "    path=f'{args.output}/compounds/{cur_fname}.parq', \n",
    "    engine='fastparquet',\n",
    "    compression='snappy')        \n",
    "        \n",
    "   \n",
    "    modifiers=pd.concat(modifiers_list,ignore_index=True)\n",
    "    modifiers=modifiers.groupby(['modifier','context','year'])['count'].sum().to_frame()\n",
    "    modifiers.reset_index(inplace=True)\n",
    "    \n",
    "    modifiers.to_parquet(\n",
    "    path=f'{args.output}/modifiers/{cur_fname}.parq', \n",
    "    engine='fastparquet',\n",
    "    compression='snappy')\n",
    "\n",
    "    \n",
    "    heads=pd.concat(heads_list,ignore_index=True)\n",
    "    heads=heads.groupby(['head','context','year'])['count'].sum().to_frame()\n",
    "    heads.reset_index(inplace=True)\n",
    "    \n",
    "    heads.to_parquet(\n",
    "    path=f'{args.output}/heads/{cur_fname}.parq', \n",
    "    engine='fastparquet',\n",
    "    compression='snappy')\n",
    "    \n",
    "    print(\"Done with file \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
