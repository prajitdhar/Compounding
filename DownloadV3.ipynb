{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77f0fee-bc2e-4523-9070-0ee771d0de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gentle-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import time\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import spacy\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "detokenizer = Detok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b36748b5-c74d-46de-b826-808227df3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save_path='/data/dharp/compounds/datasets/'\n",
    "keep_string=r\"(.+_(NOUN|ADV|VERB|ADJ|X|PRT|CONJ|PRON|DET|ADP|NUM|\\.)|_END_|_START_)\\s*\"\n",
    "try_keep_string=r\"(.+_(NOUN|ADV|VERB|ADJ|X|PRT|CONJ|PRON|DET|ADP|NUM|\\.)|_NOUN_|_ADV_|_VERB_|_ADJ_|_X_|_PRT_|_CONJ_|_PRON_|_DET_|_ADP_|_NUM_|_\\._)\"\n",
    "\n",
    "word='.*'\n",
    "\n",
    "nn='(?!(?:NOUN|PROPN)).*'\n",
    "nn_comp='(?:NOUN|PROPN)\\s(?:NOUN|PROPN)'\n",
    "an_comp='ADJ\\s(?:NOUN|PROPN)'\n",
    "\n",
    "ner_cats=['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
    "n1=f'^{nn_comp}\\s{nn}\\s{nn_comp}$'\n",
    "n2=f'^{nn_comp}\\s{nn}\\s{word}\\s{word}$'\n",
    "n3=f'^{nn}\\s{nn_comp}\\s{nn}\\s{word}$'\n",
    "n4=f'^{word}\\s{nn}\\s{nn_comp}\\s{nn}$'\n",
    "n5=f'^{word}\\s{word}\\s{nn}\\s{nn_comp}$'\n",
    "\n",
    "a1=f'^{an_comp}\\s{nn}\\s{an_comp}$'\n",
    "a2=f'^{an_comp}\\s{nn}\\s{word}\\s{word}$'\n",
    "a3=f'^{nn}\\s{an_comp}\\s{nn}\\s{word}$'\n",
    "a4=f'^{word}\\s{nn}\\s{an_comp}\\s{nn}$'\n",
    "a5=f'^{word}\\s{word}\\s{nn}\\s{an_comp}$'\n",
    "\n",
    "\n",
    "c1=f'^{nn_comp}\\s{nn}\\s{an_comp}$'\n",
    "c2=f'^{an_comp}\\s{nn}\\s{nn_comp}$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c949e60-8094-4d40-baf8-0a4626abd84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fmodel = fasttext.load_model('/data/dharp/packages/lid.176.bin')\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8448d92a-6861-483b-82d5-6ffa35fc41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_lemma_reducer(sent):\n",
    "    ner_sent=[]\n",
    "    lemma=[]\n",
    "    pos=[]\n",
    "    dep=[]\n",
    "    comp_ner_type=[]\n",
    "    parsed_sent=nlp(sent)\n",
    "    for token in parsed_sent:\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        dep.append(token.dep_)\n",
    "        if token.dep_==\"compound\":\n",
    "            if token.ent_type_!=\"\":\n",
    "                comp_ner_type.append(token.ent_type_)\n",
    "\n",
    "    comp_ner_sent=' '.join(comp_ner_type)\n",
    "    if len(parsed_sent)<5:\n",
    "        new_lemma_list=[\"eos\"]*(5-len(parsed_sent))\n",
    "        new_pos_list=[\"X\"]*(5-len(parsed_sent))\n",
    "        lemma.extend(new_lemma_list)\n",
    "        pos.extend(new_pos_list)\n",
    "        \n",
    "    comp_ner_sent=' '.join(comp_ner_type)\n",
    "    lemma_sent=' '.join(lemma)\n",
    "    pos_sent=' '.join(pos)\n",
    "    \n",
    "    dep_sent=' '.join(dep)\n",
    "        \n",
    "    num_count=len(re.findall(\"compound\\s(?!compound)\", dep_sent))\n",
    "   \n",
    "    return lemma_sent,pos_sent,num_count,comp_ner_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d5c18-0cf0-4108-9422-af3c65fb4555",
   "metadata": {},
   "source": [
    "    ner_sent' '.join(comp_ner_type)\n",
    "    lemma_sent_lst=[]\n",
    "    pos_sent_lst=[]\n",
    "    dep_sent_lst=[]\n",
    "    num_count_lst=[]\n",
    "    \n",
    "    temp_lemma=[]\n",
    "    temp_pos=[]\n",
    "    temp_dep=[]\n",
    "    temp_num_count=[]\n",
    "    \n",
    "    for i in range(len(parsed_sent) - 5 + 1):\n",
    "        temp_lemma=lemma[i: i + 5]\n",
    "        temp_pos=pos[i:i+5]\n",
    "        temp_dep=dep[i:i+5]\n",
    "        \n",
    "        lemma_sent_lst.append(' '.join(temp_lemma))\n",
    "        pos_sent_lst.append(' '.join(temp_pos))\n",
    "        \n",
    "        dep_sent=' '.join(temp_dep)\n",
    "        dep_sent_lst.append(dep_sent)\n",
    "        \n",
    "        num_count_lst.append(len(re.findall(\"compound\\s(?!compound)\", dep_sent)))\n",
    "\n",
    "    return lemma_sent,pos_sent,num_count,comp_ner_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e29b5e-3d66-445b-908c-5bd032967ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delist_lang(lst):\n",
    "    lang_lst=[]\n",
    "    for i,lang in enumerate(lst):\n",
    "        if not lang:\n",
    "            lang_lst.append(None)\n",
    "        else:\n",
    "            lang_lst.append(lang[0])\n",
    "    return lang_lst\n",
    "\n",
    "\n",
    "def significance(lst):\n",
    "    significance_list=[]\n",
    "    for l in lst:\n",
    "        if len(l)>1:\n",
    "            significance_list.append(abs(l[0]-l[1])/np.mean(l[0]+l[1])>0.1)\n",
    "            #print(f'{conf[0]} {conf[1]} {abs(conf[0]-conf[1])/np.mean(conf[0]+conf[1])>0.1}')\n",
    "        else:\n",
    "            significance_list.append(True)\n",
    "    return significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115e0f1f-923b-4006-a8db-9845c5179691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_binner(year,val=10):\n",
    "    return year - year%val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "126db617-df83-47e9-a228-405ee25700b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_tagger(parsed_sent):\n",
    "    labels,confs=fmodel.predict(parsed_sent,k=-1,threshold=0.1)\n",
    "    lang_list=delist_lang(labels)    \n",
    "    significance_list=significance(confs)\n",
    "    assert len(lang_list)==len(significance_list)\n",
    "    return lang_list,significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d53b63-07d5-43dd-8536-e06ce4fc3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_processor(df):\n",
    "    \n",
    "    df['sent']=np.vectorize(detokenizer.detokenize)(df.old_index.str.split(\" \").values)\n",
    "    df['sent']=df.sent.str.replace('\\s*,\\s*',', ',regex=False).copy()\n",
    "    df['sent']=df.sent.str.replace('\\s*\\.\\s*','. ',regex=False).copy()\n",
    "    df['sent']=df.sent.str.replace('\\s*\\?\\s*','? ',regex=False).copy()\n",
    "    df['sent']=df.sent.str.replace('__',' ',regex=False).copy()\n",
    "\n",
    "    df['sent']=df.sent.str.replace('_START_ ','',regex=False).copy()\n",
    "    df['sent']=df['sent'].str.replace(' _END_','',regex=False).copy()\n",
    "     \n",
    "    #df['sent']=df['sent'].str.replace(r\"(.+)'\\s(.+)\",r\"\\1'\\2\",regex=True).copy()\n",
    "    #df['sent']=df['sent'].str.replace(r\"(.+)\\s'(.+)\",r\"\\1'\\2\",regex=True).copy()\n",
    "\n",
    "    lang_list,significance_list=lang_tagger(df.sent.values.tolist())\n",
    "    df['lang']=lang_list\n",
    "    df['lang_conf']=significance_list\n",
    "    df.lang=df.lang.str.split('_',n=4).str[-1]\n",
    "    \n",
    "    df=df.loc[(df.lang=='en') &(df.lang_conf==True)]\n",
    "\n",
    "    lemma_sent,pos_sent,comp_count,comp_ner_sent=np.vectorize(ner_lemma_reducer)(df.sent.values)\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    df['lemma_sent']=lemma_sent\n",
    "    df['pos_sent']=pos_sent\n",
    "    df['comp_count']=comp_count\n",
    "    df['comp_ner_sent']=comp_ner_sent\n",
    "    \n",
    "    df['is_comp']=False\n",
    "    df.loc[df.comp_count!=0,'is_comp']=True\n",
    "    #results_df=results_df.loc[~results_df.ner_token_sent.str.contains(\"PERSON PERSON\")]\n",
    "\n",
    "    #index_df=pd.concat([df,results_df],axis=1,ignore_index=True)\n",
    "\n",
    "    #return results_df,df\n",
    "\n",
    "    #index_df=index_df.loc[(index_df.lang=='en') &(index_df.lang_conf==True)]\n",
    "\n",
    "    df['nwords']=df.pos_sent.str.count(' ').add(1).copy()\n",
    "    \n",
    "    pd.options.mode.chained_assignment = 'warn'\n",
    "    df=df.loc[df.nwords==5]\n",
    "    \n",
    "    df.lemma_sent=df.lemma_sent.str.lower()\n",
    "\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('PROPN','NOUN',regex=False)\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('AUX','VERB',regex=False)\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('CCONJ','CONJ',regex=False)\n",
    "    #index_df.g_pos=index_df.g_pos.str.replace('.','PUNCT',regex=False)\n",
    "    #index_df.g_pos=index_df.g_pos.str.replace('PRT','ADP',regex=False)\n",
    "    if df.shape[0]==0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['lemma_pos']=str_joiner(df)\n",
    "    df['nX']=df.pos_sent.str.count('X')-df.pos_sent.str.count('AUX')\n",
    "    df=df.loc[~(df.nX==5)]\n",
    "       \n",
    "    df['comp_class']=0\n",
    "\n",
    "    df.loc[df.pos_sent.str.contains(n1),'comp_class']=1\n",
    "    df.loc[~(df.pos_sent.str.contains(n1))& df.pos_sent.str.contains(n2),'comp_class']=2\n",
    "    df.loc[df.pos_sent.str.contains(n3),'comp_class']=3\n",
    "    df.loc[df.pos_sent.str.contains(n4),'comp_class']=4\n",
    "    df.loc[~(df.pos_sent.str.contains(n1))& df.pos_sent.str.contains(n5),'comp_class']=5\n",
    "    \n",
    "    df.loc[df.pos_sent.str.contains(a1),'comp_class']=6\n",
    "    df.loc[~(df.pos_sent.str.contains(a1))& df.pos_sent.str.contains(a2),'comp_class']=7\n",
    "    df.loc[df.pos_sent.str.contains(a3),'comp_class']=8\n",
    "    df.loc[df.pos_sent.str.contains(a4),'comp_class']=9\n",
    "    df.loc[~(df.pos_sent.str.contains(a1))& df.pos_sent.str.contains(a5),'comp_class']=10\n",
    "\n",
    "    df.loc[df.pos_sent.str.contains(c1),'comp_class']=11\n",
    "    df.loc[df.pos_sent.str.contains(c2),'comp_class']=12\n",
    "\n",
    "    df.drop(['sent','pos_sent','lang','lang_conf','nwords','nX','lemma_sent'],axis=1,inplace=True)\n",
    "\n",
    "    index_year_df=year_count_split(df)\n",
    "    index_df=df.merge(index_year_df, on='old_index',how='right')\n",
    "\n",
    "    index_df['count']=index_df['count'].astype(\"int64\")\n",
    "    index_df['year']=index_df['year'].astype(\"int64\")\n",
    "\n",
    "    index_df['decade']=year_binner(index_df['year'].values,10)\n",
    "    index_df['decade']=index_df['decade'].astype(\"int64\")\n",
    "    index_df=index_df.loc[index_df.decade>=1800]\n",
    "    \n",
    "    index_df=index_df.groupby(['lemma_pos','decade','comp_class','is_comp','comp_ner_sent'])['count'].sum().to_frame().reset_index()\n",
    "    return index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6b7860-c1d3-4fb7-8c4b-63d2dc9cb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_count_split(df):\n",
    "    trial_df=pd.concat([df.old_index, df.year_counts.str.split(\"\\t\", expand=True)], axis=1)\n",
    "    trial_df=pd.melt(trial_df, id_vars=[\"old_index\"], value_vars=list(range(len(trial_df.columns)-1))).dropna().drop(\"variable\", axis = 1)\n",
    "    trial_df[['year','count']] = trial_df.value.str.split(\",\", n=3, expand=True)[[0,1]]\n",
    "    return trial_df.drop(['value'],axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb5c354a-6cb1-4248-a1be-18fe2cf1b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_joiner(df):\n",
    "    #print(df)\n",
    "    new_df=pd.DataFrame()\n",
    "    try:\n",
    "        new_df[['l1','l2','l3','l4','l5']]=df.lemma_sent.str.split(\" \",expand=True,n=4)\n",
    "        new_df[['p1','p2','p3','p4','p5']]=df.pos_sent.str.split(\" \",expand=True,n=4)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    new_df['lemma_pos']=new_df.l1+\"_\"+new_df.p1+\" \"+\\\n",
    "                        new_df.l2+\"_\"+new_df.p2+\" \"+\\\n",
    "                        new_df.l3+\"_\"+new_df.p3+\" \"+\\\n",
    "                        new_df.l4+\"_\"+new_df.p4+\" \"+\\\n",
    "                        new_df.l5+\"_\"+new_df.p5\n",
    "    return new_df['lemma_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978f52bd-80d8-489b-859f-26041ee2f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='5-18104-of-19423'\n",
    "\n",
    "fname='5-09107-of-19423'\n",
    "\n",
    "fname='5-00000-of-19423'\n",
    "\n",
    "lnk=f'http://storage.googleapis.com/books/ngrams/books/20200217/eng/{fname}.gz'\n",
    "index_df   = pd.read_csv(lnk, compression='gzip', header=None, sep=u\"\\u0001\", quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5619027e-7b96-493b-b87d-da265c5638de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    index_df[['old_index','year_counts']]=index_df[0].str.split('\\t',n=1,expand=True)\n",
    "    #index_df=index_df.loc[index_df.old_index.str.match(\"^\"+keep_string*5+\"$\",na=False)]\n",
    "    index_df=index_df.loc[~index_df.old_index.str.contains(try_keep_string,na=False,regex=True)]\n",
    "    index_df.drop(0,axis=1,inplace=True)\n",
    "    index_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "045b1fa4-aa88-49de-9dbc-1ba90fd6cec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parallelization\n",
      "Total time taken 56 secs\n"
     ]
    }
   ],
   "source": [
    "if index_df.shape[0]<10_000:\n",
    "    \n",
    "    cur_time=time.time()\n",
    "    new_index_df=index_processor(index_df)\n",
    "    print(f'Total time taken {round(time.time()-cur_time)} secs')\n",
    "    \n",
    "else:\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    cur_time=time.time()\n",
    "    df_split = np.array_split(index_df, num_partitions)\n",
    "    pool = Pool(num_partitions)\n",
    "    print('Started parallelization')\n",
    "    results=pool.map_async(index_processor,df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "        \n",
    "    curr_df_list=results.get()\n",
    "    new_index_df=pd.concat(curr_df_list,ignore_index=True)\n",
    "    print(f'Total time taken {round(time.time()-cur_time)} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c0fbf5-dae7-4c7c-8f9c-819e352b04ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1830,\n",
       " 1850,\n",
       " 1860,\n",
       " 1870,\n",
       " 1880,\n",
       " 1890,\n",
       " 1900,\n",
       " 1910,\n",
       " 1920,\n",
       " 1940,\n",
       " 1950,\n",
       " 1960,\n",
       " 1970,\n",
       " 1980,\n",
       " 1990,\n",
       " 2000,\n",
       " 2010,\n",
       " 1810,\n",
       " 1820,\n",
       " 1840,\n",
       " 1930,\n",
       " 1800]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decade_lists=new_index_df.decade.unique().tolist()\n",
    "decade_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a2a069b-594d-4dcf-b6f8-38ddb34341d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_df.to_pickle('/datanaco/dharp/compounds/datasets/googleV3/13902.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fadcd7cb-dbf6-40ad-866e-0327178ba92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_pos</th>\n",
       "      <th>decade</th>\n",
       "      <th>comp_class</th>\n",
       "      <th>is_comp</th>\n",
       "      <th>comp_ner_sent</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>\"_PUNCT \"_PUNCT \"_PUNCT '_PUNCT they_PRON</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>\"_PUNCT \"_PUNCT \"_PUNCT ~._NUM eos_X</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>\"_PUNCT \"_PUNCT \"_PUNCT •_PUNCT the_PRON</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>\"_PUNCT \"_PUNCT '_PUNCT for_ADP the_PRON</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>\"_PUNCT \"_PUNCT ,_PUNCT in_ADP order_NOUN</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519458</th>\n",
       "      <td>\"_PUNCT '_PUNCT my_PRON dear_ADJ friend_NOUN</td>\n",
       "      <td>1800</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519828</th>\n",
       "      <td>\"_PUNCT '_PUNCT such_ADJ be_AUX the_PRON</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520302</th>\n",
       "      <td>\"_PUNCT '_PUNCT tis_VERB just_ADV to_ADP</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520570</th>\n",
       "      <td>\"_PUNCT '_PUNCT what_PRON have_AUX be_AUX</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520641</th>\n",
       "      <td>\"_PUNCT '_PUNCT whilst_SCONJ i_PRON be_AUX</td>\n",
       "      <td>1800</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25976 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            lemma_pos  decade  comp_class  \\\n",
       "86          \"_PUNCT \"_PUNCT \"_PUNCT '_PUNCT they_PRON    1800           0   \n",
       "334              \"_PUNCT \"_PUNCT \"_PUNCT ~._NUM eos_X    1800           0   \n",
       "351          \"_PUNCT \"_PUNCT \"_PUNCT •_PUNCT the_PRON    1800           0   \n",
       "371          \"_PUNCT \"_PUNCT '_PUNCT for_ADP the_PRON    1800           0   \n",
       "488         \"_PUNCT \"_PUNCT ,_PUNCT in_ADP order_NOUN    1800           0   \n",
       "...                                               ...     ...         ...   \n",
       "4519458  \"_PUNCT '_PUNCT my_PRON dear_ADJ friend_NOUN    1800          10   \n",
       "4519828      \"_PUNCT '_PUNCT such_ADJ be_AUX the_PRON    1800           0   \n",
       "4520302      \"_PUNCT '_PUNCT tis_VERB just_ADV to_ADP    1800           0   \n",
       "4520570     \"_PUNCT '_PUNCT what_PRON have_AUX be_AUX    1800           0   \n",
       "4520641    \"_PUNCT '_PUNCT whilst_SCONJ i_PRON be_AUX    1800           0   \n",
       "\n",
       "         is_comp comp_ner_sent  count  \n",
       "86         False                    1  \n",
       "334        False                    1  \n",
       "351        False                    1  \n",
       "371        False                    1  \n",
       "488        False                    2  \n",
       "...          ...           ...    ...  \n",
       "4519458    False                    1  \n",
       "4519828    False                    3  \n",
       "4520302    False                    4  \n",
       "4520570    False                    1  \n",
       "4520641    False                    1  \n",
       "\n",
       "[25976 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index_df.loc[new_index_df.decade==1800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83208ccf-95aa-4f2d-a46d-4fa0b01c7614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_pos</th>\n",
       "      <th>decade</th>\n",
       "      <th>comp_class</th>\n",
       "      <th>is_comp</th>\n",
       "      <th>comp_ner_sent</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>!_PUNCT '_PUNCT \"_PUNCT \"_PUNCT eos_X</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT --------------...</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>\"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT 10,000_NUM</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>\"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT nine_NUM</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>\"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT san_PROPN</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520697</th>\n",
       "      <td>\"_PUNCT '_PUNCT you_PRON be_AUX much_ADV</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520713</th>\n",
       "      <td>\"_PUNCT '_PUNCT you_PRON owe_VERB i_PRON</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520728</th>\n",
       "      <td>\"_PUNCT '_PUNCT you_PRON want_VERB your_PRON</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520755</th>\n",
       "      <td>\"_PUNCT '_PUNCT your_PRON letter_NOUN of_ADP</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4520768</th>\n",
       "      <td>\"_PUNCT '_PUNCT your_PRON mother_NOUN and_CCONJ</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253218 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 lemma_pos  decade  \\\n",
       "6                    !_PUNCT '_PUNCT \"_PUNCT \"_PUNCT eos_X    1900   \n",
       "26       \"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT --------------...    1900   \n",
       "43              \"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT 10,000_NUM    1900   \n",
       "60                \"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT nine_NUM    1900   \n",
       "74               \"_PUNCT \"_PUNCT \"_PUNCT \"_PUNCT san_PROPN    1900   \n",
       "...                                                    ...     ...   \n",
       "4520697           \"_PUNCT '_PUNCT you_PRON be_AUX much_ADV    1900   \n",
       "4520713           \"_PUNCT '_PUNCT you_PRON owe_VERB i_PRON    1900   \n",
       "4520728       \"_PUNCT '_PUNCT you_PRON want_VERB your_PRON    1900   \n",
       "4520755       \"_PUNCT '_PUNCT your_PRON letter_NOUN of_ADP    1900   \n",
       "4520768    \"_PUNCT '_PUNCT your_PRON mother_NOUN and_CCONJ    1900   \n",
       "\n",
       "         comp_class  is_comp comp_ner_sent  count  \n",
       "6                 0    False                   13  \n",
       "26                0    False                   14  \n",
       "43                0    False                   38  \n",
       "60                0    False                    3  \n",
       "74                0    False                   18  \n",
       "...             ...      ...           ...    ...  \n",
       "4520697           0    False                   15  \n",
       "4520713           0    False                   11  \n",
       "4520728           0    False                    1  \n",
       "4520755           0    False                   25  \n",
       "4520768           0    False                    3  \n",
       "\n",
       "[253218 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index_df.loc[new_index_df.decade==1900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dae51c56-1969-4f70-8007-32ee435a79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for decade in decade_lists:\n",
    "\n",
    "    path = f\"/datanaco/dharp/compounds/datasets/googleV3/{decade}\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        new_index_df.loc[new_index_df.decade==decade].reset_index(drop=True).to_pickle(f'{path}/{i}.pkl')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
