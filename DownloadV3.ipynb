{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gentle-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import time\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import spacy\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b36748b5-c74d-46de-b826-808227df3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save_path='/data/dharp/compounds/datasets/'\n",
    "keep_string=r\"(.+_(NOUN|ADV|VERB|ADJ|X|PRT|CONJ|PRON|DET|ADP|NUM|\\.)|_END_)\\s*\"\n",
    "\n",
    "word='.*'\n",
    "\n",
    "nn='(?!(?:NOUN|PROPN)).*'\n",
    "comp='(?:NOUN|PROPN)\\s(?:NOUN|PROPN)'\n",
    "\n",
    "ner_cats=['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
    "n1=f'^{comp}\\s{nn}\\s{comp}$'\n",
    "n2=f'^{comp}\\s{nn}\\s{word}\\s{word}$'\n",
    "n3=f'^{nn}\\s{comp}\\s{nn}\\s{word}$'\n",
    "n4=f'^{word}\\s{nn}\\s{comp}\\s{nn}$'\n",
    "n5=f'^{word}\\s{word}\\s{nn}\\s{comp}$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c949e60-8094-4d40-baf8-0a4626abd84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fmodel = fasttext.load_model('/data/dharp/packages/lid.176.bin')\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e29b5e-3d66-445b-908c-5bd032967ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delist_lang(lst):\n",
    "    lang_lst=[]\n",
    "    for i,lang in enumerate(lst):\n",
    "        if not lang:\n",
    "            lang_lst.append(None)\n",
    "        else:\n",
    "            lang_lst.append(lang[0])\n",
    "    return lang_lst\n",
    "\n",
    "\n",
    "def significance(lst):\n",
    "    significance_list=[]\n",
    "    for l in lst:\n",
    "        if len(l)>1:\n",
    "            significance_list.append(abs(l[0]-l[1])/np.mean(l[0]+l[1])>0.1)\n",
    "            #print(f'{conf[0]} {conf[1]} {abs(conf[0]-conf[1])/np.mean(conf[0]+conf[1])>0.1}')\n",
    "        else:\n",
    "            significance_list.append(True)\n",
    "    return significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d74222-17b9-4c74-8b67-cebe37e5f647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_maker(sent_lst):\n",
    "    ret_sents=[]\n",
    "    g_pos=[]\n",
    "    for sent in sent_lst:\n",
    "        cur_words=[]\n",
    "        pos_sent=[]\n",
    "        sent=sent.replace('_END_','@@@_.')\n",
    "        for word_pos in sent.split(' '):\n",
    "            word,pos=word_pos.rsplit('_',1)\n",
    "            cur_words.append(word)\n",
    "            pos_sent.append(pos)\n",
    "            cur_sent=' '.join(cur_words)\n",
    "            cur_pos=' '.join(pos_sent)\n",
    "        ret_sents.append(cur_sent)\n",
    "        g_pos.append(cur_pos)\n",
    "    return ret_sents,g_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a2701d-072f-45e0-9a17-a59d195c7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_lemma_reducer(sent):\n",
    "    ner_sent=[]\n",
    "    lemma=[]\n",
    "    pos=[]\n",
    "    #parse=[]\n",
    "    is_comp=False\n",
    "    comp_ner_type=[]\n",
    "    parsed_sent=nlp(sent)\n",
    "    for token in parsed_sent:\n",
    "        #parse.append(token.text)\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        #if token.ent_type_==\"\":\n",
    "            #to_add=\"NONNER\"\n",
    "        #else:\n",
    "            #to_add=token.ent_type_\n",
    "        if token.dep_==\"compound\":\n",
    "            is_comp=True\n",
    "            if token.ent_type_!=\"\":\n",
    "                comp_ner_type.append(token.ent_type_)\n",
    "    #print(parse)\n",
    "    comp_ner_sent=' '.join(comp_ner_type)\n",
    "    lemma_sent=' '.join(lemma)\n",
    "    pos_sent=' '.join(pos)\n",
    "    #ner_token_sent=' '.join(ner_token)\n",
    "    #dep_sent=' '.join(dep)\n",
    "    #ner_length=0\n",
    "    #if parsed_sent.ents:\n",
    "    #    for ent in parsed_sent.ents:\n",
    "            #cur_ner=\n",
    "            #cur_ner='_'.join([str(ent.start_char), str(ent.end_char), ent.label_])\n",
    "    #        ner_length+=ent.end_char-ent.start_char\n",
    "            #ner.append(cur_ner)\n",
    "    #else:\n",
    "        #ner.append(\"\")\n",
    "    #ner_sent=' '.join(ner)\n",
    "    \n",
    "    return lemma_sent,pos_sent,is_comp,comp_ner_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126db617-df83-47e9-a228-405ee25700b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_tagger(parsed_sent):\n",
    "    labels,confs=fmodel.predict(parsed_sent,k=-1,threshold=0.1)\n",
    "    lang_list=delist_lang(labels)    \n",
    "    significance_list=significance(confs)\n",
    "    assert len(lang_list)==len(significance_list)\n",
    "    return lang_list,significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d53b63-07d5-43dd-8536-e06ce4fc3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_processor(df):\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    ret_lst=sent_maker(df.old_index)\n",
    "    \n",
    "    df['sent']=ret_lst[0]\n",
    "    df['g_pos']=ret_lst[1]\n",
    "    \n",
    "    results=np.vectorize(ner_lemma_reducer)(df.sent.values)\n",
    "    results_df=pd.DataFrame(results)\n",
    "    results_df=results_df.transpose()\n",
    "    results_df.columns=['lemma_sent','pos_sent','num_comp','comp_ner_sent']\n",
    "\n",
    "    #results_df=results_df.loc[~results_df.ner_token_sent.str.contains(\"PERSON PERSON\")]\n",
    "\n",
    "    index_df=pd.concat([df,results_df],axis=1,ignore_index=False)\n",
    "\n",
    "    lang_list,significance_list=lang_tagger(index_df.sent.values.tolist())\n",
    "    index_df['lang']=lang_list\n",
    "    index_df['lang_conf']=significance_list\n",
    "    index_df.lang=index_df.lang.str.split('_',n=4).str[-1]\n",
    "    index_df=index_df.loc[(index_df.lang=='en') &(index_df.lang_conf==True)]\n",
    "\n",
    "    index_df['nwords']=index_df.pos_sent.str.count(' ').add(1)\n",
    "    index_df=index_df.loc[index_df.nwords==5]\n",
    "    \n",
    "    index_df.lemma_sent=index_df.lemma_sent.str.lower()\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('PROPN','NOUN',regex=False)\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('AUX','VERB',regex=False)\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('CCONJ','CONJ',regex=False)\n",
    "    #index_df.g_pos=index_df.g_pos.str.replace('.','PUNCT',regex=False)\n",
    "    #index_df.g_pos=index_df.g_pos.str.replace('PRT','ADP',regex=False)\n",
    "    if index_df.shape[0]==0:\n",
    "        return pd.DataFrame()\n",
    "    index_df['lemma_pos']=str_joiner(index_df)\n",
    "    index_df['nX']=index_df.pos_sent.str.count('X')-index_df.pos_sent.str.count('AUX')\n",
    "    index_df=index_df.loc[~(index_df.nX>1)]\n",
    "    \n",
    "    #index_df['ner_perc']=index_df.ner_length/index_df.sent.str.len()\n",
    "   \n",
    "    index_df['comp_class']=0\n",
    "\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n1),'comp_class']=1\n",
    "    index_df.loc[~(index_df.pos_sent.str.contains(n1))& index_df.pos_sent.str.contains(n2),'comp_class']=2\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n3),'comp_class']=3\n",
    "    index_df.loc[index_df.pos_sent.str.contains(n4),'comp_class']=4\n",
    "    index_df.loc[~(index_df.pos_sent.str.contains(n1))& index_df.pos_sent.str.contains(n5),'comp_class']=5\n",
    "    \n",
    "    index_df.drop(['old_index','g_pos','lang','lang_conf','nwords','nX','lemma_sent'],axis=1,inplace=True)\n",
    "    index_year_df=year_count_split(index_df)\n",
    "    index_df=index_df.merge(index_year_df, on='lemma_pos',how='right')\n",
    "    index_df['count']=index_df['count'].astype(\"int64\")\n",
    "    index_df['year']=index_df['year'].astype(\"int64\")\n",
    "    index_df=index_df.groupby(['lemma_pos','pos_sent','year','comp_class','num_comp','comp_ner_sent'])['count'].sum().to_frame().reset_index()\n",
    "    return index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6b7860-c1d3-4fb7-8c4b-63d2dc9cb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_count_split(df):\n",
    "    trial_df=pd.concat([df.lemma_pos, df.year_counts.str.split(\"\\t\", expand=True)], axis=1)\n",
    "    trial_df=pd.melt(trial_df, id_vars=[\"lemma_pos\"], value_vars=list(range(len(trial_df.columns)-1))).dropna().drop(\"variable\", axis = 1)\n",
    "    trial_df[['year','count']] = trial_df.value.str.split(\",\", n=3, expand=True)[[0,1]]\n",
    "    return trial_df.drop(['value'],axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb5c354a-6cb1-4248-a1be-18fe2cf1b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_joiner(df):\n",
    "    #print(df)\n",
    "    new_df=pd.DataFrame()\n",
    "    try:\n",
    "        new_df[['l1','l2','l3','l4','l5']]=df.lemma_sent.str.split(\" \",expand=True)\n",
    "        new_df[['p1','p2','p3','p4','p5']]=df.pos_sent.str.split(\" \",expand=True)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    new_df['lemma_pos']=new_df.l1+\"_\"+new_df.p1+\" \"+\\\n",
    "                        new_df.l2+\"_\"+new_df.p2+\" \"+\\\n",
    "                        new_df.l3+\"_\"+new_df.p3+\" \"+\\\n",
    "                        new_df.l4+\"_\"+new_df.p4+\" \"+\\\n",
    "                        new_df.l5+\"_\"+new_df.p5\n",
    "    return new_df['lemma_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "978f52bd-80d8-489b-859f-26041ee2f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='5-01700-of-19423'\n",
    "lnk=f'http://storage.googleapis.com/books/ngrams/books/20200217/eng/{fname}.gz'\n",
    "#index_df   = pd.read_csv(lnk, compression='gzip', squeeze=True, header=None,quoting=csv.QUOTE_NONE)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f48bbaa8-65bf-4cd1-9890-a33dd266c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "\n",
    "# defining the url\n",
    "url = \"https://data.brasil.io/dataset/covid19/caso_full.csv.gz\"\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "print(type(content))\n",
    "\n",
    "#compressed_file = io.BytesIO(response.read())\n",
    "#decompressed_file = gzip.GzipFile(fileobj=compressed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bff2573e-ec51-4dec-b2f9-35f936123712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x7f67cfc4e850>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ef6a8a7-2e1f-470c-908f-1e321fa2a1b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tt\u001b[38;5;241m=\u001b[39m\u001b[43mdecompressed_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "tt=decompressed_file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed877c70-9915-454a-a827-189949ccca68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'D _NOUN_ _NOUN_ _ADP_ depression\\t1952,2,1\\t1953,5,3\\t1956,3,3\\t1959,1,1\\t1964,1,1\\t1967,1,1\\t1971,1,1\\t1980'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "261e0464-26b6-402c-8fd9-5bc21f77085f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_index</th>\n",
       "      <th>year_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>D'_NOUN Artagnan_NOUN ,_. coloring_VERB a_DET</td>\n",
       "      <td>1888,1,1\\t1890,1,1\\t1892,1,1\\t1893,8,8\\t1894,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>D'_NOUN AMATO_NOUN :_. I_PRON understand_VERB</td>\n",
       "      <td>1975,1,1\\t1986,15,11\\t1988,11,8\\t1989,2,2\\t199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>D'_NOUN E_NOUN '_. is_VERB the_DET</td>\n",
       "      <td>1826,4,1\\t1832,8,2\\t1835,4,1\\t1836,4,1\\t1838,8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>D'_NOUN Onofrio_NOUN -_. Flores_NOUN ,_.</td>\n",
       "      <td>1976,1,1\\t1978,1,1\\t1980,1,1\\t1981,1,1\\t1982,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>D'_NOUN you_PRON know_VERB ,_. only_ADV</td>\n",
       "      <td>1886,1,1\\t1924,9,9\\t1925,3,3\\t1926,1,1\\t1928,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198962</th>\n",
       "      <td>D'_NOUN Amato_NOUN Laxalt_NOUN Stennis_NOUN Da...</td>\n",
       "      <td>1981,3,3\\t1982,18,14\\t1983,21,17\\t1984,5,5\\t19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199391</th>\n",
       "      <td>D'_NOUN you_PRON mean_VERB that_DET ?_.</td>\n",
       "      <td>1896,1,1\\t1900,6,4\\t1901,2,2\\t1908,3,3\\t1909,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199392</th>\n",
       "      <td>D'_NOUN Hondt_NOUN ,_. W._NOUN ,_.</td>\n",
       "      <td>1966,3,1\\t1982,5,1\\t1985,2,1\\t1987,3,3\\t1988,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199422</th>\n",
       "      <td>D'_NOUN s_VERB 31_NUM consecutive_ADJ day_NOUN</td>\n",
       "      <td>1974,5,5\\t1981,1,1\\t1986,1,1\\t1987,8,8\\t1989,2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199447</th>\n",
       "      <td>D'_NOUN Alessandro_NOUN ,_. W._NOUN ,_.</td>\n",
       "      <td>1981,1,1\\t1989,1,1\\t1990,8,1\\t1992,1,1\\t1995,4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23355 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 old_index  \\\n",
       "39           D'_NOUN Artagnan_NOUN ,_. coloring_VERB a_DET   \n",
       "551          D'_NOUN AMATO_NOUN :_. I_PRON understand_VERB   \n",
       "679                     D'_NOUN E_NOUN '_. is_VERB the_DET   \n",
       "798               D'_NOUN Onofrio_NOUN -_. Flores_NOUN ,_.   \n",
       "1235               D'_NOUN you_PRON know_VERB ,_. only_ADV   \n",
       "...                                                    ...   \n",
       "3198962  D'_NOUN Amato_NOUN Laxalt_NOUN Stennis_NOUN Da...   \n",
       "3199391            D'_NOUN you_PRON mean_VERB that_DET ?_.   \n",
       "3199392                 D'_NOUN Hondt_NOUN ,_. W._NOUN ,_.   \n",
       "3199422     D'_NOUN s_VERB 31_NUM consecutive_ADJ day_NOUN   \n",
       "3199447            D'_NOUN Alessandro_NOUN ,_. W._NOUN ,_.   \n",
       "\n",
       "                                               year_counts  \n",
       "39       1888,1,1\\t1890,1,1\\t1892,1,1\\t1893,8,8\\t1894,1...  \n",
       "551      1975,1,1\\t1986,15,11\\t1988,11,8\\t1989,2,2\\t199...  \n",
       "679      1826,4,1\\t1832,8,2\\t1835,4,1\\t1836,4,1\\t1838,8...  \n",
       "798      1976,1,1\\t1978,1,1\\t1980,1,1\\t1981,1,1\\t1982,2...  \n",
       "1235     1886,1,1\\t1924,9,9\\t1925,3,3\\t1926,1,1\\t1928,2...  \n",
       "...                                                    ...  \n",
       "3198962  1981,3,3\\t1982,18,14\\t1983,21,17\\t1984,5,5\\t19...  \n",
       "3199391  1896,1,1\\t1900,6,4\\t1901,2,2\\t1908,3,3\\t1909,1...  \n",
       "3199392  1966,3,1\\t1982,5,1\\t1985,2,1\\t1987,3,3\\t1988,2...  \n",
       "3199422  1974,5,5\\t1981,1,1\\t1986,1,1\\t1987,8,8\\t1989,2...  \n",
       "3199447  1981,1,1\\t1989,1,1\\t1990,8,1\\t1992,1,1\\t1995,4...  \n",
       "\n",
       "[23355 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df[['old_index','year_counts']]=index_df[0].str.split('\\t',n=1,expand=True)\n",
    "index_df=index_df.loc[index_df.old_index.str.match(\"^\"+keep_string*5+\"$\",na=False)]\n",
    "index_df.drop(0,axis=1,inplace=True)\n",
    "index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "045b1fa4-aa88-49de-9dbc-1ba90fd6cec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parallelization\n",
      "Total time taken 18 secs\n"
     ]
    }
   ],
   "source": [
    "if index_df.shape[0]<10_000:\n",
    "    \n",
    "    cur_time=time.time()\n",
    "    new_index_df=index_processor(index_df)\n",
    "    print(f'Total time taken {round(time.time()-cur_time)} secs')\n",
    "    \n",
    "else:\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    cur_time=time.time()\n",
    "    df_split = np.array_split(index_df, num_partitions)\n",
    "    pool = Pool(num_partitions)\n",
    "    print('Started parallelization')\n",
    "    results=pool.map_async(index_processor,df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "        \n",
    "    curr_df_list=results.get()\n",
    "    new_index_df=pd.concat(curr_df_list,ignore_index=True)\n",
    "    print(f'Total time taken {round(time.time()-cur_time)} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98d128f7-a739-422f-a7b6-0d77eb88c237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_pos</th>\n",
       "      <th>pos_sent</th>\n",
       "      <th>year</th>\n",
       "      <th>comp_class</th>\n",
       "      <th>num_comp</th>\n",
       "      <th>comp_ner_sent</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT NUM</td>\n",
       "      <td>1961</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT NUM</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT NUM</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT NUM</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...</td>\n",
       "      <td>NOUN PUNCT NOUN PUNCT NUM</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV</td>\n",
       "      <td>PROPN VERB ADP ADV ADV</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV</td>\n",
       "      <td>PROPN VERB ADP ADV ADV</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV</td>\n",
       "      <td>PROPN VERB ADP ADV ADV</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV</td>\n",
       "      <td>PROPN VERB ADP ADV ADV</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV</td>\n",
       "      <td>PROPN VERB ADP ADV ADV</td>\n",
       "      <td>2019</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lemma_pos  \\\n",
       "0    d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...   \n",
       "1    d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...   \n",
       "2    d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...   \n",
       "3    d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...   \n",
       "4    d'alessandro_NOUN ,_PUNCT a._NOUN (_PUNCT 2008...   \n",
       "..                                                 ...   \n",
       "706  d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV   \n",
       "707  d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV   \n",
       "708  d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV   \n",
       "709  d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV   \n",
       "710  d'artagnan_PROPN go_VERB up_ADP once_ADV more_ADV   \n",
       "\n",
       "                      pos_sent  year  comp_class  num_comp comp_ner_sent  \\\n",
       "0    NOUN PUNCT NOUN PUNCT NUM  1961           0     False                 \n",
       "1    NOUN PUNCT NOUN PUNCT NUM  2008           0     False                 \n",
       "2    NOUN PUNCT NOUN PUNCT NUM  2009           0     False                 \n",
       "3    NOUN PUNCT NOUN PUNCT NUM  2010           0     False                 \n",
       "4    NOUN PUNCT NOUN PUNCT NUM  2011           0     False                 \n",
       "..                         ...   ...         ...       ...           ...   \n",
       "706     PROPN VERB ADP ADV ADV  2015           0     False                 \n",
       "707     PROPN VERB ADP ADV ADV  2016           0     False                 \n",
       "708     PROPN VERB ADP ADV ADV  2017           0     False                 \n",
       "709     PROPN VERB ADP ADV ADV  2018           0     False                 \n",
       "710     PROPN VERB ADP ADV ADV  2019           0     False                 \n",
       "\n",
       "     count  \n",
       "0        1  \n",
       "1        1  \n",
       "2        2  \n",
       "3       12  \n",
       "4        9  \n",
       "..     ...  \n",
       "706      4  \n",
       "707      4  \n",
       "708     14  \n",
       "709     21  \n",
       "710     10  \n",
       "\n",
       "[711 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a2a069b-594d-4dcf-b6f8-38ddb34341d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_df.to_pickle('/data/dharp/compounds/datasets/googleV3/1700.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c9d9f-7c8f-4f49-aa6f-2dfe5a5ac062",
   "metadata": {},
   "outputs": [],
   "source": [
    "N - N\n",
    "N 's N\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
