{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77f0fee-bc2e-4523-9070-0ee771d0de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gentle-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import time\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok\n",
    "detokenizer = Detok()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36748b5-c74d-46de-b826-808227df3679",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save_path='/data/dharp/compounds/datasets/'\n",
    "keep_string=r\"(.+_(NOUN|ADV|VERB|ADJ|X|PRT|CONJ|PRON|DET|ADP|NUM|\\.)|_END_|_START_)\\s*\"\n",
    "try_keep_string=r\"(.+_(NOUN|ADV|VERB|ADJ|X|PRT|CONJ|PRON|DET|ADP|NUM|\\.)|_NOUN_|_ADV_|_VERB_|_ADJ_|_X_|_PRT_|_CONJ_|_PRON_|_DET_|_ADP_|_NUM_|_\\._)\"\n",
    "\n",
    "word='.*'\n",
    "\n",
    "nn='(?!(?:NOUN|PROPN)).*'\n",
    "nn_comp='(?:NOUN|PROPN)\\s(?:NOUN|PROPN)'\n",
    "an_comp='ADJ\\s(?:NOUN|PROPN)'\n",
    "\n",
    "ner_cats=['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
    "n1=f'^{nn_comp}\\s{nn}\\s{nn_comp}$'\n",
    "n2=f'^{nn_comp}\\s{nn}\\s{word}\\s{word}$'\n",
    "n3=f'^{nn}\\s{nn_comp}\\s{nn}\\s{word}$'\n",
    "n4=f'^{word}\\s{nn}\\s{nn_comp}\\s{nn}$'\n",
    "n5=f'^{word}\\s{word}\\s{nn}\\s{nn_comp}$'\n",
    "\n",
    "a1=f'^{an_comp}\\s{nn}\\s{an_comp}$'\n",
    "a2=f'^{an_comp}\\s{nn}\\s{word}\\s{word}$'\n",
    "a3=f'^{nn}\\s{an_comp}\\s{nn}\\s{word}$'\n",
    "a4=f'^{word}\\s{nn}\\s{an_comp}\\s{nn}$'\n",
    "a5=f'^{word}\\s{word}\\s{nn}\\s{an_comp}$'\n",
    "\n",
    "\n",
    "c1=f'^{nn_comp}\\s{nn}\\s{an_comp}$'\n",
    "c2=f'^{an_comp}\\s{nn}\\s{nn_comp}$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c949e60-8094-4d40-baf8-0a4626abd84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fmodel = fasttext.load_model('/data/dharp/packages/lid.176.bin')\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8448d92a-6861-483b-82d5-6ffa35fc41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_lemma_reducer(sent):\n",
    "    ner_sent=[]\n",
    "    lemma=[]\n",
    "    pos=[]\n",
    "    dep=[]\n",
    "    comp_ner_type=[]\n",
    "    parsed_sent=nlp(sent)\n",
    "    for token in parsed_sent:\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        dep.append(token.dep_)\n",
    "        if token.dep_==\"compound\":\n",
    "            if token.ent_type_!=\"\":\n",
    "                comp_ner_type.append(token.ent_type_)\n",
    "\n",
    "    comp_ner_sent=' '.join(comp_ner_type)\n",
    "    if len(parsed_sent)<5:\n",
    "        new_lemma_list=[\"eos\"]*(5-len(parsed_sent))\n",
    "        new_pos_list=[\"X\"]*(5-len(parsed_sent))\n",
    "        lemma.extend(new_lemma_list)\n",
    "        pos.extend(new_pos_list)\n",
    "        \n",
    "    comp_ner_sent=' '.join(comp_ner_type)\n",
    "    lemma_sent=' '.join(lemma)\n",
    "    pos_sent=' '.join(pos)\n",
    "    \n",
    "    dep_sent=' '.join(dep)\n",
    "        \n",
    "    num_count=len(re.findall(\"compound\\s(?!compound)\", dep_sent))\n",
    "   \n",
    "    return lemma_sent,pos_sent,num_count,comp_ner_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d5c18-0cf0-4108-9422-af3c65fb4555",
   "metadata": {},
   "source": [
    "    ner_sent' '.join(comp_ner_type)\n",
    "    lemma_sent_lst=[]\n",
    "    pos_sent_lst=[]\n",
    "    dep_sent_lst=[]\n",
    "    num_count_lst=[]\n",
    "    \n",
    "    temp_lemma=[]\n",
    "    temp_pos=[]\n",
    "    temp_dep=[]\n",
    "    temp_num_count=[]\n",
    "    \n",
    "    for i in range(len(parsed_sent) - 5 + 1):\n",
    "        temp_lemma=lemma[i: i + 5]\n",
    "        temp_pos=pos[i:i+5]\n",
    "        temp_dep=dep[i:i+5]\n",
    "        \n",
    "        lemma_sent_lst.append(' '.join(temp_lemma))\n",
    "        pos_sent_lst.append(' '.join(temp_pos))\n",
    "        \n",
    "        dep_sent=' '.join(temp_dep)\n",
    "        dep_sent_lst.append(dep_sent)\n",
    "        \n",
    "        num_count_lst.append(len(re.findall(\"compound\\s(?!compound)\", dep_sent)))\n",
    "\n",
    "    return lemma_sent,pos_sent,num_count,comp_ner_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e29b5e-3d66-445b-908c-5bd032967ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delist_lang(lst):\n",
    "    lang_lst=[]\n",
    "    for i,lang in enumerate(lst):\n",
    "        if not lang:\n",
    "            lang_lst.append(None)\n",
    "        else:\n",
    "            lang_lst.append(lang[0])\n",
    "    return lang_lst\n",
    "\n",
    "\n",
    "def significance(lst):\n",
    "    significance_list=[]\n",
    "    for l in lst:\n",
    "        if len(l)>1:\n",
    "            significance_list.append(abs(l[0]-l[1])/np.mean(l[0]+l[1])>0.1)\n",
    "            #print(f'{conf[0]} {conf[1]} {abs(conf[0]-conf[1])/np.mean(conf[0]+conf[1])>0.1}')\n",
    "        else:\n",
    "            significance_list.append(True)\n",
    "    return significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "126db617-df83-47e9-a228-405ee25700b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_tagger(parsed_sent):\n",
    "    labels,confs=fmodel.predict(parsed_sent,k=-1,threshold=0.1)\n",
    "    lang_list=delist_lang(labels)    \n",
    "    significance_list=significance(confs)\n",
    "    assert len(lang_list)==len(significance_list)\n",
    "    return lang_list,significance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d53b63-07d5-43dd-8536-e06ce4fc3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_processor(df):\n",
    "    \n",
    "    df['sent']=np.vectorize(detokenizer.detokenize)(df.old_index.str.split(\" \").values)\n",
    "    df['sent']=df.sent.str.replace('\\s*,\\s*',', ',regex=False).copy()\n",
    "    df['sent']=df.sent.str.replace('\\s*\\.\\s*','. ',regex=False).copy()\n",
    "    df['sent']=df.sent.str.replace('\\s*\\?\\s*','? ',regex=False).copy()\n",
    "    df['sent']=df.sent.str.replace('__',' ',regex=False).copy()\n",
    "\n",
    "    df['sent']=df.sent.str.replace('_START_ ','',regex=False).copy()\n",
    "    df['sent']=df['sent'].str.replace(' _END_','',regex=False).copy()\n",
    "     \n",
    "    #df['sent']=df['sent'].str.replace(r\"(.+)'\\s(.+)\",r\"\\1'\\2\",regex=True).copy()\n",
    "    #df['sent']=df['sent'].str.replace(r\"(.+)\\s'(.+)\",r\"\\1'\\2\",regex=True).copy()\n",
    "\n",
    "    lang_list,significance_list=lang_tagger(df.sent.values.tolist())\n",
    "    df['lang']=lang_list\n",
    "    df['lang_conf']=significance_list\n",
    "    df.lang=df.lang.str.split('_',n=4).str[-1]\n",
    "    \n",
    "    df=df.loc[(df.lang=='en') &(df.lang_conf==True)]\n",
    "\n",
    "    lemma_sent,pos_sent,comp_count,comp_ner_sent=np.vectorize(ner_lemma_reducer)(df.sent.values)\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    df['lemma_sent']=lemma_sent\n",
    "    df['pos_sent']=pos_sent\n",
    "    df['comp_count']=comp_count\n",
    "    df['comp_ner_sent']=comp_ner_sent\n",
    "    \n",
    "    df['is_comp']=False\n",
    "    df.loc[df.comp_count!=0,'is_comp']=True\n",
    "    #results_df=results_df.loc[~results_df.ner_token_sent.str.contains(\"PERSON PERSON\")]\n",
    "\n",
    "    #index_df=pd.concat([df,results_df],axis=1,ignore_index=True)\n",
    "\n",
    "    #return results_df,df\n",
    "\n",
    "    #index_df=index_df.loc[(index_df.lang=='en') &(index_df.lang_conf==True)]\n",
    "\n",
    "    df['nwords']=df.pos_sent.str.count(' ').add(1).copy()\n",
    "    \n",
    "    pd.options.mode.chained_assignment = 'warn'\n",
    "    df=df.loc[df.nwords==5]\n",
    "    \n",
    "    df.lemma_sent=df.lemma_sent.str.lower()\n",
    "\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('PROPN','NOUN',regex=False)\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('AUX','VERB',regex=False)\n",
    "    #index_df.pos_sent=index_df.pos_sent.str.replace('CCONJ','CONJ',regex=False)\n",
    "    #index_df.g_pos=index_df.g_pos.str.replace('.','PUNCT',regex=False)\n",
    "    #index_df.g_pos=index_df.g_pos.str.replace('PRT','ADP',regex=False)\n",
    "    if df.shape[0]==0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['lemma_pos']=str_joiner(df)\n",
    "    df['nX']=df.pos_sent.str.count('X')-df.pos_sent.str.count('AUX')\n",
    "    df=df.loc[~(df.nX==5)]\n",
    "       \n",
    "    df['comp_class']=0\n",
    "\n",
    "    df.loc[df.pos_sent.str.contains(n1),'comp_class']=1\n",
    "    df.loc[~(df.pos_sent.str.contains(n1))& df.pos_sent.str.contains(n2),'comp_class']=2\n",
    "    df.loc[df.pos_sent.str.contains(n3),'comp_class']=3\n",
    "    df.loc[df.pos_sent.str.contains(n4),'comp_class']=4\n",
    "    df.loc[~(df.pos_sent.str.contains(n1))& df.pos_sent.str.contains(n5),'comp_class']=5\n",
    "    \n",
    "    df.loc[df.pos_sent.str.contains(a1),'comp_class']=6\n",
    "    df.loc[~(df.pos_sent.str.contains(a1))& df.pos_sent.str.contains(a2),'comp_class']=7\n",
    "    df.loc[df.pos_sent.str.contains(a3),'comp_class']=8\n",
    "    df.loc[df.pos_sent.str.contains(a4),'comp_class']=9\n",
    "    df.loc[~(df.pos_sent.str.contains(a1))& df.pos_sent.str.contains(a5),'comp_class']=10\n",
    "\n",
    "    df.loc[df.pos_sent.str.contains(c1),'comp_class']=11\n",
    "    df.loc[df.pos_sent.str.contains(c2),'comp_class']=12\n",
    "\n",
    "    df.drop(['sent','pos_sent','lang','lang_conf','nwords','nX','lemma_sent'],axis=1,inplace=True)\n",
    "\n",
    "    index_year_df=year_count_split(df)\n",
    "    index_df=df.merge(index_year_df, on='old_index',how='right')\n",
    "    index_df['count']=index_df['count'].astype(\"int64\")\n",
    "    index_df['year']=index_df['year'].astype(\"int64\")\n",
    "    index_df=index_df.groupby(['lemma_pos','year','comp_class','is_comp','comp_ner_sent'])['count'].sum().to_frame().reset_index()\n",
    "    return index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc6b7860-c1d3-4fb7-8c4b-63d2dc9cb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_count_split(df):\n",
    "    trial_df=pd.concat([df.old_index, df.year_counts.str.split(\"\\t\", expand=True)], axis=1)\n",
    "    trial_df=pd.melt(trial_df, id_vars=[\"old_index\"], value_vars=list(range(len(trial_df.columns)-1))).dropna().drop(\"variable\", axis = 1)\n",
    "    trial_df[['year','count']] = trial_df.value.str.split(\",\", n=3, expand=True)[[0,1]]\n",
    "    return trial_df.drop(['value'],axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb5c354a-6cb1-4248-a1be-18fe2cf1b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_joiner(df):\n",
    "    #print(df)\n",
    "    new_df=pd.DataFrame()\n",
    "    try:\n",
    "        new_df[['l1','l2','l3','l4','l5']]=df.lemma_sent.str.split(\" \",expand=True,n=4)\n",
    "        new_df[['p1','p2','p3','p4','p5']]=df.pos_sent.str.split(\" \",expand=True,n=4)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    new_df['lemma_pos']=new_df.l1+\"_\"+new_df.p1+\" \"+\\\n",
    "                        new_df.l2+\"_\"+new_df.p2+\" \"+\\\n",
    "                        new_df.l3+\"_\"+new_df.p3+\" \"+\\\n",
    "                        new_df.l4+\"_\"+new_df.p4+\" \"+\\\n",
    "                        new_df.l5+\"_\"+new_df.p5\n",
    "    return new_df['lemma_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "978f52bd-80d8-489b-859f-26041ee2f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='5-18104-of-19423'\n",
    "\n",
    "fname='5-09107-of-19423'\n",
    "\n",
    "fname='5-00604-of-19423'\n",
    "\n",
    "lnk=f'http://storage.googleapis.com/books/ngrams/books/20200217/eng/{fname}.gz'\n",
    "index_df   = pd.read_csv(lnk, compression='gzip', header=None, sep=u\"\\u0001\", quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ddd24e-1c39-4d55-bb88-ba64e2346239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23976/1080052919.py:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  index_df=index_df.loc[~index_df.old_index.str.contains(try_keep_string,na=False,regex=True)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_index</th>\n",
       "      <th>year_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, and without an indifferent</td>\n",
       "      <td>1800,1,1\\t1820,2,2\\t1823,2,2\\t1825,4,4\\t1826,4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>, and where the deponent</td>\n",
       "      <td>1771,2,2\\t1779,1,1\\t1780,5,5\\t1782,1,1\\t1813,5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>, and were such men</td>\n",
       "      <td>1832,1,1\\t1851,1,1\\t1855,1,1\\t1862,6,6\\t1871,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>, and will reveal their</td>\n",
       "      <td>1889,3,3\\t1894,2,2\\t1896,10,10\\t1901,1,1\\t1909...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, and would even out</td>\n",
       "      <td>1864,3,3\\t1865,1,1\\t1866,1,1\\t1870,4,4\\t1871,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233257</th>\n",
       "      <td>, and who think very</td>\n",
       "      <td>1836,1,1\\t1840,5,5\\t1841,5,5\\t1843,4,4\\t1848,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233258</th>\n",
       "      <td>, and with no obstructions</td>\n",
       "      <td>1825,3,3\\t1832,1,1\\t1847,1,1\\t1866,1,1\\t1868,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233259</th>\n",
       "      <td>, annong others , are</td>\n",
       "      <td>1808,1,1\\t1840,1,1\\t1890,3,3\\t1893,1,1\\t1898,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233260</th>\n",
       "      <td>, and when they age</td>\n",
       "      <td>1911,2,2\\t1920,1,1\\t1971,1,1\\t1972,3,3\\t1975,1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233261</th>\n",
       "      <td>, annealing of gamma ,</td>\n",
       "      <td>1962,45,22\\t1963,3,2\\t1964,1,1\\t1965,1,1\\t1966...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1233262 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            old_index  \\\n",
       "0        , and without an indifferent   \n",
       "1            , and where the deponent   \n",
       "2                 , and were such men   \n",
       "3             , and will reveal their   \n",
       "4                , and would even out   \n",
       "...                               ...   \n",
       "1233257          , and who think very   \n",
       "1233258    , and with no obstructions   \n",
       "1233259         , annong others , are   \n",
       "1233260           , and when they age   \n",
       "1233261        , annealing of gamma ,   \n",
       "\n",
       "                                               year_counts  \n",
       "0        1800,1,1\\t1820,2,2\\t1823,2,2\\t1825,4,4\\t1826,4...  \n",
       "1        1771,2,2\\t1779,1,1\\t1780,5,5\\t1782,1,1\\t1813,5...  \n",
       "2        1832,1,1\\t1851,1,1\\t1855,1,1\\t1862,6,6\\t1871,1...  \n",
       "3        1889,3,3\\t1894,2,2\\t1896,10,10\\t1901,1,1\\t1909...  \n",
       "4        1864,3,3\\t1865,1,1\\t1866,1,1\\t1870,4,4\\t1871,1...  \n",
       "...                                                    ...  \n",
       "1233257  1836,1,1\\t1840,5,5\\t1841,5,5\\t1843,4,4\\t1848,1...  \n",
       "1233258  1825,3,3\\t1832,1,1\\t1847,1,1\\t1866,1,1\\t1868,1...  \n",
       "1233259  1808,1,1\\t1840,1,1\\t1890,3,3\\t1893,1,1\\t1898,1...  \n",
       "1233260  1911,2,2\\t1920,1,1\\t1971,1,1\\t1972,3,3\\t1975,1...  \n",
       "1233261  1962,45,22\\t1963,3,2\\t1964,1,1\\t1965,1,1\\t1966...  \n",
       "\n",
       "[1233262 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_df[['old_index','year_counts']]=index_df[0].str.split('\\t',n=1,expand=True)\n",
    "#index_df=index_df.loc[index_df.old_index.str.match(\"^\"+keep_string*5+\"$\",na=False)]\n",
    "index_df=index_df.loc[~index_df.old_index.str.contains(try_keep_string,na=False,regex=True)]\n",
    "index_df.drop(0,axis=1,inplace=True)\n",
    "index_df.reset_index(drop=True,inplace=True)\n",
    "index_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "045b1fa4-aa88-49de-9dbc-1ba90fd6cec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parallelization\n",
      "Total time taken 1919 secs\n"
     ]
    }
   ],
   "source": [
    "if index_df.shape[0]<10_000:\n",
    "    \n",
    "    cur_time=time.time()\n",
    "    new_index_df=index_processor(index_df)\n",
    "    print(f'Total time taken {round(time.time()-cur_time)} secs')\n",
    "    \n",
    "else:\n",
    "    num_partitions=round(0.95*mp.cpu_count())\n",
    "    cur_time=time.time()\n",
    "    df_split = np.array_split(index_df, num_partitions)\n",
    "    pool = Pool(num_partitions)\n",
    "    print('Started parallelization')\n",
    "    results=pool.map_async(index_processor,df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "        \n",
    "        \n",
    "    curr_df_list=results.get()\n",
    "    new_index_df=pd.concat(curr_df_list,ignore_index=True)\n",
    "    print(f'Total time taken {round(time.time()-cur_time)} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec1c2952-9268-4c6e-adff-90b14cbea7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 65785301 entries, 0 to 65785300\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   lemma_pos      object\n",
      " 1   year           int64 \n",
      " 2   comp_class     int64 \n",
      " 3   is_comp        bool  \n",
      " 4   comp_ner_sent  object\n",
      " 5   count          int64 \n",
      "dtypes: bool(1), int64(3), object(2)\n",
      "memory usage: 2.5+ GB\n"
     ]
    }
   ],
   "source": [
    "new_index_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a2a069b-594d-4dcf-b6f8-38ddb34341d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index_df.to_pickle('/data/dharp/compounds/datasets/googleV3/1700.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c9d9f-7c8f-4f49-aa6f-2dfe5a5ac062",
   "metadata": {},
   "outputs": [],
   "source": [
    "N - N\n",
    "N 's N\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
